{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55af2858-9bc5-45f2-9b55-e07379262644",
   "metadata": {},
   "source": [
    "# Step 5 - Compare Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this final notebook, we'll bring together all the metrics collected throughout our migration evaluation process to make data-driven decisions about model selection. Building on the latency measurements from Step 3 and quality assessments from Step 4, we'll now add cost projections and generate comprehensive visualizations to compare our candidate models.\n",
    "\n",
    "Whether you're migrating between foundation models or selecting a model for a new application, it's essential to consider multiple dimensions simultaneously.Our analysis framework helps you visualize and quantify these considerations, providing clear guidance for your model selection decision regardless of your starting point.\n",
    "\n",
    "\n",
    "### What You'll Get From This Analysis\n",
    "\n",
    "This notebook produces two key artifacts:\n",
    "\n",
    "1. **PDF Analysis Report**: A comprehensive document with visualizations comparing models across all dimensions\n",
    "2. **CSV Summary**: Raw data for further analysis or integration with other business metrics\n",
    "\n",
    "Let's begin consolidating our evaluation results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd90a5-0de6-409c-9f9c-893a9801e6ff",
   "metadata": {},
   "source": [
    "## Evaluation Tools and Utilities\n",
    "\n",
    "### Leveraging Pre-built Analysis Components\n",
    "\n",
    "To streamline our model comparison process, we'll use two specialized Python classes created for this workshop:\n",
    "\n",
    "1. **`pricing`**: A utility class that enables accurate cost calculations based on token usage\n",
    "  \n",
    "2. **`generate_analysis_report`**: A function that produces professional PDF reports\n",
    "\n",
    "These utilities encapsulate complex functionality to let us focus on analysis rather than implementation details. The complete source code for these classes is available in the `../src` directory and can be customized for your specific evaluation needs beyond this workshop.\n",
    "\n",
    "> **Workshop Note**: When adapting this notebook for your own migration projects, consider extending these classes to incorporate additional metrics specific to your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9d929a-c7d2-4326-bc6c-690386f9bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src import pricing\n",
    "from src import generate_analysis_report\n",
    "\n",
    "\n",
    "# AWS Configuration\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "BUCKET_NAME = f\"genai-evaluation-migration-bucket-{account_id}\"\n",
    "PREFIX = \"genai_migration\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bee547-dbd3-4ab0-a552-d0ee1bc5d0c5",
   "metadata": {},
   "source": [
    "## Loading Evaluation Data\n",
    "\n",
    "### Retrieving Our Model Tracking Information\n",
    "\n",
    "Before we can analyze our models, we need to access the tracking information maintained throughout our evaluation process.\n",
    "By loading this tracking information first, we establish a foundation for our consolidated analysis and ensure we're working with consistent model metadata throughout our comparison process.\n",
    "\n",
    "> **Note for Self-Paced Learners**: If you've modified any paths or filenames during previous steps, ensure those changes are reflected in your tracking file path below. The evaluation process relies on consistent tracking information across all notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5d9cc0-a413-46f8-8cb0-ae444cce95e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_clean_name</th>\n",
       "      <th>text_prompt</th>\n",
       "      <th>region</th>\n",
       "      <th>inference_profile</th>\n",
       "      <th>latency_evaluation_output</th>\n",
       "      <th>quality_evaluation_input</th>\n",
       "      <th>quality_evaluation_jobArn</th>\n",
       "      <th>quality_evaluation_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>source_model</td>\n",
       "      <td>source_model</td>\n",
       "      <td>\\nFirst, please read the article below.\\n{cont...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>../outputs/</td>\n",
       "      <td>s3://genai-evaluation-migration-bucket-3397128...</td>\n",
       "      <td>arn:aws:bedrock:us-east-1:339712833052:evaluat...</td>\n",
       "      <td>s3://genai-evaluation-migration-bucket-3397128...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon.nova-lite-v1:0</td>\n",
       "      <td>amazon.nova-lite-v1-0</td>\n",
       "      <td>## Instruction\\nYour task is to read the given...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>../outputs/</td>\n",
       "      <td>s3://genai-evaluation-migration-bucket-3397128...</td>\n",
       "      <td>arn:aws:bedrock:us-east-1:339712833052:evaluat...</td>\n",
       "      <td>s3://genai-evaluation-migration-bucket-3397128...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1:0</td>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1-0</td>\n",
       "      <td>&lt;task&gt;\\nYour task is to provide an extremely c...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>../outputs/</td>\n",
       "      <td>s3://genai-evaluation-migration-bucket-3397128...</td>\n",
       "      <td>arn:aws:bedrock:us-east-1:339712833052:evaluat...</td>\n",
       "      <td>s3://genai-evaluation-migration-bucket-3397128...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model  \\\n",
       "0                                 source_model   \n",
       "1                        amazon.nova-lite-v1:0   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1:0   \n",
       "\n",
       "                              model_clean_name  \\\n",
       "0                                 source_model   \n",
       "1                        amazon.nova-lite-v1-0   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1-0   \n",
       "\n",
       "                                         text_prompt     region  \\\n",
       "0  \\nFirst, please read the article below.\\n{cont...  us-east-1   \n",
       "1  ## Instruction\\nYour task is to read the given...  us-east-1   \n",
       "2  <task>\\nYour task is to provide an extremely c...  us-east-1   \n",
       "\n",
       "  inference_profile latency_evaluation_output  \\\n",
       "0          standard               ../outputs/   \n",
       "1          standard               ../outputs/   \n",
       "2          standard               ../outputs/   \n",
       "\n",
       "                            quality_evaluation_input  \\\n",
       "0  s3://genai-evaluation-migration-bucket-3397128...   \n",
       "1  s3://genai-evaluation-migration-bucket-3397128...   \n",
       "2  s3://genai-evaluation-migration-bucket-3397128...   \n",
       "\n",
       "                           quality_evaluation_jobArn  \\\n",
       "0  arn:aws:bedrock:us-east-1:339712833052:evaluat...   \n",
       "1  arn:aws:bedrock:us-east-1:339712833052:evaluat...   \n",
       "2  arn:aws:bedrock:us-east-1:339712833052:evaluat...   \n",
       "\n",
       "                           quality_evaluation_output  \n",
       "0  s3://genai-evaluation-migration-bucket-3397128...  \n",
       "1  s3://genai-evaluation-migration-bucket-3397128...  \n",
       "2  s3://genai-evaluation-migration-bucket-3397128...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### load our tracking df\n",
    "\n",
    "evaluation_tracking_file = '../data/evaluation_tracking.csv'\n",
    "evaluation_tracking = pd.read_csv(evaluation_tracking_file)\n",
    "display(evaluation_tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8918a183-a7c0-4536-8c8f-93457465c0a5",
   "metadata": {},
   "source": [
    "## Cost Analysis Preparation\n",
    "\n",
    "### Calculating Per-Request and Projected Costs\n",
    "\n",
    "While we're waiting for our LLM-as-a-Judge quality evaluation jobs to complete, we can analyze the economic impact of each model option. Cost considerations are crucial for production deployments, as even small per-request differences can result in significant operational expense at scale.\n",
    "\n",
    "Our cost analysis includes two key components:\n",
    "\n",
    "1. **Per-Inference Cost**: Calculated based on token usage from our latency evaluation\n",
    "cost_per_inference = (input_tokens × input_token_price) + (output_tokens × output_token_price)\n",
    "This cost information will be crucial when making the final model selection, allowing us to balance performance and quality against budgetary constraints. In production environments, this analysis can be extended to include projected monthly costs based on expected request volumes.\n",
    "\n",
    "2. **Auxiliary Service Costs**: Additional expenses beyond direct model inference\n",
    "- **LLM-as-a-Judge**: Charged based on evaluator model usage\n",
    "- **Prompt Optimization**: Charged per token for input and optimized prompts\n",
    "\n",
    "\n",
    "\n",
    "> **Workshop Note**: AWS Bedrock pricing updates periodically. For the most current pricing, refer to the [Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e6223a-0343-4d26-ab1d-c14bf236c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218970a2-e42e-4def-b4bd-464465915484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document_summarization_us.anthropic.claude-3-5-haiku-20241022-v1:0_20250624_003534.csv...\n",
      "Total rows: 10\n",
      "\n",
      "Processing document_summarization_source_model.csv...\n",
      "Total rows: 73\n",
      "\n",
      "Processing document_summarization_amazon.nova-lite-v1:0_20250624_003419.csv...\n",
      "Total rows: 10\n"
     ]
    }
   ],
   "source": [
    "calculator = pricing.PriceCalculator()\n",
    "\n",
    "# Find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(directory, \"document_summarization_*.csv\"))\n",
    "\n",
    "# Process each file\n",
    "for filename in all_files:\n",
    "\n",
    "    \n",
    "    print(f\"\\nProcessing {os.path.basename(filename)}...\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    document_summarization_df = pd.read_csv(filename)\n",
    "\n",
    "\n",
    "    model_id = document_summarization_df[\"model\"][0]  ## change the name back to match the pricing config\n",
    "    #print(model_id)\n",
    "    \n",
    "    print(f\"Total rows: {len(document_summarization_df)}\")\n",
    "    \n",
    "\n",
    "    # Calculate input costs for all rows at once\n",
    "    input_costs = document_summarization_df[\"model_input_tokens\"].apply(\n",
    "        lambda tokens: calculator.calculate_input_price(tokens, model_id)\n",
    "    )\n",
    "\n",
    "    # Calculate output costs for all rows at once\n",
    "    output_costs = document_summarization_df[\"model_output_tokens\"].apply(\n",
    "        lambda tokens: calculator.calculate_output_price(tokens, model_id)\n",
    "    )\n",
    "    \n",
    "    # Calculate total costs\n",
    "    document_summarization_df[\"cost\"] = (input_costs + output_costs).round(6)\n",
    "\n",
    "\n",
    "    # Write back to the same file\n",
    "    document_summarization_df.to_csv(filename, index=False)\n",
    "    #print(f\"Updated and saved {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bed098-a06b-462d-a18d-cf347683fae9",
   "metadata": {},
   "source": [
    "## Prompt Optimization Economics\n",
    "\n",
    "### Calculating the Cost of Automated Prompt Improvement\n",
    "\n",
    "In addition to model inference and quality evaluation, our migration process leverages Amazon Bedrock's Prompt Optimization service to improve prompt effectiveness across models. Like other AI services, this capability has its own pricing structure that should be considered in your total migration cost.\n",
    "\n",
    "#### Understanding Prompt Optimization Pricing\n",
    "\n",
    "Bedrock charges for prompt optimization based on token volume:\n",
    "\n",
    "- **Rate**: \\$0.030 per 1,000 tokens\n",
    "- **Counted Tokens**: Both input prompts and optimized output prompts\n",
    "- **Billing Cycle**: Monthly, based on total token usage\n",
    "\n",
    "This pricing model means that costs scale with both the size of your prompts and the number of optimizations you perform. For most migration scenarios, this represents a small one-time cost, but it's still valuable to estimate for complete budget planning.\n",
    "\n",
    "#### Calculation Example\n",
    "\n",
    "To illustrate how prompt optimization costs work in practice, consider this example:\n",
    "\n",
    "An application developer optimizes a news summarization prompt originally written for Claude 3.5:\n",
    "- Original prompt: 429 tokens\n",
    "- Optimized prompt for Claude 3.5: 511 tokens\n",
    "- This optimized prompt is then used as input to generate variants for:\n",
    "  - Claude 3.7: 582 tokens\n",
    "  - Nova Pro: 579 tokens\n",
    "\n",
    "**Token calculation**:\n",
    "- Input tokens: 429 + 511 + 511 = 1,451 tokens\n",
    "- Output tokens: 511 + 582 + 579 = 1,672 tokens\n",
    "- Total tokens: 3,123 tokens\n",
    "\n",
    "**Cost calculation**:\n",
    "3,123 tokens ÷ 1,000 × \\$0.03 = \\$0.09\n",
    "\n",
    "For our workshop scenario, prompt optimization represents a minimal expense compared to ongoing inference costs, but tracking it provides a complete picture of migration economics.\n",
    "\n",
    "> **Workshop Note**: In production systems, prompt optimization might be performed periodically as models evolve or requirements change. While the per-optimization cost is low, enterprises with many different prompts should account for this in their operational budgets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1154f17-1628-43c3-bd9e-769eae83f167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost for prompt optimization: $0.0134025\n"
     ]
    }
   ],
   "source": [
    "### total tokens for prompt optimization:\n",
    "\n",
    "input_prompt_len = 0\n",
    "optimized_prompts = []\n",
    "for index, evaluation in evaluation_tracking.iterrows():\n",
    "    model_id = evaluation['model']\n",
    "    if model_id == \"source_model\":\n",
    "        input_prompt_len = len(evaluation['text_prompt'])\n",
    "    else:\n",
    "        optimized_prompts.append(evaluation['text_prompt']) \n",
    "\n",
    "total_prompt_len = sum(len(prompt) for prompt in optimized_prompts) + input_prompt_len * len(optimized_prompts)\n",
    "\n",
    "prompt_optimization_cost = total_prompt_len/4/1000 * 0.03\n",
    "\n",
    "print(f\"Estimated cost for prompt optimization: ${prompt_optimization_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb2bc2-a2e7-4320-bfab-b84aeba09e75",
   "metadata": {},
   "source": [
    "## Understanding LLM-as-a-Judge Costs\n",
    "\n",
    "### Breaking Down Evaluation Economics\n",
    "\n",
    "Beyond the direct cost of model inference, it's important to account for the expense of quality evaluation itself. LLM-as-a-Judge is a powerful evaluation method, but it also has associated costs that should be factored into your migration planning.\n",
    "\n",
    "#### Pricing Components\n",
    "\n",
    "Bedrock charges for LLM-as-a-Judge evaluations based on the following components:\n",
    "\n",
    "1. **Model Inference**: The primary cost is for the evaluator model's usage\n",
    "   - Automatically-generated algorithmic scores are provided without additional charges\n",
    "   - For human-based evaluation with your own workstream, there's a \\$0.21 charge per completed human task\n",
    "\n",
    "2. **Token Consumption**: Each evaluation involves several components:\n",
    "   - **Judge Prompts**: Each metric/evaluator uses its own [specialized prompt](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-judge-prompt-nova.html) (~300 tokens per metric)\n",
    "   - **Input Content**: The original prompts and model responses being evaluated\n",
    "   - **Output Results**: JSON output with evaluation scores (~20 tokens per metric)\n",
    "\n",
    "#### Cost Calculation Formula\n",
    "\n",
    "For accurate budgeting, we can estimate evaluation costs using this formula for each metric:\n",
    "\n",
    "Evaluation Cost = [((Tokens in prompts) + (Tokens in responses) + (Judge prompt tokens))/1000 × (Evaluator input token price)] + [(Output tokens)/1000 × (Evaluator output token price)]\n",
    "\n",
    "\n",
    "Where:\n",
    "- Input tokens = Number of tokens in your prompts + responses + judge prompts (typically ~300 tokens)\n",
    "- Output tokens = Number of tokens in the evaluator's output (typically ~20 tokens per metric)\n",
    "\n",
    "This detailed understanding of evaluation costs helps build a complete economic picture when comparing different model options and planning for ongoing quality assessment in production.\n",
    "\n",
    "> **Workshop Note**: When designing your evaluation strategy, consider the trade-off between comprehensive evaluation (using many metrics) and cost efficiency. For routine evaluations, you might select a smaller subset of critical metrics to control costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec26179-d983-4ba7-bfe3-e0d9a119a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator model input price: $0.0008; output price: $0.0002\n"
     ]
    }
   ],
   "source": [
    "evaluator_id = \"amazon.nova-pro-v1:0\"\n",
    "\n",
    "evaluator_input_price = calculator.model_input_token_prices.get(evaluator_id)\n",
    "evaluator_output_price = calculator.model_output_token_prices.get(evaluator_id)\n",
    "\n",
    "print(f\"Evaluator model input price: ${evaluator_input_price}; output price: ${evaluator_output_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25dc479-8728-4cfc-937d-3c8ea4b09fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost per metric to evaluate ../outputs/quality_evaluation.source_model.jsonl: $0.007137\n",
      "\n",
      "Estimated cost per metric to evaluate ../outputs/quality_evaluation.us.anthropic.claude-3-5-haiku-20241022-v1:0.jsonl: $0.013481\n",
      "\n",
      "Estimated cost per metric to evaluate ../outputs/quality_evaluation.amazon.nova-lite-v1:0.jsonl: $0.013224\n",
      "\n",
      "Estimated total cost to evaluate: $0.101527\n"
     ]
    }
   ],
   "source": [
    "### Double check your evaluation metrics. By default our workshop evaluate 3 metrics: \"Builtin.Correctness\", \"Builtin.Completeness\",  \"Builtin.ProfessionalStyleAndTone\"\n",
    "\n",
    "\n",
    "# Find all matching json files\n",
    "quality_evaluation_inputs = glob.glob(os.path.join(directory, \"quality_evaluation*\"))\n",
    "\n",
    "# Process each file\n",
    "total_quality_evaluation_cost = 0\n",
    "\n",
    "for filename in quality_evaluation_inputs:\n",
    "    #print(filename)    \n",
    "\n",
    "    evaluation_price = 0\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            prompt_length = len(data[\"prompt\"])\n",
    "            reference_length = len(data[\"referenceResponse\"])\n",
    "            model_response_length = len(data[\"modelResponses\"][0][\"response\"])\n",
    "            \n",
    "            # Calculate tokens according to the formula\n",
    "            input_tokens = (prompt_length + reference_length + model_response_length) / 4 + 300 ## 300 is a rough estimation, see details in above\n",
    "            output_tokens = 20 ## it is a rough estimation\n",
    "            \n",
    "            # Calculate price for this entry\n",
    "            entry_price = (input_tokens/1000 * evaluator_input_price) + (output_tokens/1000 * evaluator_output_price)\n",
    "            evaluation_price += entry_price\n",
    "            \n",
    "            #print(f\"Entry: Input tokens = {input_tokens:.2f}, Output tokens = {output_tokens}\")\n",
    "    \n",
    "    print(f\"Estimated cost per metric to evaluate {filename}: ${evaluation_price:.6f}\\n\")\n",
    "    total_quality_evaluation_cost +=  evaluation_price\n",
    "\n",
    "\n",
    "## We evaluated 3 metrics: \"Builtin.Correctness\", \"Builtin.Completeness\",\"Builtin.ProfessionalStyleAndTone\"\n",
    "total_quality_evaluation_cost = total_quality_evaluation_cost * 3\n",
    "\n",
    "print(f\"Estimated total cost to evaluate: ${total_quality_evaluation_cost:.6f}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a63024-8f5b-42f8-b83e-985a06a055f5",
   "metadata": {},
   "source": [
    "## Consolidating Latency and Inference Cost Metrics\n",
    "\n",
    "In this section, we'll import and process the latency data collected during Step 3. This data is crucial for understanding the performance characteristics of each model, particularly for applications with real-time requirements or interactive user experiences.\n",
    "\n",
    "By comparing these metrics across models, we can identify performance differences that might impact user experience in production environments. This is especially important for applications with strict latency requirements or those serving large numbers of concurrent users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1abeeb8-862d-478b-8b22-e0abcb52b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_latency_evaluation_files(directory):\n",
    "    \"\"\"Combine all CSV files in the directory into a single DataFrame.\"\"\"\n",
    "    all_files = glob.glob(os.path.join(directory, \"document_summarization_*.csv\"))\n",
    "    df_list = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename)\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "def calculate_metrics(df, group_columns):\n",
    "    \"\"\"Calculate latency metrics grouped by model, region, and inference profile.\"\"\"\n",
    "    metrics = df.groupby(group_columns).agg({\n",
    "        'model_input_tokens': ['count', 'mean'],\n",
    "        'model_output_tokens': ['mean'],\n",
    "        'cost': ['mean'],\n",
    "        'latency': ['mean', 'median', \n",
    "                             lambda x: x.quantile(0.9),lambda x: x.std()],\n",
    "        'model_latencyMs': ['mean', 'median', \n",
    "                             lambda x: x.quantile(0.9),lambda x: x.std()]\n",
    "    }).round(6)\n",
    "\n",
    "    metrics.columns = ['sample_size', \n",
    "                      'avg_input_tokens',\n",
    "                      'avg_output_tokens',\n",
    "                       'avg_cost',\n",
    "                       'latency_mean', 'latency_p50', 'latency_p90', 'latency_std',\n",
    "                      'model_latencyMs_mean', 'model_latencyMs_p50', 'model_latencyMs_p90', 'model_latencyMs_std']\n",
    "    \n",
    "    metrics = metrics.reset_index()\n",
    "    #print(metrics)\n",
    "    \n",
    "    ### add the quality metrics\n",
    "    for model in metrics[\"model\"]:\n",
    "        print(model)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b874e68-df91-41fb-b7fa-b760ace8d146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon.nova-lite-v1:0\n",
      "source_model\n",
      "us.anthropic.claude-3-5-haiku-20241022-v1:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>region</th>\n",
       "      <th>inference_profile</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>avg_input_tokens</th>\n",
       "      <th>avg_output_tokens</th>\n",
       "      <th>avg_cost</th>\n",
       "      <th>latency_mean</th>\n",
       "      <th>latency_p50</th>\n",
       "      <th>latency_p90</th>\n",
       "      <th>latency_std</th>\n",
       "      <th>model_latencyMs_mean</th>\n",
       "      <th>model_latencyMs_p50</th>\n",
       "      <th>model_latencyMs_p90</th>\n",
       "      <th>model_latencyMs_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon.nova-lite-v1:0</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>10</td>\n",
       "      <td>1127.400000</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.49000</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.122384</td>\n",
       "      <td>479.500000</td>\n",
       "      <td>477.5</td>\n",
       "      <td>626.6</td>\n",
       "      <td>123.810114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>source_model</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>73</td>\n",
       "      <td>656.438356</td>\n",
       "      <td>90.493151</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>1.65863</td>\n",
       "      <td>1.550</td>\n",
       "      <td>2.178</td>\n",
       "      <td>0.497715</td>\n",
       "      <td>1518.643836</td>\n",
       "      <td>1469.0</td>\n",
       "      <td>2031.4</td>\n",
       "      <td>418.496328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1:0</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>10</td>\n",
       "      <td>1277.600000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>2.49600</td>\n",
       "      <td>2.405</td>\n",
       "      <td>3.151</td>\n",
       "      <td>0.681929</td>\n",
       "      <td>2486.700000</td>\n",
       "      <td>2396.5</td>\n",
       "      <td>3148.1</td>\n",
       "      <td>682.633308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model     region inference_profile  \\\n",
       "0                        amazon.nova-lite-v1:0  us-east-1          standard   \n",
       "1                                 source_model  us-east-1          standard   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1:0  us-east-1          standard   \n",
       "\n",
       "   sample_size  avg_input_tokens  avg_output_tokens  avg_cost  latency_mean  \\\n",
       "0           10       1127.400000          43.800000  0.000078       0.49000   \n",
       "1           73        656.438356          90.493151  0.000436       1.65863   \n",
       "2           10       1277.600000          73.000000  0.001314       2.49600   \n",
       "\n",
       "   latency_p50  latency_p90  latency_std  model_latencyMs_mean  \\\n",
       "0        0.485        0.640     0.122384            479.500000   \n",
       "1        1.550        2.178     0.497715           1518.643836   \n",
       "2        2.405        3.151     0.681929           2486.700000   \n",
       "\n",
       "   model_latencyMs_p50  model_latencyMs_p90  model_latencyMs_std  \n",
       "0                477.5                626.6           123.810114  \n",
       "1               1469.0               2031.4           418.496328  \n",
       "2               2396.5               3148.1           682.633308  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "latency_evaluation_raw = combine_latency_evaluation_files(directory)\n",
    "metrics = calculate_metrics(latency_evaluation_raw, ['model', 'region', 'inference_profile'])\n",
    "\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd9b2d-31f0-4df3-9e88-76ed0ea95654",
   "metadata": {},
   "source": [
    "## Consolidating Quality Metrics Integration\n",
    "\n",
    "With our latency and cost metrics prepared, we now need to incorporate the quality evaluation results from our LLM-as-a-Judge jobs. Before we can analyze these results, we need to:\n",
    "1. Verify that all evaluation jobs have completed\n",
    "2. Locate the output files in our S3 bucket\n",
    "3. Extract and parse the evaluation scores for each model\n",
    "\n",
    "This quality data completes our three-dimensional view of model performance (latency, cost, and quality), enabling truly informed selection decisions.\n",
    "\n",
    "> <span style=\"color:red\">**Workshop Note**: Evaluation jobs may take several minutes to complete. If your jobs are still running, you can monitor their status in the AWS console or wait for completion before proceeding.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3397d616-d1d9-4446-85a6-f2298bec7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client('bedrock')\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de717db7-cf7c-48e2-b761-e205f7da9adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_model: Completed\n",
      "amazon.nova-lite-v1:0: Completed\n",
      "us.anthropic.claude-3-5-haiku-20241022-v1:0: Completed\n",
      "\n",
      "Automatically detected S3 output keys:\n",
      "  model: ['source_model', 'amazon.nova-lite-v1:0', 'us.anthropic.claude-3-5-haiku-20241022-v1:0']\n",
      "  key: ['genai_migration/llmaaj-source-model-amazon-2025-06-24-00-36-39/bcly6857m113/models/source_model/taskTypes/General/datasets/CustomDataset/30cc6f66-297c-4afd-afe2-b2864f3a1eb8_output.jsonl', 'genai_migration/llmaaj-amazon-amazon-2025-06-24-00-36-41/3zkkftou112q/models/amazon.nova-lite-v1-0/taskTypes/General/datasets/CustomDataset/422f0852-53f3-4bea-aa1e-a655f633a8e5_output.jsonl', 'genai_migration/llmaaj-us-amazon-2025-06-24-00-36-43/z33p2of2ey1o/models/us.anthropic.claude-3-5-haiku-20241022-v1-0/taskTypes/General/datasets/CustomDataset/90e82d7a-95c4-4b5c-a35d-c1d52ce2a692_output.jsonl']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_s3_output_keys(evaluation_tracking):\n",
    "    # Initialize result structure\n",
    "    result = {\n",
    "        \"model\": [],\n",
    "        \"key\": []\n",
    "    }\n",
    "    \n",
    "    for index, evaluation in evaluation_tracking.iterrows():\n",
    "        model_id = evaluation['model']\n",
    "        evaluation_job_arn = evaluation['quality_evaluation_jobArn']\n",
    "\n",
    "        # Check job status\n",
    "        check_status = bedrock_client.get_evaluation_job(jobIdentifier=evaluation_job_arn)\n",
    "        print(f\"{model_id}: {check_status['status']}\")\n",
    "        \n",
    "        if check_status['status'] == \"Completed\":\n",
    "            output_path = evaluation['quality_evaluation_output']\n",
    "            try:\n",
    "                response = s3_client.list_objects_v2(\n",
    "                    Bucket=BUCKET_NAME,\n",
    "                    Prefix=PREFIX\n",
    "                )\n",
    "\n",
    "                # Find the JSONL output file for this model\n",
    "                for obj in response.get('Contents', []):\n",
    "                    key = obj['Key']\n",
    "                    # Add model identifier check\n",
    "                    if key.endswith('_output.jsonl') and model_id.replace(':', '-') in key:\n",
    "                        result[\"model\"].append(model_id)\n",
    "                        result[\"key\"].append(key)\n",
    "                        break\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error listing objects for {model_id}: {str(e)}\")\n",
    "        else:\n",
    "            print(\"\\x1b[31mQuality evaluation job is still in progress, please wait..\\x1b[0m\")\n",
    "            sys.exit()\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage\n",
    "s3_output_keys = get_s3_output_keys(evaluation_tracking)\n",
    "\n",
    "# Now s3_output_keys contains a dictionary mapping model IDs to their S3 output keys\n",
    "print(\"\\nAutomatically detected S3 output keys:\")\n",
    "for model, key in s3_output_keys.items():\n",
    "    print(f\"  {model}: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef376325-d56e-4776-b66a-ac5c205c9c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         model  Builtin.Correctness  \\\n",
      "0                                 source_model                 0.95   \n",
      "1                        amazon.nova-lite-v1:0                 1.00   \n",
      "2  us.anthropic.claude-3-5-haiku-20241022-v1:0                 1.00   \n",
      "\n",
      "   Builtin.Completeness  Builtin.ProfessionalStyleAndTone  \n",
      "0                 0.925                               1.0  \n",
      "1                 0.900                               1.0  \n",
      "2                 1.000                               1.0  \n"
     ]
    }
   ],
   "source": [
    "### quality metrics\n",
    "\n",
    "file_key_df = pd.DataFrame(s3_output_keys)\n",
    "model_quality_list = []\n",
    "\n",
    "# print(file_key_df)\n",
    "\n",
    "for index, row in file_key_df.iterrows():  \n",
    "    metrics_dict = {}\n",
    "\n",
    "    model = row[\"model\"]\n",
    "    if row[\"key\"] == \"\":\n",
    "        continue\n",
    "    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=row[\"key\"])\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    for line in content.strip().split('\\n'):\n",
    "        if line:\n",
    "            data = json.loads(line)\n",
    "            if 'automatedEvaluationResult' in data and 'scores' in data['automatedEvaluationResult']:\n",
    "                for score in data['automatedEvaluationResult']['scores']:\n",
    "                    metric_name = score['metricName']\n",
    "                    if 'result' in score:\n",
    "                        metric_value = score['result']\n",
    "                        if metric_name not in metrics_dict:\n",
    "                            metrics_dict[metric_name] = []\n",
    "                        metrics_dict[metric_name].append(metric_value)\n",
    "    \n",
    "    df = pd.DataFrame(metrics_dict)\n",
    "    df['model'] = model\n",
    "    model_quality_average = df.groupby(\"model\").mean()\n",
    "    model_quality_average = model_quality_average.reset_index()\n",
    "    model_quality_list.append(model_quality_average)\n",
    "\n",
    "\n",
    "model_quality = pd.concat(model_quality_list, axis=0, ignore_index=True)\n",
    "print(model_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87813056-46a0-4b1b-827e-ad45f9a1d6d5",
   "metadata": {},
   "source": [
    ">**Workshop Note**: These evaluation results may not be able to differentiate enough between the models. That's because we have a really small sample size (n=10), for the sake of time. In real-world implementation, you will need to increase the sample size to accurately measure the quality of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419b481-b554-42b7-b90c-3a6f8b8a2545",
   "metadata": {},
   "source": [
    "## Consolidated Metrics Analysis\n",
    "\n",
    "### Combining Performance, Quality, and Cost Data\n",
    "\n",
    "With our quality metrics now added to our performance and cost data, we have a complete view of each model's capabilities. This consolidated metrics framework lets us:\n",
    "\n",
    "1. **Compare trade-offs**: See how models balance speed, quality, and cost\n",
    "2. **Identify strengths**: Determine which models excel in specific dimensions\n",
    "3. **Match to requirements**: Align capabilities with application-specific priorities\n",
    "\n",
    "This multidimensional analysis is essential for making informed decisions that go beyond simplistic \"best model\" thinking. Different applications have unique requirements - a customer service chatbot might prioritize response quality and tone, while a high-volume processing application might favor speed and cost efficiency.\n",
    "\n",
    "The merged metrics dataframe we've created serves as the foundation for our visualizations and final report, providing a clear picture of the relative advantages of each model option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1512eb0a-7823-4f14-8282-2e34cc8ea5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>region</th>\n",
       "      <th>inference_profile</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>avg_input_tokens</th>\n",
       "      <th>avg_output_tokens</th>\n",
       "      <th>avg_cost</th>\n",
       "      <th>latency_mean</th>\n",
       "      <th>latency_p50</th>\n",
       "      <th>latency_p90</th>\n",
       "      <th>latency_std</th>\n",
       "      <th>model_latencyMs_mean</th>\n",
       "      <th>model_latencyMs_p50</th>\n",
       "      <th>model_latencyMs_p90</th>\n",
       "      <th>model_latencyMs_std</th>\n",
       "      <th>Builtin.Correctness</th>\n",
       "      <th>Builtin.Completeness</th>\n",
       "      <th>Builtin.ProfessionalStyleAndTone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon.nova-lite-v1:0</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>10</td>\n",
       "      <td>1127.400000</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.49000</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.122384</td>\n",
       "      <td>479.500000</td>\n",
       "      <td>477.5</td>\n",
       "      <td>626.6</td>\n",
       "      <td>123.810114</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>source_model</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>73</td>\n",
       "      <td>656.438356</td>\n",
       "      <td>90.493151</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>1.65863</td>\n",
       "      <td>1.550</td>\n",
       "      <td>2.178</td>\n",
       "      <td>0.497715</td>\n",
       "      <td>1518.643836</td>\n",
       "      <td>1469.0</td>\n",
       "      <td>2031.4</td>\n",
       "      <td>418.496328</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1:0</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>10</td>\n",
       "      <td>1277.600000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>2.49600</td>\n",
       "      <td>2.405</td>\n",
       "      <td>3.151</td>\n",
       "      <td>0.681929</td>\n",
       "      <td>2486.700000</td>\n",
       "      <td>2396.5</td>\n",
       "      <td>3148.1</td>\n",
       "      <td>682.633308</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model     region inference_profile  \\\n",
       "0                        amazon.nova-lite-v1:0  us-east-1          standard   \n",
       "1                                 source_model  us-east-1          standard   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1:0  us-east-1          standard   \n",
       "\n",
       "   sample_size  avg_input_tokens  avg_output_tokens  avg_cost  latency_mean  \\\n",
       "0           10       1127.400000          43.800000  0.000078       0.49000   \n",
       "1           73        656.438356          90.493151  0.000436       1.65863   \n",
       "2           10       1277.600000          73.000000  0.001314       2.49600   \n",
       "\n",
       "   latency_p50  latency_p90  latency_std  model_latencyMs_mean  \\\n",
       "0        0.485        0.640     0.122384            479.500000   \n",
       "1        1.550        2.178     0.497715           1518.643836   \n",
       "2        2.405        3.151     0.681929           2486.700000   \n",
       "\n",
       "   model_latencyMs_p50  model_latencyMs_p90  model_latencyMs_std  \\\n",
       "0                477.5                626.6           123.810114   \n",
       "1               1469.0               2031.4           418.496328   \n",
       "2               2396.5               3148.1           682.633308   \n",
       "\n",
       "   Builtin.Correctness  Builtin.Completeness  Builtin.ProfessionalStyleAndTone  \n",
       "0                 1.00                 0.900                               1.0  \n",
       "1                 0.95                 0.925                               1.0  \n",
       "2                 1.00                 1.000                               1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## merge metrics to  metrics df\n",
    "metrics = pd.merge(metrics, model_quality, on=['model'])\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85efd494-b946-48d7-8eb0-fc79c1f69bc1",
   "metadata": {},
   "source": [
    "## AI-Enhanced Analysis Summary\n",
    "\n",
    "### Using LLMs to Interpret Evaluation Results\n",
    "\n",
    "One of the powerful capabilities of advanced LLMs is their ability to analyze complex data and generate insightful summaries. In this section, we'll leverage this capability by asking Claude Haiku to interpret our evaluation results and provide a concise summary of the key findings.\n",
    "\n",
    "This approach demonstrates an important pattern for working with foundation models: using them not just as content generators but as analytical tools that can extract insights from structured data. \n",
    "\n",
    "The resulting summary provides a human-readable interpretation of our metrics, complementing our visualizations and raw data with narrative insights.\n",
    "\n",
    "> **Workshop Note**: This pattern of \"LLM as data analyst\" can be applied to many business intelligence scenarios beyond model evaluation. Consider how your organization might leverage foundation models to generate insights from other complex datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92385c91-8e94-4d8a-91f1-dc5945f426b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = boto3.client(\"bedrock-runtime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4953abd2-89ef-4de2-981e-33dc79f88e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For latency metrics, Amazon Nova Lite emerges as the fastest model with the lowest mean latency (0.49s), p50 (0.485s), and p90 (0.640s) times. In terms of cost-efficiency, Amazon Nova Lite offers the most economical solution with the lowest average cost of $0.000078 per inference. Regarding output quality, both Amazon Nova Lite and Claude 3.5 Haiku achieve perfect correctness (1.00), but Claude 3.5 Haiku demonstrates superior output completeness with a 1.000 score compared to Nova Lite's 0.900.\n"
     ]
    }
   ],
   "source": [
    "## generate an analysis summary\n",
    "\n",
    "summary_prompt = \"\"\"Using the dataset provided below, create a concise 3-sentence summary that identifies which model performs best in each of these three metric categories:\n",
    "1. Latency metrics (latency_mean, latency_p50, latency_p90)\n",
    "2. Cost metrics (avg_cost)\n",
    "3. Quality metrics (Builtin.Correctness and output completeness as suggested by avg_output_tokens)\n",
    "4. Don't start with  \"Based on the provided...\", just give your summary\n",
    "The summary should clearly state which model is optimal for users prioritizing speed, cost-efficiency, or output quality. \n",
    "\n",
    "Dataset: {dataset}\n",
    "\"\"\"\n",
    "\n",
    "max_tokens=1000\n",
    "temperature=0\n",
    "top_p=0.9\n",
    "\n",
    "response = client.converse(\n",
    "    modelId=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": summary_prompt.format(dataset=metrics)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    inferenceConfig={\n",
    "        \"temperature\": temperature,\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"topP\": top_p\n",
    "    }\n",
    ")\n",
    "\n",
    "analysis_summary = response['output']['message']['content'][0]['text']\n",
    "\n",
    "print(analysis_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e756123-3caa-4ee4-805d-f28f574af72c",
   "metadata": {},
   "source": [
    "## Final Report Generation\n",
    "\n",
    "### Creating Comprehensive Analysis Documentation\n",
    "\n",
    "The culmination of our evaluation process is a professionally formatted report that presents our findings in a clear, visually appealing format. This report serves multiple purposes:\n",
    "\n",
    "1. **Documentation**: Creates a permanent record of our evaluation methodology and results\n",
    "2. **Communication**: Provides shareable artifacts for stakeholder discussions\n",
    "3. **Decision Support**: Organizes information to facilitate informed choices\n",
    "\n",
    "Our `generate_analysis_report` utility handles the complex work of:\n",
    "- Creating consistent visualizations across dimensions\n",
    "- Formatting tables for readability\n",
    "- Generating performance distribution charts\n",
    "- Including our AI-generated summary alongside raw metrics\n",
    "\n",
    "The final output includes both a PDF report and a CSV summary, providing options for both high-level reviews and detailed analysis.\n",
    "\n",
    "> **Workshop Note**: After running this cell, check the `../outputs-analysis/` directory to view your generated report. This report can serve as a template for your own model evaluation documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0df52a67-3ef5-4bf2-b88a-2bf17d4057b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon.nova-lite-v1:0' 'source_model'\n",
      " 'us.anthropic.claude-3-5-haiku-20241022-v1:0']\n",
      "\n",
      "Analysis complete!\n",
      "PDF report saved to: ../outputs-analysis/Model_evaluation_analysis_report_20250624_013541.pdf\n",
      "CSV summary saved to: ../outputs-analysis/analysis_summary_20250624_013541.csv\n"
     ]
    }
   ],
   "source": [
    "report = generate_analysis_report.Analysis_Report()\n",
    "report.generate_report(latency_evaluation_raw, directory, metrics, analysis_summary, total_quality_evaluation_cost, prompt_optimization_cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608ad27-d240-49bc-b5a8-42984a89f746",
   "metadata": {},
   "source": [
    "## Summary and Takeaways\n",
    "\n",
    "### Congratulations on Completing the Model Evaluation Workshop!\n",
    "\n",
    "You've successfully navigated the entire model evaluation and migration process, gaining hands-on experience with a methodology that can be applied to your own GenAI projects. In this final notebook, you've:\n",
    "\n",
    "1. ✅ **Consolidated multi-dimensional metrics** across latency, quality, and cost\n",
    "2. ✅ **Calculated economic implications** of different model choices at scale  \n",
    "3. ✅ **Analyzed quality assessments** from LLM-as-a-Judge evaluations\n",
    "4. ✅ **Generated AI-enhanced insights** to interpret complex evaluation data\n",
    "5. ✅ **Created professional documentation** to support decision-making\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Model Selection is Multi-Dimensional**: There's rarely a single \"best\" model - different models excel in different areas, and the optimal choice depends on your specific requirements.\n",
    "2. **Data-Driven Migration**: Successful model migrations require objective measurements rather than assumptions or specifications alone.\n",
    "3. **Trade-off Analysis**: Understanding the relationship between speed, quality, and cost allows for informed decisions that balance competing priorities.\n",
    "4. **Documentation Matters**: Thorough documentation of your evaluation methodology and results helps build consensus and supports future migration decisions.\n",
    "5. **Continuous Evaluation**: As new models are released and existing ones are updated, the evaluation process should be repeated periodically to ensure you're using optimal components.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Consider applying this evaluation framework to your own use cases:\n",
    "- **Customize success criteria**: Define specific thresholds for acceptance based on your application requirements\n",
    "- **Widen the comparison**: Evaluate more models across different providers and architectures\n",
    "- **Expand the metrics**: Add domain-specific evaluation criteria relevant to your applications\n",
    "- **Continue prompt optimization**: Remember that prompt engineering is an ongoing effort - consider building a systematic pipeline for testing prompt variations with automated metrics or human-in-the-loop evaluation\n",
    "- **Explore more Inference options**: Learn and explore more inference optimization options in Bedrock that may improve latency for your use cases, e.g.: [Optimized Inference](https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html), [Prompt Caching](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html), [Inference Profile](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles.html) and others.\n",
    "- **Build RAG evaluation component**: If your use case requires a knowledge base built with RAG, consider adding a quality evaluation component for RAG. RAG evaluation options include [Bedrock RAG evaluator](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-kb.html), open source solutions like [RAGAS](https://docs.ragas.io/en/stable/getstarted/rag_eval/). \n",
    "- **Automate the workflow**: Integrate these evaluation steps into your CI/CD pipeline for ongoing model assessment\n",
    "- **Establish feedback loops**: Create mechanisms to capture production performance data to inform future prompt and model optimizations\n",
    "\n",
    "Thank you for participating in this workshop! We hope the skills and methodology you've learned will help you make confident, data-driven decisions about model selection and migration in your GenAI journey.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
