{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **Passo 5: Compara√ß√£o Final e Decis√£o**\n",
       "\n",
       "## **Aula 5.1: O Momento da Verdade**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© compara√ß√£o final?**\n",
       "\n",
       "Imagina que voc√™ t√° comprando um carro e testou tr√™s op√ß√µes diferentes. Agora chegou a hora de reunir todas as informa√ß√µes: qual √© mais r√°pido, qual √© mais econ√¥mico, qual √© mais confi√°vel, e qual oferece o melhor custo-benef√≠cio. √â exatamente isso que vamos fazer aqui - s√≥ que em vez de carros, s√£o modelos de IA! üòÑ\n",
       "\n",
       "**Por que compara√ß√£o final √© importante?**\n",
       "\n",
       "At√© agora medimos velocidade, qualidade e custo separadamente. Mas na vida real, voc√™ precisa tomar uma decis√£o baseada em TUDO junto. √â como escolher um restaurante: n√£o √© s√≥ o pre√ßo, n√£o √© s√≥ a velocidade, n√£o √© s√≥ a qualidade - √© a combina√ß√£o de tudo!\n",
       "\n",
       "### **O Que Voc√™ Vai Ganhar Desta An√°lise**\n",
       "\n",
       "Este notebook produz dois artefatos chave:\n",
       "\n",
       "1. **Relat√≥rio PDF de An√°lise**: Um documento abrangente com visualiza√ß√µes comparando modelos em todas as dimens√µes\n",
       "2. **Resumo CSV**: Dados brutos pra an√°lise adicional ou integra√ß√£o com outras m√©tricas de neg√≥cio\n",
       "\n",
       "Vamos come√ßar consolidando nossos resultados de avalia√ß√£o!\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um gr√°fico de radar mostrando m√∫ltiplas dimens√µes de performance"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üõ†Ô∏è IMPORTANDO AS FERRAMENTAS NECESS√ÅRIAS\n",
       "import json\n",
       "import boto3\n",
       "import numpy as np\n",
       "from scipy import stats\n",
       "import pandas as pd\n",
       "import glob\n",
       "import os\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from IPython.display import display, HTML\n",
       "from matplotlib.backends.backend_pdf import PdfPages\n",
       "import datetime\n",
       "import sys\n",
       "from IPython.display import display\n",
       "\n",
       "sys.path.append(\"../\")\n",
       "\n",
       "from src import pricing\n",
       "from src import generate_analysis_report\n",
       "\n",
       "print(\"‚úÖ Ferramentas importadas! Vamos fazer a compara√ß√£o final!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ‚öôÔ∏è CONFIGURA√á√ÉO AWS\n",
       "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
       "\n",
       "BUCKET_NAME = f\"genai-evaluation-migration-bucket-{account_id}\"\n",
       "PREFIX = \"genai_migration\"\n",
       "\n",
       "print(f\"üè¢ Account ID: {account_id}\")\n",
       "print(f\"ü™£ Bucket: {BUCKET_NAME}\")\n",
       "print(f\"üìÅ Prefix: {PREFIX}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Carregando Dados de Avalia√ß√£o**\n",
       "\n",
       "#### **Recuperando Nossas Informa√ß√µes de Tracking**\n",
       "\n",
       "Antes de analisar nossos modelos, precisamos acessar as informa√ß√µes de tracking mantidas durante todo o processo de avalia√ß√£o. Carregando essas informa√ß√µes de tracking primeiro, estabelecemos uma base pra nossa an√°lise consolidada e garantimos que estamos trabalhando com metadados consistentes dos modelos durante todo o processo de compara√ß√£o.\n",
       "\n",
       "> **üí° Nota para Aprendizes Autodidatas**: Se voc√™ modificou qualquer caminho ou nome de arquivo durante os passos anteriores, certifique-se de que essas mudan√ßas est√£o refletidas no caminho do arquivo de tracking abaixo. O processo de avalia√ß√£o depende de informa√ß√µes de tracking consistentes em todos os notebooks."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üìä CARREGANDO NOSSO TRACKING\n",
       "evaluation_tracking_file = '../data/evaluation_tracking.csv'\n",
       "evaluation_tracking = pd.read_csv(evaluation_tracking_file)\n",
       "display(evaluation_tracking)\n",
       "\n",
       "print(\"\\nüí° Perfeito! Agora temos nosso plano de avalia√ß√£o carregado.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Prepara√ß√£o da An√°lise de Custos**\n",
       "\n",
       "#### **Calculando Custos por Requisi√ß√£o e Proje√ß√µes**\n",
       "\n",
       "Enquanto estamos aguardando nossos jobs de avalia√ß√£o de qualidade LLM-as-a-Judge completarem, podemos analisar o impacto econ√¥mico de cada op√ß√£o de modelo. Considera√ß√µes de custo s√£o cruciais pra implanta√ß√µes de produ√ß√£o, j√° que mesmo pequenas diferen√ßas por requisi√ß√£o podem resultar em despesas operacionais significativas em escala.\n",
       "\n",
       "Nossa an√°lise de custo inclui dois componentes chave:\n",
       "\n",
       "1. **Custo por Infer√™ncia**: Calculado baseado no uso de tokens da nossa avalia√ß√£o de lat√™ncia\n",
       "   custo_por_inferencia = (tokens_entrada √ó pre√ßo_token_entrada) + (tokens_sa√≠da √ó pre√ßo_token_sa√≠da)\n",
       "   \n",
       "   Essas informa√ß√µes de custo ser√£o cruciais ao fazer a sele√ß√£o final do modelo, permitindo-nos balancear performance e qualidade contra restri√ß√µes or√ßament√°rias. Em ambientes de produ√ß√£o, essa an√°lise pode ser estendida para incluir custos mensais projetados baseados em volumes de requisi√ß√£o esperados.\n",
       "\n",
       "2. **Custos de Servi√ßos Auxiliares**: Despesas adicionais al√©m da infer√™ncia direta do modelo\n",
       "   - **LLM-as-a-Judge**: Cobrado baseado no uso do modelo avaliador\n",
       "   - **Otimiza√ß√£o de Prompt**: Cobrado por token para prompts de entrada e otimizados\n",
       "\n",
       "> **üí° Nota do Workshop**: Os pre√ßos do AWS Bedrock s√£o atualizados periodicamente. Para os pre√ßos mais atuais, consulte a [P√°gina de Pre√ßos do Bedrock](https://aws.amazon.com/bedrock/pricing/)."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ CONFIGURANDO O DIRET√ìRIO\n",
       "directory = \"../outputs\"\n",
       "print(f\"ÔøΩÔøΩ Diret√≥rio de sa√≠da: {directory}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üí∞ CALCULANDO CUSTOS PRA TODOS OS MODELOS\n",
       "calculator = pricing.PriceCalculator()\n",
       "\n",
       "# Encontrando todos os arquivos CSV correspondentes\n",
       "all_files = glob.glob(os.path.join(directory, \"document_summarization_*.csv\"))\n",
       "\n",
       "print(\"üí∞ CALCULANDO CUSTOS...\")\n",
       "print(\"=\" * 40)\n",
       "\n",
       "# Processando cada arquivo\n",
       "for filename in all_files:\n",
       "    print(f\"\\nüìä Processando {os.path.basename(filename)}...\")\n",
       "    \n",
       "    # Lendo o arquivo CSV\n",
       "    document_summarization_df = pd.read_csv(filename)\n",
       "\n",
       "    model_id = document_summarization_df[\"model\"][0]  # Mudando o nome de volta pra combinar com a config de pre√ßos\n",
       "    \n",
       "    print(f\"üéØ Modelo: {model_id}\")\n",
       "    print(f\"ÔøΩÔøΩ Total de linhas: {len(document_summarization_df)}\")\n",
       "    \n",
       "    # Calculando custos de entrada para todas as linhas de uma vez\n",
       "    input_costs = document_summarization_df[\"model_input_tokens\"].apply(\n",
       "        lambda tokens: calculator.calculate_input_price(tokens, model_id)\n",
       "    )\n",
       "\n",
       "    # Calculando custos de sa√≠da para todas as linhas de uma vez\n",
       "    output_costs = document_summarization_df[\"model_output_tokens\"].apply(\n",
       "        lambda tokens: calculator.calculate_output_price(tokens, model_id)\n",
       "    )\n",
       "    \n",
       "    # Calculando custos totais\n",
       "    document_summarization_df[\"cost\"] = (input_costs + output_costs).round(6)\n",
       "\n",
       "    # Escrevendo de volta no mesmo arquivo\n",
       "    document_summarization_df.to_csv(filename, index=False)\n",
       "    print(f\"‚úÖ Custos calculados e salvos!\")\n",
       "    \n",
       "    # Mostrando estat√≠sticas r√°pidas\n",
       "    avg_cost = document_summarization_df[\"cost\"].mean()\n",
       "    total_cost = document_summarization_df[\"cost\"].sum()\n",
       "    print(f\"üí∞ Custo m√©dio por infer√™ncia: ${avg_cost:.6f}\")\n",
       "    print(f\"üí∞ Custo total: ${total_cost:.6f}\")\n",
       "\n",
       "print(\"\\nüéâ C√°lculo de custos conclu√≠do!\")\n",
       "print(\"üí° Agora todos os arquivos t√™m informa√ß√µes de custo inclu√≠das.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Economia da Otimiza√ß√£o de Prompts**\n",
       "\n",
       "#### **Calculando o Custo da Melhoria Autom√°tica de Prompts**\n",
       "\n",
       "Al√©m da infer√™ncia do modelo e avalia√ß√£o de qualidade, nosso processo de migra√ß√£o aproveita o servi√ßo de Otimiza√ß√£o de Prompts do Amazon Bedrock pra melhorar a efic√°cia dos prompts entre modelos. Como outros servi√ßos de IA, essa capacidade tem sua pr√≥pria estrutura de pre√ßos que deve ser considerada no custo total da migra√ß√£o.\n",
       "\n",
       "#### **Entendendo a Precifica√ß√£o da Otimiza√ß√£o de Prompts**\n",
       "\n",
       "O Bedrock cobra pela otimiza√ß√£o de prompts baseado no volume de tokens:\n",
       "\n",
       "- **Taxa**: $0.030 por 1.000 tokens\n",
       "- **Tokens Contados**: Tanto prompts de entrada quanto prompts de sa√≠da otimizados\n",
       "- **Ciclo de Cobran√ßa**: Mensal, baseado no uso total de tokens\n",
       "\n",
       "Esse modelo de pre√ßos significa que os custos escalam com tanto o tamanho dos seus prompts quanto o n√∫mero de otimiza√ß√µes que voc√™ executa. Para a maioria dos cen√°rios de migra√ß√£o, isso representa um pequeno custo √∫nico, mas ainda √© valioso estimar para planejamento or√ßament√°rio completo.\n",
       "\n",
       "#### **Exemplo de C√°lculo**\n",
       "\n",
       "Pra ilustrar como os custos de otimiza√ß√£o de prompts funcionam na pr√°tica, considere este exemplo:\n",
       "\n",
       "Um desenvolvedor de aplica√ß√£o otimiza um prompt de sumariza√ß√£o de not√≠cias originalmente escrito para Claude 3.5:\n",
       "- Prompt original: 429 tokens\n",
       "- Prompt otimizado para Claude 3.5: 511 tokens\n",
       "- Este prompt otimizado √© ent√£o usado como entrada para gerar variantes para:\n",
       "  - Claude 3.7: 582 tokens\n",
       "  - Nova Pro: 579 tokens\n",
       "\n",
       "**C√°lculo de tokens**:\n",
       "- Tokens de entrada: 429 + 511 + 511 = 1.451 tokens\n",
       "- Tokens de sa√≠da: 511 + 582 + 579 = 1.672 tokens\n",
       "- Total de tokens: 3.123 tokens\n",
       "\n",
       "**C√°lculo de custo**:\n",
       "3.123 tokens √∑ 1.000 √ó $0.03 = $0.09\n",
       "\n",
       "Para nosso cen√°rio de workshop, a otimiza√ß√£o de prompts representa uma despesa m√≠nima comparada aos custos cont√≠nuos de infer√™ncia, mas rastre√°-la fornece uma imagem completa da economia da migra√ß√£o.\n",
       "\n",
       "> **üí° Nota do Workshop**: Em sistemas de produ√ß√£o, a otimiza√ß√£o de prompts pode ser executada periodicamente conforme os modelos evoluem ou os requisitos mudam. Embora o custo por otimiza√ß√£o seja baixo, empresas com muitos prompts diferentes devem contabilizar isso em seus or√ßamentos operacionais."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üí∞ CALCULANDO CUSTO DA OTIMIZA√á√ÉO DE PROMPTS\n",
       "print(\"üí∞ CUSTO DA OTIMIZA√á√ÉO DE PROMPTS:\")\n",
       "print(\"=\" * 40)\n",
       "\n",
       "# Calculando tokens totais para otimiza√ß√£o de prompts\n",
       "input_prompt_len = 0\n",
       "optimized_prompts = []\n",
       "\n",
       "for index, evaluation in evaluation_tracking.iterrows():\n",
       "    model_id = evaluation['model']\n",
       "    if model_id == \"source_model\":\n",
       "        input_prompt_len = len(evaluation['text_prompt'])\n",
       "    else:\n",
       "        optimized_prompts.append(evaluation['text_prompt']) \n",
       "\n",
       "total_prompt_len = sum(len(prompt) for prompt in optimized_prompts) + input_prompt_len * len(optimized_prompts)\n",
       "\n",
       "# Estimativa de custo (aproximadamente 4 caracteres por token)\n",
       "prompt_optimization_cost = total_prompt_len/4/1000 * 0.03\n",
       "\n",
       "print(f\"ÔøΩÔøΩ Tokens de entrada: {input_prompt_len}\")\n",
       "print(f\"ÔøΩÔøΩ Prompts otimizados: {len(optimized_prompts)}\")\n",
       "print(f\"üìù Total de caracteres: {total_prompt_len}\")\n",
       "print(f\"üí∞ Custo estimado para otimiza√ß√£o de prompts: ${prompt_optimization_cost:.6f}\")\n",
       "\n",
       "print(\"\\nüí° Este √© um custo √∫nico que representa a otimiza√ß√£o dos prompts para diferentes modelos.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Entendendo Custos do LLM-as-a-Judge**\n",
       "\n",
       "#### **Quebrando a Economia da Avalia√ß√£o**\n",
       "\n",
       "Al√©m do custo direto da infer√™ncia do modelo, √© importante contabilizar a despesa da pr√≥pria avalia√ß√£o de qualidade. LLM-as-a-Judge √© um m√©todo poderoso de avalia√ß√£o, mas tamb√©m tem custos associados que devem ser considerados no seu planejamento de migra√ß√£o.\n",
       "\n",
       "#### **Componentes de Pre√ßos**\n",
       "\n",
       "O Bedrock cobra pelas avalia√ß√µes LLM-as-a-Judge baseado nos seguintes componentes:\n",
       "\n",
       "1. **Infer√™ncia do Modelo**: O custo principal √© pelo uso do modelo avaliador\n",
       "   - Scores algor√≠tmicos gerados automaticamente s√£o fornecidos sem cobran√ßas adicionais\n",
       "   - Para avalia√ß√£o baseada em humanos com seu pr√≥prio workstream, h√° uma cobran√ßa de $0.21 por tarefa humana completada\n",
       "\n",
       "2. **Consumo de Tokens**: Cada avalia√ß√£o envolve v√°rios componentes:\n",
       "   - **Prompts do Juiz**: Cada m√©trica/avaliador usa seu pr√≥prio [prompt especializado](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-judge-prompt-nova.html) (~300 tokens por m√©trica)\n",
       "   - **Conte√∫do de Entrada**: Os prompts originais e respostas do modelo sendo avaliados\n",
       "   - **Resultados de Sa√≠da**: Sa√≠da JSON com scores de avalia√ß√£o (~20 tokens por m√©trica)\n",
       "\n",
       "#### **F√≥rmula de C√°lculo de Custo**\n",
       "\n",
       "Para or√ßamenta√ß√£o precisa, podemos estimar custos de avalia√ß√£o usando esta f√≥rmula para cada m√©trica:\n",
       "\n",
       "Custo de Avalia√ß√£o = [((Tokens nos prompts) + (Tokens nas respostas) + (Tokens do prompt do juiz))/1000 √ó (Pre√ßo do token de entrada do avaliador)] + [(Tokens de sa√≠da)/1000 √ó (Pre√ßo do token de sa√≠da do avaliador)]\n",
       "\n",
       "Onde:\n",
       "- Tokens de entrada = N√∫mero de tokens nos seus prompts + respostas + prompts do juiz (tipicamente ~300 tokens)\n",
       "- Tokens de sa√≠da = N√∫mero de tokens na sa√≠da do avaliador (tipicamente ~20 tokens por m√©trica)\n",
       "\n",
       "Esse entendimento detalhado dos custos de avalia√ß√£o ajuda a construir uma imagem econ√¥mica completa ao comparar diferentes op√ß√µes de modelo e planejar avalia√ß√£o cont√≠nua de qualidade em produ√ß√£o.\n",
       "\n",
       "> **üí° Nota do Workshop**: Ao projetar sua estrat√©gia de avalia√ß√£o, considere o trade-off entre avalia√ß√£o abrangente (usando muitas m√©tricas) e efici√™ncia de custo. Para avalia√ß√µes de rotina, voc√™ pode selecionar um subconjunto menor de m√©tricas cr√≠ticas para controlar custos."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üí∞ CONFIGURANDO CUSTOS DO AVALIADOR\n",
       "evaluator_id = \"amazon.nova-pro-v1:0\"\n",
       "\n",
       "evaluator_input_price = calculator.model_input_token_prices.get(evaluator_id)\n",
       "evaluator_output_price = calculator.model_output_token_prices.get(evaluator_id)\n",
       "\n",
       "print(f\"‚öñÔ∏è Modelo avaliador: {evaluator_id}\")\n",
       "print(f\"üí∞ Pre√ßo de entrada: ${evaluator_input_price}; pre√ßo de sa√≠da: ${evaluator_output_price}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üí∞ CALCULANDO CUSTOS DE AVALIA√á√ÉO DE QUALIDADE\n",
       "print(\"üí∞ CUSTOS DE AVALIA√á√ÉO DE QUALIDADE:\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "# Verificando suas m√©tricas de avalia√ß√£o. Por padr√£o nosso workshop avalia 3 m√©tricas: \"Builtin.Correctness\", \"Builtin.Completeness\", \"Builtin.ProfessionalStyleAndTone\"\n",
       "\n",
       "# Encontrando todos os arquivos json correspondentes\n",
       "quality_evaluation_inputs = glob.glob(os.path.join(directory, \"quality_evaluation*\"))\n",
       "\n",
       "# Processando cada arquivo\n",
       "total_quality_evaluation_cost = 0\n",
       "\n",
       "for filename in quality_evaluation_inputs:\n",
       "    print(f\"\\nüìä Processando {os.path.basename(filename)}...\")\n",
       "    \n",
       "    evaluation_price = 0\n",
       "    with open(filename, 'r') as f:\n",
       "        for line in f:\n",
       "            data = json.loads(line)\n",
       "            \n",
       "            prompt_length = len(data[\"prompt\"])\n",
       "            reference_length = len(data[\"referenceResponse\"])\n",
       "            model_response_length = len(data[\"modelResponses\"][0][\"response\"])\n",
       "            \n",
       "            # Calculando tokens de acordo com a f√≥rmula\n",
       "            input_tokens = (prompt_length + reference_length + model_response_length) / 4 + 300  # 300 √© uma estimativa aproximada, veja detalhes acima\n",
       "            output_tokens = 20  # √© uma estimativa aproximada\n",
       "            \n",
       "            # Calculando pre√ßo para esta entrada\n",
       "            entry_price = (input_tokens/1000 * evaluator_input_price) + (output_tokens/1000 * evaluator_output_price)\n",
       "            evaluation_price += entry_price\n",
       "            \n",
       "    print(f\"ÔøΩÔøΩ Custo estimado por m√©trica para avaliar {os.path.basename(filename)}: ${evaluation_price:.6f}\")\n",
       "    total_quality_evaluation_cost += evaluation_price\n",
       "\n",
       "## Avaliamos 3 m√©tricas: \"Builtin.Correctness\", \"Builtin.Completeness\", \"Builtin.ProfessionalStyleAndTone\"\n",
       "total_quality_evaluation_cost = total_quality_evaluation_cost * 3\n",
       "\n",
       "print(f\"\\nüí∞ Custo total estimado para avaliar: ${total_quality_evaluation_cost:.6f}\")\n",
       "print(\"üí° Este custo representa a avalia√ß√£o autom√°tica de qualidade usando LLM-as-a-Judge.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Consolida√ß√£o de M√©tricas de Lat√™ncia e Custo de Infer√™ncia**\n",
       "\n",
       "Nesta se√ß√£o, vamos importar e processar os dados de lat√™ncia coletados durante o Passo 3. Esses dados s√£o cruciais para entender as caracter√≠sticas de performance de cada modelo, particularmente para aplica√ß√µes com requisitos em tempo real ou experi√™ncias de usu√°rio interativas.\n",
       "\n",
       "Ao comparar essas m√©tricas entre modelos, podemos identificar diferen√ßas de performance que podem impactar a experi√™ncia do usu√°rio em ambientes de produ√ß√£o. Isso √© especialmente importante para aplica√ß√µes com requisitos de lat√™ncia estritos ou aquelas que servem grandes n√∫meros de usu√°rios concorrentes."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ FUN√á√ïES PRA CONSOLIDAR DADOS DE LAT√äNCIA\n",
       "def combine_latency_evaluation_files(directory):\n",
       "    \"\"\"Combina todos os arquivos CSV no diret√≥rio em um √∫nico DataFrame.\"\"\"\n",
       "    all_files = glob.glob(os.path.join(directory, \"document_summarization_*.csv\"))\n",
       "    df_list = []\n",
       "    for filename in all_files:\n",
       "        df = pd.read_csv(filename)\n",
       "        df_list.append(df)\n",
       "    return pd.concat(df_list, axis=0, ignore_index=True)\n",
       "\n",
       "def calculate_metrics(df, group_columns):\n",
       "    \"\"\"Calcula m√©tricas de lat√™ncia agrupadas por modelo, regi√£o e perfil de infer√™ncia.\"\"\"\n",
       "    metrics = df.groupby(group_columns).agg({\n",
       "        'model_input_tokens': ['count', 'mean'],\n",
       "        'model_output_tokens': ['mean'],\n",
       "        'cost': ['mean'],\n",
       "        'latency': ['mean', 'median', \n",
       "                             lambda x: x.quantile(0.9),lambda x: x.std()],\n",
       "        'model_latencyMs': ['mean', 'median', \n",
       "                             lambda x: x.quantile(0.9),lambda x: x.std()]\n",
       "    }).round(6)\n",
       "\n",
       "    metrics.columns = ['sample_size', \n",
       "                      'avg_input_tokens',\n",
       "                      'avg_output_tokens',\n",
       "                       'avg_cost',\n",
       "                       'latency_mean', 'latency_p50', 'latency_p90', 'latency_std',\n",
       "                      'model_latencyMs_mean', 'model_latencyMs_p50', 'model_latencyMs_p90', 'model_latencyMs_std']\n",
       "    \n",
       "    metrics = metrics.reset_index()\n",
       "    \n",
       "    return metrics\n",
       "\n",
       "print(\"‚úÖ Fun√ß√µes de consolida√ß√£o criadas! Vamos processar os dados.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ CONSOLIDANDO DADOS DE LAT√äNCIA\n",
       "print(\"üìä CONSOLIDANDO M√âTRICAS DE LAT√äNCIA...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "latency_evaluation_raw = combine_latency_evaluation_files(directory)\n",
       "metrics = calculate_metrics(latency_evaluation_raw, ['model', 'region', 'inference_profile'])\n",
       "\n",
       "print(\"üéØ Modelos encontrados:\")\n",
       "for model in metrics[\"model\"]:\n",
       "    print(f\"‚Ä¢ {model}\")\n",
       "\n",
       "display(metrics)\n",
       "\n",
       "print(\"\\nüí° Estas s√£o as m√©tricas consolidadas de lat√™ncia e custo para todos os modelos.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Integra√ß√£o de M√©tricas de Qualidade**\n",
       "\n",
       "Com nossas m√©tricas de lat√™ncia e custo preparadas, agora precisamos incorporar os resultados da avalia√ß√£o de qualidade dos nossos jobs LLM-as-a-Judge. Antes de analisar esses resultados, precisamos:\n",
       "1. Verificar que todos os jobs de avalia√ß√£o completaram\n",
       "2. Localizar os arquivos de sa√≠da no nosso bucket S3\n",
       "3. Extrair e analisar os scores de avalia√ß√£o para cada modelo\n",
       "\n",
       "Esses dados de qualidade completam nossa vis√£o tridimensional da performance do modelo (lat√™ncia, custo e qualidade), permitindo decis√µes de sele√ß√£o verdadeiramente informadas.\n",
       "\n",
       "> **‚ö†Ô∏è Nota do Workshop**: Jobs de avalia√ß√£o podem demorar v√°rios minutos para completar. Se seus jobs ainda est√£o rodando, voc√™ pode monitorar o status no console AWS ou aguardar a conclus√£o antes de prosseguir."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üîß CONFIGURANDO CLIENTES AWS\n",
       "bedrock_client = boto3.client('bedrock')\n",
       "s3_client = boto3.client('s3')\n",
       "\n",
       "print(\"‚úÖ Clientes AWS configurados! Vamos verificar os jobs de avalia√ß√£o.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üîç FUN√á√ÉO PRA OBTER CHAVES DE SA√çDA DO S3\n",
       "import re\n",
       "from collections import defaultdict\n",
       "\n",
       "def get_s3_output_keys(evaluation_tracking):\n",
       "    # Inicializando estrutura de resultado\n",
       "    result = {\n",
       "        \"model\": [],\n",
       "        \"key\": []\n",
       "    }\n",
       "    \n",
       "    for index, evaluation in evaluation_tracking.iterrows():\n",
       "        model_id = evaluation['model']\n",
       "        evaluation_job_arn = evaluation['quality_evaluation_jobArn']\n",
       "\n",
       "        # Verificando status do job\n",
       "        check_status = bedrock_client.get_evaluation_job(jobIdentifier=evaluation_job_arn)\n",
       "        print(f\"{model_id}: {check_status['status']}\")\n",
       "        \n",
       "        if check_status['status'] == \"Completed\":\n",
       "            output_path = evaluation['quality_evaluation_output']\n",
       "            try:\n",
       "                response = s3_client.list_objects_v2(\n",
       "                    Bucket=BUCKET_NAME,\n",
       "                    Prefix=PREFIX\n",
       "                )\n",
       "\n",
       "                # Encontrando o arquivo JSONL de sa√≠da para este modelo\n",
       "                for obj in response.get('Contents', []):\n",
       "                    key = obj['Key']\n",
       "                    # Adicionando verifica√ß√£o de identificador do modelo\n",
       "                    if key.endswith('_output.jsonl') and model_id.replace(':', '-') in key:\n",
       "                        result[\"model\"].append(model_id)\n",
       "                        result[\"key\"].append(key)\n",
       "                        break\n",
       "            \n",
       "            except Exception as e:\n",
       "                print(f\"‚ùå Erro listando objetos para {model_id}: {str(e)}\")\n",
       "        else:\n",
       "            print(\"\\x1b[31mJob de avalia√ß√£o de qualidade ainda est√° em andamento, por favor aguarde..\\x1b[0m\")\n",
       "            sys.exit()\n",
       "    \n",
       "    return result\n",
       "\n",
       "print(\"‚úÖ Fun√ß√£o de verifica√ß√£o criada! Vamos detectar as chaves automaticamente.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üîç DETECTANDO CHAVES DE SA√çDA DO S3\n",
       "print(\"üîç DETECTANDO CHAVES DE SA√çDA DO S3...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "# Uso\n",
       "s3_output_keys = get_s3_output_keys(evaluation_tracking)\n",
       "\n",
       "# Agora s3_output_keys cont√©m um dicion√°rio mapeando IDs de modelo para suas chaves de sa√≠da S3\n",
       "print(\"\\nÔøΩÔøΩ Chaves de sa√≠da S3 detectadas automaticamente:\")\n",
       "for model, key in zip(s3_output_keys[\"model\"], s3_output_keys[\"key\"]):\n",
       "    print(f\"  {model}: {key}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ EXTRAINDO M√âTRICAS DE QUALIDADE\n",
       "print(\"ÔøΩÔøΩ EXTRAINDO M√âTRICAS DE QUALIDADE...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "### M√©tricas de qualidade\n",
       "file_key_df = pd.DataFrame(s3_output_keys)\n",
       "model_quality_list = []\n",
       "\n",
       "for index, row in file_key_df.iterrows():  \n",
       "    metrics_dict = {}\n",
       "\n",
       "    model = row[\"model\"]\n",
       "    if row[\"key\"] == \"\":\n",
       "        continue\n",
       "        \n",
       "    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=row[\"key\"])\n",
       "    content = response['Body'].read().decode('utf-8')\n",
       "    \n",
       "    for line in content.strip().split('\\n'):\n",
       "        if line:\n",
       "            data = json.loads(line)\n",
       "            if 'automatedEvaluationResult' in data and 'scores' in data['automatedEvaluationResult']:\n",
       "                for score in data['automatedEvaluationResult']['scores']:\n",
       "                    metric_name = score['metricName']\n",
       "                    if 'result' in score:\n",
       "                        metric_value = score['result']\n",
       "                        if metric_name not in metrics_dict:\n",
       "                            metrics_dict[metric_name] = []\n",
       "                        metrics_dict[metric_name].append(metric_value)\n",
       "    \n",
       "    df = pd.DataFrame(metrics_dict)\n",
       "    df['model'] = model\n",
       "    model_quality_average = df.groupby(\"model\").mean()\n",
       "    model_quality_average = model_quality_average.reset_index()\n",
       "    model_quality_list.append(model_quality_average)\n",
       "\n",
       "model_quality = pd.concat(model_quality_list, axis=0, ignore_index=True)\n",
       "print(\"ÔøΩÔøΩ M√âTRICAS DE QUALIDADE EXTRA√çDAS:\")\n",
       "print(model_quality)\n",
       "\n",
       "print(\"\\nüí° Estes s√£o os scores de qualidade para cada modelo.\")\n",
       "print(\"ÔøΩÔøΩ Scores v√£o de 0 a 1, onde 1 √© a melhor qualidade poss√≠vel.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "> **üí° Nota do Workshop**: Esses resultados de avalia√ß√£o podem n√£o conseguir diferenciar o suficiente entre os modelos. Isso √© porque temos um tamanho de amostra muito pequeno (n=10), por quest√£o de tempo. Em implementa√ß√£o real, voc√™ precisar√° aumentar o tamanho da amostra para medir com precis√£o a qualidade dos modelos."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **An√°lise Consolidada de M√©tricas**\n",
       "\n",
       "#### **Combinando Dados de Performance, Qualidade e Custo**\n",
       "\n",
       "Com nossas m√©tricas de qualidade agora adicionadas aos nossos dados de performance e custo, temos uma vis√£o completa das capacidades de cada modelo. Este framework de m√©tricas consolidado nos permite:\n",
       "\n",
       "1. **Comparar trade-offs**: Ver como os modelos balanceiam velocidade, qualidade e custo\n",
       "2. **Identificar pontos fortes**: Determinar quais modelos se destacam em dimens√µes espec√≠ficas\n",
       "3. **Combinar com requisitos**: Alinhar capacidades com prioridades espec√≠ficas da aplica√ß√£o\n",
       "\n",
       "Esta an√°lise multidimensional √© essencial para tomar decis√µes informadas que v√£o al√©m do pensamento simplista de \"melhor modelo\". Diferentes aplica√ß√µes t√™m requisitos √∫nicos - um chatbot de atendimento ao cliente pode priorizar qualidade de resposta e tom, enquanto uma aplica√ß√£o de processamento de alto volume pode favorecer velocidade e efici√™ncia de custo.\n",
       "\n",
       "O dataframe de m√©tricas mescladas que criamos serve como base para nossas visualiza√ß√µes e relat√≥rio final, fornecendo uma imagem clara das vantagens relativas de cada op√ß√£o de modelo."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ MESCLANDO M√âTRICAS\n",
       "print(\"ÔøΩÔøΩ MESCLANDO M√âTRICAS...\")\n",
       "print(\"=\" * 30)\n",
       "\n",
       "## Mesclando m√©tricas para o dataframe de m√©tricas\n",
       "metrics = pd.merge(metrics, model_quality, on=['model'])\n",
       "display(metrics)\n",
       "\n",
       "print(\"\\nüéâ Todas as m√©tricas foram consolidadas em um √∫nico dataframe!\")\n",
       "print(\"üí° Agora temos lat√™ncia, custo e qualidade para todos os modelos.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Resumo de An√°lise Aprimorado por IA**\n",
       "\n",
       "#### **Usando LLMs para Interpretar Resultados de Avalia√ß√£o**\n",
       "\n",
       "Uma das capacidades poderosas dos LLMs avan√ßados √© sua habilidade de analisar dados complexos e gerar resumos perspicazes. Nesta se√ß√£o, vamos aproveitar essa capacidade perguntando ao Claude Haiku para interpretar nossos resultados de avalia√ß√£o e fornecer um resumo conciso dos principais achados.\n",
       "\n",
       "Esta abordagem demonstra um padr√£o importante para trabalhar com modelos foundation: us√°-los n√£o apenas como geradores de conte√∫do, mas como ferramentas anal√≠ticas que podem extrair insights de dados estruturados.\n",
       "\n",
       "O resumo resultante fornece uma interpreta√ß√£o leg√≠vel por humanos dos nossos dados, complementando nossas visualiza√ß√µes e dados brutos com insights narrativos.\n",
       "\n",
       "> **üí° Nota do Workshop**: Este padr√£o de \"LLM como analista de dados\" pode ser aplicado a muitos cen√°rios de business intelligence al√©m da avalia√ß√£o de modelos. Considere como sua organiza√ß√£o pode aproveitar modelos foundation para gerar insights de outros datasets complexos."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ü§ñ CONFIGURANDO CLIENTE BEDROCK\n",
       "client = boto3.client(\"bedrock-runtime\")\n",
       "\n",
       "print(\"‚úÖ Cliente Bedrock configurado! Vamos gerar insights com IA.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ü§ñ GERANDO RESUMO DE AN√ÅLISE\n",
       "print(\"ü§ñ GERANDO RESUMO DE AN√ÅLISE COM IA...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "summary_prompt = \"\"\"Usando o dataset fornecido abaixo, crie um resumo conciso de 3 frases que identifica qual modelo performa melhor em cada uma dessas tr√™s categorias de m√©tricas:\n",
       "1. M√©tricas de lat√™ncia (latency_mean, latency_p50, latency_p90)\n",
       "2. M√©tricas de custo (avg_cost)\n",
       "3. M√©tricas de qualidade (Builtin.Correctness e completude de sa√≠da como sugerido por avg_output_tokens)\n",
       "4. N√£o comece com \"Baseado no fornecido...\", apenas d√™ seu resumo\n",
       "O resumo deve declarar claramente qual modelo √© √≥timo para usu√°rios priorizando velocidade, efici√™ncia de custo ou qualidade de sa√≠da.\n",
       "\n",
       "Dataset: {dataset}\n",
       "\"\"\"\n",
       "\n",
       "max_tokens=1000\n",
       "temperature=0\n",
       "top_p=0.9\n",
       "\n",
       "response = client.converse(\n",
       "    modelId=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
       "    messages=[\n",
       "        {\n",
       "            \"role\": \"user\",\n",
       "            \"content\": [\n",
       "                {\n",
       "                    \"text\": summary_prompt.format(dataset=metrics)\n",
       "                }\n",
       "            ]\n",
       "        }\n",
       "    ],\n",
       "    inferenceConfig={\n",
       "        \"temperature\": temperature,\n",
       "        \"maxTokens\": max_tokens,\n",
       "        \"topP\": top_p\n",
       "    }\n",
       ")\n",
       "\n",
       "analysis_summary = response['output']['message']['content'][0]['text']\n",
       "\n",
       "print(\"üìä RESUMO DE AN√ÅLISE GERADO:\")\n",
       "print(\"-\" * 40)\n",
       "print(analysis_summary)\n",
       "\n",
       "print(\"\\nüí° Este resumo foi gerado automaticamente pela IA analisando nossos dados!\")\n",
       "print(\" Ele nos d√° uma vis√£o r√°pida e objetiva dos resultados.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Gera√ß√£o do Relat√≥rio Final**\n",
       "\n",
       "#### **Criando Documenta√ß√£o de An√°lise Abrangente**\n",
       "\n",
       "O culminar do nosso processo de avalia√ß√£o √© um relat√≥rio formatado profissionalmente que apresenta nossos achados em um formato claro e visualmente atraente. Este relat√≥rio serve m√∫ltiplos prop√≥sitos:\n",
       "\n",
       "1. **Documenta√ß√£o**: Cria um registro permanente da nossa metodologia de avalia√ß√£o e resultados\n",
       "2. **Comunica√ß√£o**: Fornece artefatos compartilh√°veis para discuss√µes com stakeholders\n",
       "3. **Suporte √† Decis√£o**: Organiza informa√ß√µes para facilitar escolhas informadas\n",
       "\n",
       "Nossa utilidade `generate_analysis_report` lida com o trabalho complexo de:\n",
       "- Criar visualiza√ß√µes consistentes entre dimens√µes\n",
       "- Formatar tabelas para legibilidade\n",
       "- Gerar gr√°ficos de distribui√ß√£o de performance\n",
       "- Incluir nosso resumo gerado por IA junto com m√©tricas brutas\n",
       "\n",
       "A sa√≠da final inclui tanto um relat√≥rio PDF quanto um resumo CSV, fornecendo op√ß√µes tanto para revis√µes de alto n√≠vel quanto para an√°lise detalhada.\n",
       "\n",
       "> **üí° Nota do Workshop**: Ap√≥s executar esta c√©lula, verifique o diret√≥rio `../outputs-analysis/` para visualizar seu relat√≥rio gerado. Este relat√≥rio pode servir como template para sua pr√≥pria documenta√ß√£o de avalia√ß√£o de modelos."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üìä GERANDO RELAT√ìRIO FINAL\n",
       "print(\"üìä GERANDO RELAT√ìRIO FINAL...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "report = generate_analysis_report.Analysis_Report()\n",
       "report.generate_report(latency_evaluation_raw, directory, metrics, analysis_summary, total_quality_evaluation_cost, prompt_optimization_cost)\n",
       "\n",
       "print(\"\\nüéâ RELAT√ìRIO GERADO COM SUCESSO!\")\n",
       "print(\"ÔøΩÔøΩ Verifique o diret√≥rio ../outputs-analysis/ para ver os arquivos gerados.\")\n",
       "print(\"ÔøΩÔøΩ Voc√™ encontrar√° um relat√≥rio PDF e um resumo CSV.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Resumo e Principais Takeaways**\n",
       "\n",
       "#### **Parab√©ns por Completar o Workshop de Avalia√ß√£o de Modelos!**\n",
       "\n",
       "Voc√™ navegou com sucesso por todo o processo de avalia√ß√£o e migra√ß√£o de modelos, ganhando experi√™ncia pr√°tica com uma metodologia que pode ser aplicada aos seus pr√≥prios projetos GenAI. Neste notebook final, voc√™:\n",
       "\n",
       "1. ‚úÖ **Consolidou m√©tricas multidimensionais** entre lat√™ncia, qualidade e custo\n",
       "2. ‚úÖ **Calculou implica√ß√µes econ√¥micas** de diferentes escolhas de modelo em escala  \n",
       "3. ‚úÖ **Analisou avalia√ß√µes de qualidade** das avalia√ß√µes LLM-as-a-Judge\n",
       "4. ‚úÖ **Gerou insights aprimorados por IA** para interpretar dados complexos de avalia√ß√£o\n",
       "5. ‚úÖ **Criou documenta√ß√£o profissional** para apoiar tomada de decis√£o\n",
       "\n",
       "### **Principais Takeaways**\n",
       "\n",
       "1. **Sele√ß√£o de Modelo √© Multidimensional**: Raramente h√° um √∫nico modelo \"melhor\" - diferentes modelos se destacam em √°reas diferentes, e a escolha √≥tima depende dos seus requisitos espec√≠ficos.\n",
       "2. **Migra√ß√£o Baseada em Dados**: Migra√ß√µes bem-sucedidas de modelos requerem medi√ß√µes objetivas em vez de apenas suposi√ß√µes ou especifica√ß√µes.\n",
       "3. **An√°lise de Trade-offs**: Entender a rela√ß√£o entre velocidade, qualidade e custo permite decis√µes informadas que balanceiam prioridades concorrentes.\n",
       "4. **Documenta√ß√£o Importa**: Documenta√ß√£o completa da sua metodologia de avalia√ß√£o e resultados ajuda a construir consenso e apoiar decis√µes futuras de migra√ß√£o.\n",
       "5. **Avalia√ß√£o Cont√≠nua**: Conforme novos modelos s√£o lan√ßados e existentes s√£o atualizados, o processo de avalia√ß√£o deve ser repetido periodicamente para garantir que voc√™ est√° usando componentes √≥timos.\n",
       "\n",
       "### **Pr√≥ximos Passos**\n",
       "\n",
       "Considere aplicar este framework de avalia√ß√£o aos seus pr√≥prios casos de uso:\n",
       "- **Personalize crit√©rios de sucesso**: Defina thresholds espec√≠ficos de aceita√ß√£o baseados nos requisitos da sua aplica√ß√£o\n",
       "- **Amplie a compara√ß√£o**: Avalie mais modelos entre diferentes provedores e arquiteturas\n",
       "- **Expanda as m√©tricas**: Adicione crit√©rios de avalia√ß√£o espec√≠ficos do dom√≠nio relevantes para suas aplica√ß√µes\n",
       "- **Continue otimiza√ß√£o de prompts**: Lembre-se que engenharia de prompts √© um esfor√ßo cont√≠nuo - considere construir um pipeline sistem√°tico para testar varia√ß√µes de prompts com m√©tricas automatizadas ou avalia√ß√£o human-in-the-loop\n",
       "- **Explore mais op√ß√µes de Infer√™ncia**: Aprenda e explore mais op√ß√µes de otimiza√ß√£o de infer√™ncia no Bedrock que podem melhorar a lat√™ncia para seus casos de uso, ex: [Infer√™ncia Otimizada](https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html), [Cache de Prompts](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html), [Perfil de Infer√™ncia](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles.html) e outros.\n",
       "- **Construa componente de avalia√ß√£o RAG**: Se seu caso de uso requer uma base de conhecimento constru√≠da com RAG, considere adicionar um componente de avalia√ß√£o de qualidade para RAG. Op√ß√µes de avalia√ß√£o RAG incluem [Bedrock RAG evaluator](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-kb.html), solu√ß√µes open source como [RAGAS](https://docs.ragas.io/en/stable/getstarted/rag_eval/). \n",
       "- **Automatize o workflow**: Integre estes passos de avalia√ß√£o no seu pipeline CI/CD para avalia√ß√£o cont√≠nua de modelos\n",
       "- **Estabele√ßa loops de feedback**: Crie mecanismos para capturar dados de performance de produ√ß√£o para informar futuras otimiza√ß√µes de prompts e modelos\n",
       "\n",
       "Obrigado por participar deste workshop! Esperamos que as habilidades e metodologia que voc√™ aprendeu ajudem voc√™ a tomar decis√µes confiantes e baseadas em dados sobre sele√ß√£o e migra√ß√£o de modelos na sua jornada GenAI.\n",
       "\n",
       "---\n",
       "\n",
       "**üí° Dica Final do Pedro**: Lembre-se, migra√ß√£o de modelo n√£o √© uma corrida - √© uma maratona! Tome seu tempo, teste bem, e sempre mantenha a qualidade em mente. √â melhor ter um modelo um pouco mais lento que funciona perfeitamente do que um super r√°pido que quebra toda hora!\n",
       "\n",
       "**ÔøΩÔøΩ Pr√≥ximo passo**: Aplicar esta metodologia aos seus pr√≥prios projetos!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }