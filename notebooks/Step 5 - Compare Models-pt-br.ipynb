{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 🚀 **Passo 5: Comparação Final e Decisão**\n",
       "\n",
       "## **Aula 5.1: O Momento da Verdade**\n",
       "\n",
       "---\n",
       "\n",
       "### **Tá, mas o que é comparação final?**\n",
       "\n",
       "Imagina que você tá comprando um carro e testou três opções diferentes. Agora chegou a hora de reunir todas as informações: qual é mais rápido, qual é mais econômico, qual é mais confiável, e qual oferece o melhor custo-benefício. É exatamente isso que vamos fazer aqui - só que em vez de carros, são modelos de IA! 😄\n",
       "\n",
       "**Por que comparação final é importante?**\n",
       "\n",
       "Até agora medimos velocidade, qualidade e custo separadamente. Mas na vida real, você precisa tomar uma decisão baseada em TUDO junto. É como escolher um restaurante: não é só o preço, não é só a velocidade, não é só a qualidade - é a combinação de tudo!\n",
       "\n",
       "### **O Que Você Vai Ganhar Desta Análise**\n",
       "\n",
       "Este notebook produz dois artefatos chave:\n",
       "\n",
       "1. **Relatório PDF de Análise**: Um documento abrangente com visualizações comparando modelos em todas as dimensões\n",
       "2. **Resumo CSV**: Dados brutos pra análise adicional ou integração com outras métricas de negócio\n",
       "\n",
       "Vamos começar consolidando nossos resultados de avaliação!\n",
       "\n",
       "---\n",
       "\n",
       "**🖼️ Sugestão de imagem**: Um gráfico de radar mostrando múltiplas dimensões de performance"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 🛠️ IMPORTANDO AS FERRAMENTAS NECESSÁRIAS\n",
       "import json\n",
       "import boto3\n",
       "import numpy as np\n",
       "from scipy import stats\n",
       "import pandas as pd\n",
       "import glob\n",
       "import os\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from IPython.display import display, HTML\n",
       "from matplotlib.backends.backend_pdf import PdfPages\n",
       "import datetime\n",
       "import sys\n",
       "from IPython.display import display\n",
       "\n",
       "sys.path.append(\"../\")\n",
       "\n",
       "from src import pricing\n",
       "from src import generate_analysis_report\n",
       "\n",
       "print(\"✅ Ferramentas importadas! Vamos fazer a comparação final!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ⚙️ CONFIGURAÇÃO AWS\n",
       "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
       "\n",
       "BUCKET_NAME = f\"genai-evaluation-migration-bucket-{account_id}\"\n",
       "PREFIX = \"genai_migration\"\n",
       "\n",
       "print(f\"🏢 Account ID: {account_id}\")\n",
       "print(f\"🪣 Bucket: {BUCKET_NAME}\")\n",
       "print(f\"📁 Prefix: {PREFIX}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Carregando Dados de Avaliação**\n",
       "\n",
       "#### **Recuperando Nossas Informações de Tracking**\n",
       "\n",
       "Antes de analisar nossos modelos, precisamos acessar as informações de tracking mantidas durante todo o processo de avaliação. Carregando essas informações de tracking primeiro, estabelecemos uma base pra nossa análise consolidada e garantimos que estamos trabalhando com metadados consistentes dos modelos durante todo o processo de comparação.\n",
       "\n",
       "> **💡 Nota para Aprendizes Autodidatas**: Se você modificou qualquer caminho ou nome de arquivo durante os passos anteriores, certifique-se de que essas mudanças estão refletidas no caminho do arquivo de tracking abaixo. O processo de avaliação depende de informações de tracking consistentes em todos os notebooks."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 📊 CARREGANDO NOSSO TRACKING\n",
       "evaluation_tracking_file = '../data/evaluation_tracking.csv'\n",
       "evaluation_tracking = pd.read_csv(evaluation_tracking_file)\n",
       "display(evaluation_tracking)\n",
       "\n",
       "print(\"\\n💡 Perfeito! Agora temos nosso plano de avaliação carregado.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Preparação da Análise de Custos**\n",
       "\n",
       "#### **Calculando Custos por Requisição e Projeções**\n",
       "\n",
       "Enquanto estamos aguardando nossos jobs de avaliação de qualidade LLM-as-a-Judge completarem, podemos analisar o impacto econômico de cada opção de modelo. Considerações de custo são cruciais pra implantações de produção, já que mesmo pequenas diferenças por requisição podem resultar em despesas operacionais significativas em escala.\n",
       "\n",
       "Nossa análise de custo inclui dois componentes chave:\n",
       "\n",
       "1. **Custo por Inferência**: Calculado baseado no uso de tokens da nossa avaliação de latência\n",
       "   custo_por_inferencia = (tokens_entrada × preço_token_entrada) + (tokens_saída × preço_token_saída)\n",
       "   \n",
       "   Essas informações de custo serão cruciais ao fazer a seleção final do modelo, permitindo-nos balancear performance e qualidade contra restrições orçamentárias. Em ambientes de produção, essa análise pode ser estendida para incluir custos mensais projetados baseados em volumes de requisição esperados.\n",
       "\n",
       "2. **Custos de Serviços Auxiliares**: Despesas adicionais além da inferência direta do modelo\n",
       "   - **LLM-as-a-Judge**: Cobrado baseado no uso do modelo avaliador\n",
       "   - **Otimização de Prompt**: Cobrado por token para prompts de entrada e otimizados\n",
       "\n",
       "> **💡 Nota do Workshop**: Os preços do AWS Bedrock são atualizados periodicamente. Para os preços mais atuais, consulte a [Página de Preços do Bedrock](https://aws.amazon.com/bedrock/pricing/)."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# �� CONFIGURANDO O DIRETÓRIO\n",
       "directory = \"../outputs\"\n",
       "print(f\"�� Diretório de saída: {directory}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 💰 CALCULANDO CUSTOS PRA TODOS OS MODELOS\n",
       "calculator = pricing.PriceCalculator()\n",
       "\n",
       "# Encontrando todos os arquivos CSV correspondentes\n",
       "all_files = glob.glob(os.path.join(directory, \"document_summarization_*.csv\"))\n",
       "\n",
       "print(\"💰 CALCULANDO CUSTOS...\")\n",
       "print(\"=\" * 40)\n",
       "\n",
       "# Processando cada arquivo\n",
       "for filename in all_files:\n",
       "    print(f\"\\n📊 Processando {os.path.basename(filename)}...\")\n",
       "    \n",
       "    # Lendo o arquivo CSV\n",
       "    document_summarization_df = pd.read_csv(filename)\n",
       "\n",
       "    model_id = document_summarization_df[\"model\"][0]  # Mudando o nome de volta pra combinar com a config de preços\n",
       "    \n",
       "    print(f\"🎯 Modelo: {model_id}\")\n",
       "    print(f\"�� Total de linhas: {len(document_summarization_df)}\")\n",
       "    \n",
       "    # Calculando custos de entrada para todas as linhas de uma vez\n",
       "    input_costs = document_summarization_df[\"model_input_tokens\"].apply(\n",
       "        lambda tokens: calculator.calculate_input_price(tokens, model_id)\n",
       "    )\n",
       "\n",
       "    # Calculando custos de saída para todas as linhas de uma vez\n",
       "    output_costs = document_summarization_df[\"model_output_tokens\"].apply(\n",
       "        lambda tokens: calculator.calculate_output_price(tokens, model_id)\n",
       "    )\n",
       "    \n",
       "    # Calculando custos totais\n",
       "    document_summarization_df[\"cost\"] = (input_costs + output_costs).round(6)\n",
       "\n",
       "    # Escrevendo de volta no mesmo arquivo\n",
       "    document_summarization_df.to_csv(filename, index=False)\n",
       "    print(f\"✅ Custos calculados e salvos!\")\n",
       "    \n",
       "    # Mostrando estatísticas rápidas\n",
       "    avg_cost = document_summarization_df[\"cost\"].mean()\n",
       "    total_cost = document_summarization_df[\"cost\"].sum()\n",
       "    print(f\"💰 Custo médio por inferência: ${avg_cost:.6f}\")\n",
       "    print(f\"💰 Custo total: ${total_cost:.6f}\")\n",
       "\n",
       "print(\"\\n🎉 Cálculo de custos concluído!\")\n",
       "print(\"💡 Agora todos os arquivos têm informações de custo incluídas.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Economia da Otimização de Prompts**\n",
       "\n",
       "#### **Calculando o Custo da Melhoria Automática de Prompts**\n",
       "\n",
       "Além da inferência do modelo e avaliação de qualidade, nosso processo de migração aproveita o serviço de Otimização de Prompts do Amazon Bedrock pra melhorar a eficácia dos prompts entre modelos. Como outros serviços de IA, essa capacidade tem sua própria estrutura de preços que deve ser considerada no custo total da migração.\n",
       "\n",
       "#### **Entendendo a Precificação da Otimização de Prompts**\n",
       "\n",
       "O Bedrock cobra pela otimização de prompts baseado no volume de tokens:\n",
       "\n",
       "- **Taxa**: $0.030 por 1.000 tokens\n",
       "- **Tokens Contados**: Tanto prompts de entrada quanto prompts de saída otimizados\n",
       "- **Ciclo de Cobrança**: Mensal, baseado no uso total de tokens\n",
       "\n",
       "Esse modelo de preços significa que os custos escalam com tanto o tamanho dos seus prompts quanto o número de otimizações que você executa. Para a maioria dos cenários de migração, isso representa um pequeno custo único, mas ainda é valioso estimar para planejamento orçamentário completo.\n",
       "\n",
       "#### **Exemplo de Cálculo**\n",
       "\n",
       "Pra ilustrar como os custos de otimização de prompts funcionam na prática, considere este exemplo:\n",
       "\n",
       "Um desenvolvedor de aplicação otimiza um prompt de sumarização de notícias originalmente escrito para Claude 3.5:\n",
       "- Prompt original: 429 tokens\n",
       "- Prompt otimizado para Claude 3.5: 511 tokens\n",
       "- Este prompt otimizado é então usado como entrada para gerar variantes para:\n",
       "  - Claude 3.7: 582 tokens\n",
       "  - Nova Pro: 579 tokens\n",
       "\n",
       "**Cálculo de tokens**:\n",
       "- Tokens de entrada: 429 + 511 + 511 = 1.451 tokens\n",
       "- Tokens de saída: 511 + 582 + 579 = 1.672 tokens\n",
       "- Total de tokens: 3.123 tokens\n",
       "\n",
       "**Cálculo de custo**:\n",
       "3.123 tokens ÷ 1.000 × $0.03 = $0.09\n",
       "\n",
       "Para nosso cenário de workshop, a otimização de prompts representa uma despesa mínima comparada aos custos contínuos de inferência, mas rastreá-la fornece uma imagem completa da economia da migração.\n",
       "\n",
       "> **💡 Nota do Workshop**: Em sistemas de produção, a otimização de prompts pode ser executada periodicamente conforme os modelos evoluem ou os requisitos mudam. Embora o custo por otimização seja baixo, empresas com muitos prompts diferentes devem contabilizar isso em seus orçamentos operacionais."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 💰 CALCULANDO CUSTO DA OTIMIZAÇÃO DE PROMPTS\n",
       "print(\"💰 CUSTO DA OTIMIZAÇÃO DE PROMPTS:\")\n",
       "print(\"=\" * 40)\n",
       "\n",
       "# Calculando tokens totais para otimização de prompts\n",
       "input_prompt_len = 0\n",
       "optimized_prompts = []\n",
       "\n",
       "for index, evaluation in evaluation_tracking.iterrows():\n",
       "    model_id = evaluation['model']\n",
       "    if model_id == \"source_model\":\n",
       "        input_prompt_len = len(evaluation['text_prompt'])\n",
       "    else:\n",
       "        optimized_prompts.append(evaluation['text_prompt']) \n",
       "\n",
       "total_prompt_len = sum(len(prompt) for prompt in optimized_prompts) + input_prompt_len * len(optimized_prompts)\n",
       "\n",
       "# Estimativa de custo (aproximadamente 4 caracteres por token)\n",
       "prompt_optimization_cost = total_prompt_len/4/1000 * 0.03\n",
       "\n",
       "print(f\"�� Tokens de entrada: {input_prompt_len}\")\n",
       "print(f\"�� Prompts otimizados: {len(optimized_prompts)}\")\n",
       "print(f\"📝 Total de caracteres: {total_prompt_len}\")\n",
       "print(f\"💰 Custo estimado para otimização de prompts: ${prompt_optimization_cost:.6f}\")\n",
       "\n",
       "print(\"\\n💡 Este é um custo único que representa a otimização dos prompts para diferentes modelos.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Entendendo Custos do LLM-as-a-Judge**\n",
       "\n",
       "#### **Quebrando a Economia da Avaliação**\n",
       "\n",
       "Além do custo direto da inferência do modelo, é importante contabilizar a despesa da própria avaliação de qualidade. LLM-as-a-Judge é um método poderoso de avaliação, mas também tem custos associados que devem ser considerados no seu planejamento de migração.\n",
       "\n",
       "#### **Componentes de Preços**\n",
       "\n",
       "O Bedrock cobra pelas avaliações LLM-as-a-Judge baseado nos seguintes componentes:\n",
       "\n",
       "1. **Inferência do Modelo**: O custo principal é pelo uso do modelo avaliador\n",
       "   - Scores algorítmicos gerados automaticamente são fornecidos sem cobranças adicionais\n",
       "   - Para avaliação baseada em humanos com seu próprio workstream, há uma cobrança de $0.21 por tarefa humana completada\n",
       "\n",
       "2. **Consumo de Tokens**: Cada avaliação envolve vários componentes:\n",
       "   - **Prompts do Juiz**: Cada métrica/avaliador usa seu próprio [prompt especializado](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-judge-prompt-nova.html) (~300 tokens por métrica)\n",
       "   - **Conteúdo de Entrada**: Os prompts originais e respostas do modelo sendo avaliados\n",
       "   - **Resultados de Saída**: Saída JSON com scores de avaliação (~20 tokens por métrica)\n",
       "\n",
       "#### **Fórmula de Cálculo de Custo**\n",
       "\n",
       "Para orçamentação precisa, podemos estimar custos de avaliação usando esta fórmula para cada métrica:\n",
       "\n",
       "Custo de Avaliação = [((Tokens nos prompts) + (Tokens nas respostas) + (Tokens do prompt do juiz))/1000 × (Preço do token de entrada do avaliador)] + [(Tokens de saída)/1000 × (Preço do token de saída do avaliador)]\n",
       "\n",
       "Onde:\n",
       "- Tokens de entrada = Número de tokens nos seus prompts + respostas + prompts do juiz (tipicamente ~300 tokens)\n",
       "- Tokens de saída = Número de tokens na saída do avaliador (tipicamente ~20 tokens por métrica)\n",
       "\n",
       "Esse entendimento detalhado dos custos de avaliação ajuda a construir uma imagem econômica completa ao comparar diferentes opções de modelo e planejar avaliação contínua de qualidade em produção.\n",
       "\n",
       "> **💡 Nota do Workshop**: Ao projetar sua estratégia de avaliação, considere o trade-off entre avaliação abrangente (usando muitas métricas) e eficiência de custo. Para avaliações de rotina, você pode selecionar um subconjunto menor de métricas críticas para controlar custos."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 💰 CONFIGURANDO CUSTOS DO AVALIADOR\n",
       "evaluator_id = \"amazon.nova-pro-v1:0\"\n",
       "\n",
       "evaluator_input_price = calculator.model_input_token_prices.get(evaluator_id)\n",
       "evaluator_output_price = calculator.model_output_token_prices.get(evaluator_id)\n",
       "\n",
       "print(f\"⚖️ Modelo avaliador: {evaluator_id}\")\n",
       "print(f\"💰 Preço de entrada: ${evaluator_input_price}; preço de saída: ${evaluator_output_price}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 💰 CALCULANDO CUSTOS DE AVALIAÇÃO DE QUALIDADE\n",
       "print(\"💰 CUSTOS DE AVALIAÇÃO DE QUALIDADE:\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "# Verificando suas métricas de avaliação. Por padrão nosso workshop avalia 3 métricas: \"Builtin.Correctness\", \"Builtin.Completeness\", \"Builtin.ProfessionalStyleAndTone\"\n",
       "\n",
       "# Encontrando todos os arquivos json correspondentes\n",
       "quality_evaluation_inputs = glob.glob(os.path.join(directory, \"quality_evaluation*\"))\n",
       "\n",
       "# Processando cada arquivo\n",
       "total_quality_evaluation_cost = 0\n",
       "\n",
       "for filename in quality_evaluation_inputs:\n",
       "    print(f\"\\n📊 Processando {os.path.basename(filename)}...\")\n",
       "    \n",
       "    evaluation_price = 0\n",
       "    with open(filename, 'r') as f:\n",
       "        for line in f:\n",
       "            data = json.loads(line)\n",
       "            \n",
       "            prompt_length = len(data[\"prompt\"])\n",
       "            reference_length = len(data[\"referenceResponse\"])\n",
       "            model_response_length = len(data[\"modelResponses\"][0][\"response\"])\n",
       "            \n",
       "            # Calculando tokens de acordo com a fórmula\n",
       "            input_tokens = (prompt_length + reference_length + model_response_length) / 4 + 300  # 300 é uma estimativa aproximada, veja detalhes acima\n",
       "            output_tokens = 20  # é uma estimativa aproximada\n",
       "            \n",
       "            # Calculando preço para esta entrada\n",
       "            entry_price = (input_tokens/1000 * evaluator_input_price) + (output_tokens/1000 * evaluator_output_price)\n",
       "            evaluation_price += entry_price\n",
       "            \n",
       "    print(f\"�� Custo estimado por métrica para avaliar {os.path.basename(filename)}: ${evaluation_price:.6f}\")\n",
       "    total_quality_evaluation_cost += evaluation_price\n",
       "\n",
       "## Avaliamos 3 métricas: \"Builtin.Correctness\", \"Builtin.Completeness\", \"Builtin.ProfessionalStyleAndTone\"\n",
       "total_quality_evaluation_cost = total_quality_evaluation_cost * 3\n",
       "\n",
       "print(f\"\\n💰 Custo total estimado para avaliar: ${total_quality_evaluation_cost:.6f}\")\n",
       "print(\"💡 Este custo representa a avaliação automática de qualidade usando LLM-as-a-Judge.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Consolidação de Métricas de Latência e Custo de Inferência**\n",
       "\n",
       "Nesta seção, vamos importar e processar os dados de latência coletados durante o Passo 3. Esses dados são cruciais para entender as características de performance de cada modelo, particularmente para aplicações com requisitos em tempo real ou experiências de usuário interativas.\n",
       "\n",
       "Ao comparar essas métricas entre modelos, podemos identificar diferenças de performance que podem impactar a experiência do usuário em ambientes de produção. Isso é especialmente importante para aplicações com requisitos de latência estritos ou aquelas que servem grandes números de usuários concorrentes."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# �� FUNÇÕES PRA CONSOLIDAR DADOS DE LATÊNCIA\n",
       "def combine_latency_evaluation_files(directory):\n",
       "    \"\"\"Combina todos os arquivos CSV no diretório em um único DataFrame.\"\"\"\n",
       "    all_files = glob.glob(os.path.join(directory, \"document_summarization_*.csv\"))\n",
       "    df_list = []\n",
       "    for filename in all_files:\n",
       "        df = pd.read_csv(filename)\n",
       "        df_list.append(df)\n",
       "    return pd.concat(df_list, axis=0, ignore_index=True)\n",
       "\n",
       "def calculate_metrics(df, group_columns):\n",
       "    \"\"\"Calcula métricas de latência agrupadas por modelo, região e perfil de inferência.\"\"\"\n",
       "    metrics = df.groupby(group_columns).agg({\n",
       "        'model_input_tokens': ['count', 'mean'],\n",
       "        'model_output_tokens': ['mean'],\n",
       "        'cost': ['mean'],\n",
       "        'latency': ['mean', 'median', \n",
       "                             lambda x: x.quantile(0.9),lambda x: x.std()],\n",
       "        'model_latencyMs': ['mean', 'median', \n",
       "                             lambda x: x.quantile(0.9),lambda x: x.std()]\n",
       "    }).round(6)\n",
       "\n",
       "    metrics.columns = ['sample_size', \n",
       "                      'avg_input_tokens',\n",
       "                      'avg_output_tokens',\n",
       "                       'avg_cost',\n",
       "                       'latency_mean', 'latency_p50', 'latency_p90', 'latency_std',\n",
       "                      'model_latencyMs_mean', 'model_latencyMs_p50', 'model_latencyMs_p90', 'model_latencyMs_std']\n",
       "    \n",
       "    metrics = metrics.reset_index()\n",
       "    \n",
       "    return metrics\n",
       "\n",
       "print(\"✅ Funções de consolidação criadas! Vamos processar os dados.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# �� CONSOLIDANDO DADOS DE LATÊNCIA\n",
       "print(\"📊 CONSOLIDANDO MÉTRICAS DE LATÊNCIA...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "latency_evaluation_raw = combine_latency_evaluation_files(directory)\n",
       "metrics = calculate_metrics(latency_evaluation_raw, ['model', 'region', 'inference_profile'])\n",
       "\n",
       "print(\"🎯 Modelos encontrados:\")\n",
       "for model in metrics[\"model\"]:\n",
       "    print(f\"• {model}\")\n",
       "\n",
       "display(metrics)\n",
       "\n",
       "print(\"\\n💡 Estas são as métricas consolidadas de latência e custo para todos os modelos.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Integração de Métricas de Qualidade**\n",
       "\n",
       "Com nossas métricas de latência e custo preparadas, agora precisamos incorporar os resultados da avaliação de qualidade dos nossos jobs LLM-as-a-Judge. Antes de analisar esses resultados, precisamos:\n",
       "1. Verificar que todos os jobs de avaliação completaram\n",
       "2. Localizar os arquivos de saída no nosso bucket S3\n",
       "3. Extrair e analisar os scores de avaliação para cada modelo\n",
       "\n",
       "Esses dados de qualidade completam nossa visão tridimensional da performance do modelo (latência, custo e qualidade), permitindo decisões de seleção verdadeiramente informadas.\n",
       "\n",
       "> **⚠️ Nota do Workshop**: Jobs de avaliação podem demorar vários minutos para completar. Se seus jobs ainda estão rodando, você pode monitorar o status no console AWS ou aguardar a conclusão antes de prosseguir."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 🔧 CONFIGURANDO CLIENTES AWS\n",
       "bedrock_client = boto3.client('bedrock')\n",
       "s3_client = boto3.client('s3')\n",
       "\n",
       "print(\"✅ Clientes AWS configurados! Vamos verificar os jobs de avaliação.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 🔍 FUNÇÃO PRA OBTER CHAVES DE SAÍDA DO S3\n",
       "import re\n",
       "from collections import defaultdict\n",
       "\n",
       "def get_s3_output_keys(evaluation_tracking):\n",
       "    # Inicializando estrutura de resultado\n",
       "    result = {\n",
       "        \"model\": [],\n",
       "        \"key\": []\n",
       "    }\n",
       "    \n",
       "    for index, evaluation in evaluation_tracking.iterrows():\n",
       "        model_id = evaluation['model']\n",
       "        evaluation_job_arn = evaluation['quality_evaluation_jobArn']\n",
       "\n",
       "        # Verificando status do job\n",
       "        check_status = bedrock_client.get_evaluation_job(jobIdentifier=evaluation_job_arn)\n",
       "        print(f\"{model_id}: {check_status['status']}\")\n",
       "        \n",
       "        if check_status['status'] == \"Completed\":\n",
       "            output_path = evaluation['quality_evaluation_output']\n",
       "            try:\n",
       "                response = s3_client.list_objects_v2(\n",
       "                    Bucket=BUCKET_NAME,\n",
       "                    Prefix=PREFIX\n",
       "                )\n",
       "\n",
       "                # Encontrando o arquivo JSONL de saída para este modelo\n",
       "                for obj in response.get('Contents', []):\n",
       "                    key = obj['Key']\n",
       "                    # Adicionando verificação de identificador do modelo\n",
       "                    if key.endswith('_output.jsonl') and model_id.replace(':', '-') in key:\n",
       "                        result[\"model\"].append(model_id)\n",
       "                        result[\"key\"].append(key)\n",
       "                        break\n",
       "            \n",
       "            except Exception as e:\n",
       "                print(f\"❌ Erro listando objetos para {model_id}: {str(e)}\")\n",
       "        else:\n",
       "            print(\"\\x1b[31mJob de avaliação de qualidade ainda está em andamento, por favor aguarde..\\x1b[0m\")\n",
       "            sys.exit()\n",
       "    \n",
       "    return result\n",
       "\n",
       "print(\"✅ Função de verificação criada! Vamos detectar as chaves automaticamente.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 🔍 DETECTANDO CHAVES DE SAÍDA DO S3\n",
       "print(\"🔍 DETECTANDO CHAVES DE SAÍDA DO S3...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "# Uso\n",
       "s3_output_keys = get_s3_output_keys(evaluation_tracking)\n",
       "\n",
       "# Agora s3_output_keys contém um dicionário mapeando IDs de modelo para suas chaves de saída S3\n",
       "print(\"\\n�� Chaves de saída S3 detectadas automaticamente:\")\n",
       "for model, key in zip(s3_output_keys[\"model\"], s3_output_keys[\"key\"]):\n",
       "    print(f\"  {model}: {key}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# �� EXTRAINDO MÉTRICAS DE QUALIDADE\n",
       "print(\"�� EXTRAINDO MÉTRICAS DE QUALIDADE...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "### Métricas de qualidade\n",
       "file_key_df = pd.DataFrame(s3_output_keys)\n",
       "model_quality_list = []\n",
       "\n",
       "for index, row in file_key_df.iterrows():  \n",
       "    metrics_dict = {}\n",
       "\n",
       "    model = row[\"model\"]\n",
       "    if row[\"key\"] == \"\":\n",
       "        continue\n",
       "        \n",
       "    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=row[\"key\"])\n",
       "    content = response['Body'].read().decode('utf-8')\n",
       "    \n",
       "    for line in content.strip().split('\\n'):\n",
       "        if line:\n",
       "            data = json.loads(line)\n",
       "            if 'automatedEvaluationResult' in data and 'scores' in data['automatedEvaluationResult']:\n",
       "                for score in data['automatedEvaluationResult']['scores']:\n",
       "                    metric_name = score['metricName']\n",
       "                    if 'result' in score:\n",
       "                        metric_value = score['result']\n",
       "                        if metric_name not in metrics_dict:\n",
       "                            metrics_dict[metric_name] = []\n",
       "                        metrics_dict[metric_name].append(metric_value)\n",
       "    \n",
       "    df = pd.DataFrame(metrics_dict)\n",
       "    df['model'] = model\n",
       "    model_quality_average = df.groupby(\"model\").mean()\n",
       "    model_quality_average = model_quality_average.reset_index()\n",
       "    model_quality_list.append(model_quality_average)\n",
       "\n",
       "model_quality = pd.concat(model_quality_list, axis=0, ignore_index=True)\n",
       "print(\"�� MÉTRICAS DE QUALIDADE EXTRAÍDAS:\")\n",
       "print(model_quality)\n",
       "\n",
       "print(\"\\n💡 Estes são os scores de qualidade para cada modelo.\")\n",
       "print(\"�� Scores vão de 0 a 1, onde 1 é a melhor qualidade possível.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "> **💡 Nota do Workshop**: Esses resultados de avaliação podem não conseguir diferenciar o suficiente entre os modelos. Isso é porque temos um tamanho de amostra muito pequeno (n=10), por questão de tempo. Em implementação real, você precisará aumentar o tamanho da amostra para medir com precisão a qualidade dos modelos."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Análise Consolidada de Métricas**\n",
       "\n",
       "#### **Combinando Dados de Performance, Qualidade e Custo**\n",
       "\n",
       "Com nossas métricas de qualidade agora adicionadas aos nossos dados de performance e custo, temos uma visão completa das capacidades de cada modelo. Este framework de métricas consolidado nos permite:\n",
       "\n",
       "1. **Comparar trade-offs**: Ver como os modelos balanceiam velocidade, qualidade e custo\n",
       "2. **Identificar pontos fortes**: Determinar quais modelos se destacam em dimensões específicas\n",
       "3. **Combinar com requisitos**: Alinhar capacidades com prioridades específicas da aplicação\n",
       "\n",
       "Esta análise multidimensional é essencial para tomar decisões informadas que vão além do pensamento simplista de \"melhor modelo\". Diferentes aplicações têm requisitos únicos - um chatbot de atendimento ao cliente pode priorizar qualidade de resposta e tom, enquanto uma aplicação de processamento de alto volume pode favorecer velocidade e eficiência de custo.\n",
       "\n",
       "O dataframe de métricas mescladas que criamos serve como base para nossas visualizações e relatório final, fornecendo uma imagem clara das vantagens relativas de cada opção de modelo."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# �� MESCLANDO MÉTRICAS\n",
       "print(\"�� MESCLANDO MÉTRICAS...\")\n",
       "print(\"=\" * 30)\n",
       "\n",
       "## Mesclando métricas para o dataframe de métricas\n",
       "metrics = pd.merge(metrics, model_quality, on=['model'])\n",
       "display(metrics)\n",
       "\n",
       "print(\"\\n🎉 Todas as métricas foram consolidadas em um único dataframe!\")\n",
       "print(\"💡 Agora temos latência, custo e qualidade para todos os modelos.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Resumo de Análise Aprimorado por IA**\n",
       "\n",
       "#### **Usando LLMs para Interpretar Resultados de Avaliação**\n",
       "\n",
       "Uma das capacidades poderosas dos LLMs avançados é sua habilidade de analisar dados complexos e gerar resumos perspicazes. Nesta seção, vamos aproveitar essa capacidade perguntando ao Claude Haiku para interpretar nossos resultados de avaliação e fornecer um resumo conciso dos principais achados.\n",
       "\n",
       "Esta abordagem demonstra um padrão importante para trabalhar com modelos foundation: usá-los não apenas como geradores de conteúdo, mas como ferramentas analíticas que podem extrair insights de dados estruturados.\n",
       "\n",
       "O resumo resultante fornece uma interpretação legível por humanos dos nossos dados, complementando nossas visualizações e dados brutos com insights narrativos.\n",
       "\n",
       "> **💡 Nota do Workshop**: Este padrão de \"LLM como analista de dados\" pode ser aplicado a muitos cenários de business intelligence além da avaliação de modelos. Considere como sua organização pode aproveitar modelos foundation para gerar insights de outros datasets complexos."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 🤖 CONFIGURANDO CLIENTE BEDROCK\n",
       "client = boto3.client(\"bedrock-runtime\")\n",
       "\n",
       "print(\"✅ Cliente Bedrock configurado! Vamos gerar insights com IA.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 🤖 GERANDO RESUMO DE ANÁLISE\n",
       "print(\"🤖 GERANDO RESUMO DE ANÁLISE COM IA...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "summary_prompt = \"\"\"Usando o dataset fornecido abaixo, crie um resumo conciso de 3 frases que identifica qual modelo performa melhor em cada uma dessas três categorias de métricas:\n",
       "1. Métricas de latência (latency_mean, latency_p50, latency_p90)\n",
       "2. Métricas de custo (avg_cost)\n",
       "3. Métricas de qualidade (Builtin.Correctness e completude de saída como sugerido por avg_output_tokens)\n",
       "4. Não comece com \"Baseado no fornecido...\", apenas dê seu resumo\n",
       "O resumo deve declarar claramente qual modelo é ótimo para usuários priorizando velocidade, eficiência de custo ou qualidade de saída.\n",
       "\n",
       "Dataset: {dataset}\n",
       "\"\"\"\n",
       "\n",
       "max_tokens=1000\n",
       "temperature=0\n",
       "top_p=0.9\n",
       "\n",
       "response = client.converse(\n",
       "    modelId=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
       "    messages=[\n",
       "        {\n",
       "            \"role\": \"user\",\n",
       "            \"content\": [\n",
       "                {\n",
       "                    \"text\": summary_prompt.format(dataset=metrics)\n",
       "                }\n",
       "            ]\n",
       "        }\n",
       "    ],\n",
       "    inferenceConfig={\n",
       "        \"temperature\": temperature,\n",
       "        \"maxTokens\": max_tokens,\n",
       "        \"topP\": top_p\n",
       "    }\n",
       ")\n",
       "\n",
       "analysis_summary = response['output']['message']['content'][0]['text']\n",
       "\n",
       "print(\"📊 RESUMO DE ANÁLISE GERADO:\")\n",
       "print(\"-\" * 40)\n",
       "print(analysis_summary)\n",
       "\n",
       "print(\"\\n💡 Este resumo foi gerado automaticamente pela IA analisando nossos dados!\")\n",
       "print(\" Ele nos dá uma visão rápida e objetiva dos resultados.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Geração do Relatório Final**\n",
       "\n",
       "#### **Criando Documentação de Análise Abrangente**\n",
       "\n",
       "O culminar do nosso processo de avaliação é um relatório formatado profissionalmente que apresenta nossos achados em um formato claro e visualmente atraente. Este relatório serve múltiplos propósitos:\n",
       "\n",
       "1. **Documentação**: Cria um registro permanente da nossa metodologia de avaliação e resultados\n",
       "2. **Comunicação**: Fornece artefatos compartilháveis para discussões com stakeholders\n",
       "3. **Suporte à Decisão**: Organiza informações para facilitar escolhas informadas\n",
       "\n",
       "Nossa utilidade `generate_analysis_report` lida com o trabalho complexo de:\n",
       "- Criar visualizações consistentes entre dimensões\n",
       "- Formatar tabelas para legibilidade\n",
       "- Gerar gráficos de distribuição de performance\n",
       "- Incluir nosso resumo gerado por IA junto com métricas brutas\n",
       "\n",
       "A saída final inclui tanto um relatório PDF quanto um resumo CSV, fornecendo opções tanto para revisões de alto nível quanto para análise detalhada.\n",
       "\n",
       "> **💡 Nota do Workshop**: Após executar esta célula, verifique o diretório `../outputs-analysis/` para visualizar seu relatório gerado. Este relatório pode servir como template para sua própria documentação de avaliação de modelos."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 📊 GERANDO RELATÓRIO FINAL\n",
       "print(\"📊 GERANDO RELATÓRIO FINAL...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "report = generate_analysis_report.Analysis_Report()\n",
       "report.generate_report(latency_evaluation_raw, directory, metrics, analysis_summary, total_quality_evaluation_cost, prompt_optimization_cost)\n",
       "\n",
       "print(\"\\n🎉 RELATÓRIO GERADO COM SUCESSO!\")\n",
       "print(\"�� Verifique o diretório ../outputs-analysis/ para ver os arquivos gerados.\")\n",
       "print(\"�� Você encontrará um relatório PDF e um resumo CSV.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Resumo e Principais Takeaways**\n",
       "\n",
       "#### **Parabéns por Completar o Workshop de Avaliação de Modelos!**\n",
       "\n",
       "Você navegou com sucesso por todo o processo de avaliação e migração de modelos, ganhando experiência prática com uma metodologia que pode ser aplicada aos seus próprios projetos GenAI. Neste notebook final, você:\n",
       "\n",
       "1. ✅ **Consolidou métricas multidimensionais** entre latência, qualidade e custo\n",
       "2. ✅ **Calculou implicações econômicas** de diferentes escolhas de modelo em escala  \n",
       "3. ✅ **Analisou avaliações de qualidade** das avaliações LLM-as-a-Judge\n",
       "4. ✅ **Gerou insights aprimorados por IA** para interpretar dados complexos de avaliação\n",
       "5. ✅ **Criou documentação profissional** para apoiar tomada de decisão\n",
       "\n",
       "### **Principais Takeaways**\n",
       "\n",
       "1. **Seleção de Modelo é Multidimensional**: Raramente há um único modelo \"melhor\" - diferentes modelos se destacam em áreas diferentes, e a escolha ótima depende dos seus requisitos específicos.\n",
       "2. **Migração Baseada em Dados**: Migrações bem-sucedidas de modelos requerem medições objetivas em vez de apenas suposições ou especificações.\n",
       "3. **Análise de Trade-offs**: Entender a relação entre velocidade, qualidade e custo permite decisões informadas que balanceiam prioridades concorrentes.\n",
       "4. **Documentação Importa**: Documentação completa da sua metodologia de avaliação e resultados ajuda a construir consenso e apoiar decisões futuras de migração.\n",
       "5. **Avaliação Contínua**: Conforme novos modelos são lançados e existentes são atualizados, o processo de avaliação deve ser repetido periodicamente para garantir que você está usando componentes ótimos.\n",
       "\n",
       "### **Próximos Passos**\n",
       "\n",
       "Considere aplicar este framework de avaliação aos seus próprios casos de uso:\n",
       "- **Personalize critérios de sucesso**: Defina thresholds específicos de aceitação baseados nos requisitos da sua aplicação\n",
       "- **Amplie a comparação**: Avalie mais modelos entre diferentes provedores e arquiteturas\n",
       "- **Expanda as métricas**: Adicione critérios de avaliação específicos do domínio relevantes para suas aplicações\n",
       "- **Continue otimização de prompts**: Lembre-se que engenharia de prompts é um esforço contínuo - considere construir um pipeline sistemático para testar variações de prompts com métricas automatizadas ou avaliação human-in-the-loop\n",
       "- **Explore mais opções de Inferência**: Aprenda e explore mais opções de otimização de inferência no Bedrock que podem melhorar a latência para seus casos de uso, ex: [Inferência Otimizada](https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html), [Cache de Prompts](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html), [Perfil de Inferência](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles.html) e outros.\n",
       "- **Construa componente de avaliação RAG**: Se seu caso de uso requer uma base de conhecimento construída com RAG, considere adicionar um componente de avaliação de qualidade para RAG. Opções de avaliação RAG incluem [Bedrock RAG evaluator](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-kb.html), soluções open source como [RAGAS](https://docs.ragas.io/en/stable/getstarted/rag_eval/). \n",
       "- **Automatize o workflow**: Integre estes passos de avaliação no seu pipeline CI/CD para avaliação contínua de modelos\n",
       "- **Estabeleça loops de feedback**: Crie mecanismos para capturar dados de performance de produção para informar futuras otimizações de prompts e modelos\n",
       "\n",
       "Obrigado por participar deste workshop! Esperamos que as habilidades e metodologia que você aprendeu ajudem você a tomar decisões confiantes e baseadas em dados sobre seleção e migração de modelos na sua jornada GenAI.\n",
       "\n",
       "---\n",
       "\n",
       "**💡 Dica Final do Pedro**: Lembre-se, migração de modelo não é uma corrida - é uma maratona! Tome seu tempo, teste bem, e sempre mantenha a qualidade em mente. É melhor ter um modelo um pouco mais lento que funciona perfeitamente do que um super rápido que quebra toda hora!\n",
       "\n",
       "**�� Próximo passo**: Aplicar esta metodologia aos seus próprios projetos!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }