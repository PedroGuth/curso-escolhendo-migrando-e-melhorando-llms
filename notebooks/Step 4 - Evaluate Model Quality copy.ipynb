{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Evaluate Model Quality\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Quality evaluation is the second critical dimension in our model migration framework, alongside latency and cost. Assessing response quality ensures that your migrated solution maintains or improves upon the capabilities of your source model.\n",
    "\n",
    "In this notebook, we'll implement automated quality evaluation using **[Amazon Bedrock's LLM-as-a-Judge](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-judge.html)** feature, which leverages foundation models to objectively assess model outputs across multiple dimensions.\n",
    "\n",
    "### Understanding LLM-as-a-Judge Evaluation\n",
    "\n",
    "LLM-as-a-Judge works by:\n",
    "\n",
    "1. Taking your model's responses along with optional reference answers (ground truth)\n",
    "2. Using a specialized evaluator model to assess the quality of those responses\n",
    "3. Providing standardized scores across dimensions like correctness, completeness, and style\n",
    "\n",
    "This approach offers several advantages over traditional human evaluation:\n",
    "- **Consistency**: Applies the same standards across all evaluated responses\n",
    "- **Scalability**: Can evaluate thousands of responses quickly\n",
    "- **Objectivity**: Reduces human bias in the evaluation process\n",
    "- **Reproducibility**: Produces consistent results for the same inputs\n",
    "\n",
    "### Our Evaluation Approach\n",
    "\n",
    "For this workshop, we will:\n",
    "1. Format the responses generated in Step 3 for evaluation\n",
    "2. Create separate evaluation jobs for each model (source and candidates)\n",
    "3. Upload our datasets to S3 for processing\n",
    "4. Configure and start the evaluation jobs\n",
    "5. Store job references for analysis in the next step\n",
    "\n",
    "Let's begin by ensuring we have the necessary dependencies installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_client = boto3.client('bedrock')\n",
    "s3_client = boto3.client('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "For this notebook to run successfully, you'll need:\n",
    "\n",
    "- **IAM Role with Proper Permissions**: For creating evaluation jobs and accessing S3\n",
    "  - _Note: In the hosted workshop, these roles are pre-configured for you_\n",
    "  - _For self-paced learners: Follow the workshop instructions to create the required role_\n",
    "- **S3 Bucket**: For storing evaluation datasets and results\n",
    "  - _Note: The workshop creates this automatically with the correct permissions_\n",
    "  - _For self-paced learners: Create your own bucket and update the bucket name in this notebook_\n",
    "- **Evaluation Models**: Access to models like Claude  Haiku/Sonnet, Amazon Nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "ROLE_ARN = f\"arn:aws:iam::{account_id}:role/service-role/Bedrock-LLM-as-a-Judge-ExecutionRole\"\n",
    "\n",
    "BUCKET_NAME = f\"genai-evaluation-migration-bucket-{account_id}\"\n",
    "PREFIX = \"genai_migration\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_clean_name</th>\n",
       "      <th>text_prompt</th>\n",
       "      <th>region</th>\n",
       "      <th>inference_profile</th>\n",
       "      <th>latency_evaluation_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>source_model</td>\n",
       "      <td>source_model</td>\n",
       "      <td>\\nFirst, please read the article below.\\n{cont...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>../outputs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon.nova-lite-v1:0</td>\n",
       "      <td>amazon.nova-lite-v1-0</td>\n",
       "      <td>## Instruction\\nYour task is to read the given...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>../outputs/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1:0</td>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1-0</td>\n",
       "      <td>&lt;task&gt;\\nYour task is to provide an extremely c...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "      <td>../outputs/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model  \\\n",
       "0                                 source_model   \n",
       "1                        amazon.nova-lite-v1:0   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1:0   \n",
       "\n",
       "                              model_clean_name  \\\n",
       "0                                 source_model   \n",
       "1                        amazon.nova-lite-v1-0   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1-0   \n",
       "\n",
       "                                         text_prompt     region  \\\n",
       "0  \\nFirst, please read the article below.\\n{cont...  us-east-1   \n",
       "1  ## Instruction\\nYour task is to read the given...  us-east-1   \n",
       "2  <task>\\nYour task is to provide an extremely c...  us-east-1   \n",
       "\n",
       "  inference_profile latency_evaluation_output  \n",
       "0          standard               ../outputs/  \n",
       "1          standard               ../outputs/  \n",
       "2          standard               ../outputs/  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "### load our tracking df\n",
    "\n",
    "evaluation_tracking_file = '../data/evaluation_tracking.csv'\n",
    "evaluation_tracking = pd.read_csv(evaluation_tracking_file)\n",
    "display(evaluation_tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Quality Evaluation\n",
    "\n",
    "### Formatting Responses for LLM-as-a-Judge\n",
    "\n",
    "To evaluate our model outputs using Bedrock's LLM-as-a-Judge, we need to format our data in a specific JSONL structure. Each record in this format contains:\n",
    "\n",
    "| Field | Description | Required? | Our Usage |\n",
    "|-------|-------------|-----------|-----------|\n",
    "| `prompt` | The original input prompt | Required | The full summarization prompt with document |\n",
    "| `referenceResponse` | Ground truth/reference answer | Optional | The human-written summary from our dataset |\n",
    "| `category` | Domain category for specialized evaluation | Optional | \"Summarization\" for our use case |\n",
    "| `modelResponses` | Array of model responses to evaluate | Required | Our model's generated summary |\n",
    "\n",
    "Within `modelResponses`, we include:\n",
    "\n",
    "- `response`: The actual text output from our model\n",
    "- `modelIdentifier`: A unique identifier for the model (used for tracking)\n",
    "\n",
    "This format allows the judge model to compare each response against both the original prompt and the reference answer, providing comprehensive quality assessment.\n",
    "\n",
    "> **Note**: While you can evaluate multiple responses per prompt in other Bedrock evaluation methods, LLM-as-a-Judge supports only one response per prompt in each evaluation job. That's why we're creating separate jobs for each model.\n",
    "\n",
    "For more details on the format requirements, see the [Bedrock documentation on evaluation datasets](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets-judge.html).\n",
    "\n",
    "Below, we'll define a function to convert our CSV results from Step 3 into this specialized JSONL format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_inference_response(input_file, output_file, model_clean_name):\n",
    "    \"\"\"\n",
    "    Convert a CSV file with call transcript data to JSONL format with specific structure, for LLM-AS-A-JUDGE.\n",
    "    \"\"\"\n",
    "\n",
    "    random_sample = df.sample(n=10)\n",
    "\n",
    "    # Initialize a list to store all JSONL entries\n",
    "    jsonl_entries = []\n",
    "    \n",
    "    # Process each row individually without grouping\n",
    "    for idx, row in random_sample.iterrows():\n",
    "        prompt = row['prompt']\n",
    "        \n",
    "            \n",
    "        # Create the entry for this row\n",
    "        entry = {\n",
    "            \"prompt\": prompt,  # Store the full prompt as is\n",
    "            \"category\": \"Summarization\",\n",
    "            \"referenceResponse\":row['referenceResponse'],\n",
    "            \"modelResponses\": [{\n",
    "                \"response\": row['model_response'],\n",
    "                \"modelIdentifier\": model_clean_name\n",
    "            }]\n",
    "        }\n",
    "            \n",
    "        jsonl_entries.append(entry)\n",
    "    \n",
    "    # Write to JSONL file\n",
    "    with open(output_file, 'w') as f:\n",
    "        for entry in jsonl_entries:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    print(f\"Conversion complete. JSONL file saved to {output_file}\")\n",
    "    print(f\"Total entries: {len(jsonl_entries)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Integration for Evaluation Datasets\n",
    "\n",
    "### Managing Evaluation Data in the Cloud\n",
    "\n",
    "Our evaluation jobs run as asynchronous processes in AWS Bedrock, which requires our datasets to be accessible via S3. \n",
    "\n",
    "### Upload Process\n",
    "\n",
    "We'll implement a function to upload our formatted JSONL files to S3 with proper error handling. This function will:\n",
    "\n",
    "- Take a local file path, target bucket, and desired S3 key\n",
    "- Handle the upload process with proper error checking\n",
    "- Provide confirmation when the upload succeeds\n",
    "\n",
    "> **Note for Self-Paced Users**: If you're running this workshop in your own environment, ensure your IAM role or user has appropriate permissions (s3:PutObject) for your target bucket. You can verify permissions by checking the IAM policy attached to your role or user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(local_file, bucket, s3_key):\n",
    "    \"\"\"\n",
    "    Upload a file to S3 with error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(local_file, bucket, s3_key)\n",
    "        print(f\"✓ Successfully uploaded to s3://{bucket}/{s3_key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error uploading to S3: {str(e)}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing and Uploading Evaluation Datasets\n",
    "\n",
    "### Converting Model Responses to Evaluation Format\n",
    "\n",
    "Now we'll convert our model response data into properly formatted evaluation datasets and upload them to S3. For each model:\n",
    "\n",
    "1. We'll locate the appropriate CSV files containing model responses\n",
    "2. Format them according to LLM-as-a-Judge requirements\n",
    "3. Upload the formatted data to our S3 bucket\n",
    "4. Track the S3 locations in our evaluation tracking dataframe\n",
    "\n",
    "This process creates separate evaluation datasets for each candidate model, allowing us to run parallel quality evaluations while maintaining clear organization of our test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. JSONL file saved to ../outputs/quality_evaluation.source_model.jsonl\n",
      "Total entries: 10\n",
      "✓ Successfully uploaded to s3://genai-evaluation-migration-bucket-339712833052/genai_migration/quality_evaluation.source_model.jsonl\n",
      "Conversion complete. JSONL file saved to ../outputs/quality_evaluation.amazon.nova-lite-v1:0.jsonl\n",
      "Total entries: 10\n",
      "✓ Successfully uploaded to s3://genai-evaluation-migration-bucket-339712833052/genai_migration/quality_evaluation.amazon.nova-lite-v1-0.jsonl\n",
      "Conversion complete. JSONL file saved to ../outputs/quality_evaluation.us.anthropic.claude-3-5-haiku-20241022-v1:0.jsonl\n",
      "Total entries: 10\n",
      "✓ Successfully uploaded to s3://genai-evaluation-migration-bucket-339712833052/genai_migration/quality_evaluation.us.anthropic.claude-3-5-haiku-20241022-v1-0.jsonl\n"
     ]
    }
   ],
   "source": [
    "## tracking\n",
    "evaluation_tracking['quality_evaluation_input'] = \"\"\n",
    "\n",
    "for index, evaluation in evaluation_tracking.iterrows():\n",
    "    model_id = evaluation['model']\n",
    "    model_clean_name = evaluation['model_clean_name']\n",
    "    latency_evaluation_output = evaluation['latency_evaluation_output']\n",
    "\n",
    "    outputs = glob.glob(os.path.join(latency_evaluation_output, f\"document_summarization_{model_id}*.csv\"))\n",
    "\n",
    "    quality_evaluation_file = f\"../outputs/quality_evaluation.{model_id}.jsonl\"\n",
    "    \n",
    "    ## if experiment_counts >1, there will be more than 1 output\n",
    "    document_summarization_df_list = []\n",
    "        \n",
    "    for output in outputs:\n",
    "        df = pd.read_csv(output)\n",
    "        document_summarization_df_list.append(df)\n",
    "        \n",
    "    document_summarization_df = pd.concat(document_summarization_df_list, axis=0, ignore_index=True) \n",
    "\n",
    "    ## generate a jsonl file for LLM-AS-A-JUDGE evaluation\n",
    "    prepare_inference_response(document_summarization_df, quality_evaluation_file, model_clean_name)\n",
    "\n",
    "    ## upload to s3\n",
    "    s3_key = f\"{PREFIX}/quality_evaluation.{model_clean_name}.jsonl\"\n",
    "    upload_success = upload_to_s3(quality_evaluation_file, BUCKET_NAME, s3_key)\n",
    "\n",
    "    if not upload_success:\n",
    "        raise Exception(\"Failed to upload dataset to S3\")\n",
    "   \n",
    "    ## record the s3 path, we will need them to create LLM-AS-A-JUDGE evaluation jobs \n",
    "    evaluation_tracking.loc[evaluation_tracking['model'] == model_id, 'quality_evaluation_input'] = f\"s3://{BUCKET_NAME}/{s3_key}\"\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Job Configuration\n",
    "\n",
    "### Configuring Comprehensive Quality Metrics\n",
    "\n",
    "Now we'll configure the LLM-as-Judge evaluation with focused metrics for assessing model performance. The Bedrock evaluation service supports numerous dimensions for quality assessment:\n",
    "\n",
    "| Metric Category | Description |\n",
    "|----------------|-------------|\n",
    "| Quality | Correctness, Completeness, Faithfulness |\n",
    "| User Experience | Helpfulness, Coherence, Relevance |\n",
    "| Instructions | Following Instructions, Professional Style |\n",
    "| Safety | Harmfulness, Stereotyping, Refusal |\n",
    "\n",
    "For this workshop, we're focusing on core quality metrics to balance evaluation depth with execution time. In a production evaluation, you might expand to include more dimensions based on your specific use case requirements.\n",
    "\n",
    "> **Workshop Note**: LLM-as-Judge evaluation takes time to complete. For this workshop, we're measuring a subset of metrics: `Builtin.Correctness`, `Builtin.Completeness`, and `Builtin.ProfessionalStyleAndTone`. In production scenarios, you might include more metrics or create your own metrics for comprehensive evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_llm_judge_evaluation(\n",
    "    client,\n",
    "    job_name: str,\n",
    "    role_arn: str,\n",
    "    input_s3_uri: str,\n",
    "    output_s3_uri: str,\n",
    "    model_clean_name: str,\n",
    "    evaluator_model_id: str,\n",
    "    dataset_name: str = None,\n",
    "    task_type: str = \"General\" # must be General for LLMaaJ\n",
    "):    \n",
    "    # All available LLM-as-judge metrics\n",
    "    llm_judge_metrics = [\n",
    "        \"Builtin.Correctness\",\n",
    "         \"Builtin.Completeness\", \n",
    "        # \"Builtin.Faithfulness\",\n",
    "        # \"Builtin.Helpfulness\",\n",
    "        # \"Builtin.Coherence\",\n",
    "        # \"Builtin.Relevance\",\n",
    "        # \"Builtin.FollowingInstructions\",\n",
    "         \"Builtin.ProfessionalStyleAndTone\"\n",
    "        # \"Builtin.Harmfulness\",\n",
    "        # \"Builtin.Stereotyping\",\n",
    "        # \"Builtin.Refusal\"\n",
    "    ]\n",
    "\n",
    "    # Configure dataset\n",
    "    dataset_config = {\n",
    "        \"name\": dataset_name or \"CustomDataset\",\n",
    "        \"datasetLocation\": {\n",
    "            \"s3Uri\": input_s3_uri\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.create_evaluation_job(\n",
    "            jobName=job_name,\n",
    "            roleArn=role_arn,\n",
    "            applicationType=\"ModelEvaluation\",\n",
    "            evaluationConfig={\n",
    "                \"automated\": {\n",
    "                    \"datasetMetricConfigs\": [\n",
    "                        {\n",
    "                            \"taskType\": task_type,\n",
    "                            \"dataset\": dataset_config,\n",
    "                            \"metricNames\": llm_judge_metrics\n",
    "                        }\n",
    "                    ],\n",
    "                    \"evaluatorModelConfig\": {\n",
    "                        \"bedrockEvaluatorModels\": [\n",
    "                            {\n",
    "                                \"modelIdentifier\": evaluator_model_id\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            inferenceConfig={\n",
    "                \"models\": [\n",
    "                    {\n",
    "                        'precomputedInferenceSource': {\n",
    "                            'inferenceSourceIdentifier': model_clean_name\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            outputDataConfig={\n",
    "                \"s3Uri\": output_s3_uri\n",
    "            }\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating evaluation job: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Evaluation Jobs\n",
    "\n",
    "### Launching Quality Assessment for Each Model\n",
    "\n",
    "With our evaluation datasets prepared and uploaded to S3, we're now ready to create separate evaluation jobs for each model.\n",
    "\n",
    "Each evaluation job will:\n",
    "- Use an advanced foundation model as the evaluator (Amazon Nova Pro in this case)\n",
    "- Apply the same evaluation criteria across all models\n",
    "- Generate standardized outputs for consistent comparison\n",
    "- Store results in organized S3 locations for later analysis\n",
    "\n",
    "The execution of these jobs may take several minutes as the evaluator model carefully assesses each response across multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created evaluation job: arn:aws:bedrock:us-east-1:339712833052:evaluation-job/bcly6857m113\n",
      "✓ Created evaluation job: arn:aws:bedrock:us-east-1:339712833052:evaluation-job/3zkkftou112q\n",
      "✓ Created evaluation job: arn:aws:bedrock:us-east-1:339712833052:evaluation-job/z33p2of2ey1o\n"
     ]
    }
   ],
   "source": [
    "output_path = f\"s3://{BUCKET_NAME}/{PREFIX}\"\n",
    "\n",
    "\n",
    "evaluation_tracking['quality_evaluation_jobArn'] = \"\"\n",
    "evaluation_tracking['quality_evaluation_output'] = \"\"\n",
    "\n",
    "for index, evaluation in evaluation_tracking.iterrows():\n",
    "    model_id = evaluation['model']\n",
    "    model_clean_name = evaluation['model_clean_name']\n",
    "    quality_evaluation_input = evaluation['quality_evaluation_input']\n",
    "\n",
    "    evaluator_model = \"amazon.nova-pro-v1:0\" #https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-judge.html\n",
    "\n",
    "    if model_id == \"source_model\":\n",
    "        model_id = model_id.replace(\"_\", \"-\")\n",
    "    \n",
    "    job_name = f\"llmaaj-{model_id.split('.')[0]}-{evaluator_model.split('.')[0]}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    \n",
    "\n",
    "    # Create evaluation job\n",
    "    try:\n",
    "        llm_as_judge_response = create_llm_judge_evaluation(\n",
    "            client=bedrock_client,\n",
    "            job_name=job_name,\n",
    "            role_arn=ROLE_ARN,\n",
    "            input_s3_uri=quality_evaluation_input,\n",
    "            output_s3_uri=output_path,\n",
    "            model_clean_name = model_clean_name,\n",
    "            evaluator_model_id=evaluator_model,\n",
    "            task_type=\"General\"\n",
    "        )\n",
    "        print(f\"✓ Created evaluation job: {llm_as_judge_response['jobArn']}\")\n",
    "\n",
    "        ## record the s3 path, we will need them to retrieve LLM-AS-A-JUDGE evaluation jobs \n",
    "        evaluation_tracking.loc[evaluation_tracking['model_clean_name'] == model_clean_name, 'quality_evaluation_jobArn'] = llm_as_judge_response['jobArn']\n",
    "        evaluation_tracking.loc[evaluation_tracking['model_clean_name'] == model_clean_name, 'quality_evaluation_output'] = f\"{output_path}/{job_name}/\"\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to create evaluation job: {str(e)}\")\n",
    "        raise\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Evaluation Progress\n",
    "\n",
    "### Saving Job References for Analysis\n",
    "\n",
    "Now that we've initiated all our evaluation jobs, we'll save our tracking information to ensure we can locate and analyze the results in the next notebook. This tracking includes:\n",
    "\n",
    "- Job ARNs for monitoring status and retrieving results\n",
    "- S3 output locations where results will be stored\n",
    "- Metadata about each evaluation job for reference\n",
    "\n",
    "This information forms the bridge between our evaluation execution and the final analysis step, ensuring we can efficiently access all the data needed for our comprehensive model comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the progress\n",
    "\n",
    "evaluation_tracking.to_csv(evaluation_tracking_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. ✅ **Prepared** evaluation datasets in the correct format for quality assessment\n",
    "2. ✅ **Uploaded** these datasets to S3 for processing by Bedrock evaluation jobs\n",
    "3. ✅ **Configured** comprehensive quality metrics focused on correctness, completeness, and style\n",
    "4. ✅ **Created** separate evaluation jobs for each candidate model \n",
    "5. ✅ **Tracked** job references and output locations for subsequent analysis\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "\n",
    "In the next notebook, **Step 5 - Compare Models**, we'll consolidate all our evaluation data across the three critical dimensions: quality, latency, and cost. We'll analyze the results from our evaluation jobs, create visualizations to highlight the trade-offs between different models, and produce a comprehensive migration recommendation based on our defined success criteria.\n",
    "\n",
    "The final analysis will provide a data-driven basis for selecting the optimal model for your production deployment, balancing performance, efficiency, and cost effectiveness.\n",
    "\n",
    "> <span style=\"color:red\"> **Note**: You can navigate the the [Bedrock console](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/eval/evaluation) page to view the LLM-as-a-Judge Evaluation progress.\n",
    "While our evaluation jobs continue to run, we can proceed to the next notebook to set up our analysis framework. The final results will be incorporated once the jobs complete.</span>\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
