{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **Passo 3: Avaliando a Velocidade dos Modelos**\n",
       "\n",
       "## **Aula 3.1: Por Que Lat√™ncia √© Importante?**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© lat√™ncia?**\n",
       "\n",
       "Imagina que voc√™ t√° num restaurante e pede um hamb√∫rguer. Se o gar√ßom demora 2 minutos pra trazer, voc√™ fica feliz. Se demora 20 minutos, voc√™ fica puto! √â exatamente isso que a gente vai medir aqui - quanto tempo cada modelo de IA leva pra \"cozinhar\" sua resposta. üòÑ\n",
       "\n",
       "**Por que lat√™ncia √© importante?**\n",
       "\n",
       "Em aplica√ß√µes reais, velocidade √© tudo! Se voc√™ t√° fazendo um chatbot e ele demora 10 segundos pra responder, o usu√°rio j√° foi embora. Se voc√™ t√° processando milhares de documentos por dia, cada segundo conta pro bolso.\n",
       "\n",
       "### **M√©tricas de Lat√™ncia que Vamos Medir**\n",
       "\n",
       "Vamos focar em uma m√©trica principal pra nosso caso de uso:\n",
       "\n",
       "| M√©trica | O Que Mede | Por Que Importa |\n",
       "|---------|------------|-----------------|\n",
       "| **Lat√™ncia Total** | Tempo total do pedido at√© resposta completa | Experi√™ncia do usu√°rio final |\n",
       "| **P50/P90** | 50% e 90% das respostas mais r√°pidas | Performance consistente |\n",
       "| **Desvio Padr√£o** | Qu√£o vari√°vel √© a velocidade | Previsibilidade |\n",
       "\n",
       "### **O Que Esta Avalia√ß√£o Produz**\n",
       "\n",
       "#### **1. Arquivos de Log Detalhados**\n",
       "\n",
       "Um arquivo de log √© gerado automaticamente como `model_latency_benchmarking-{timestamp}.log`, rastreando todas as chamadas de API, erros e detalhes de execu√ß√£o. √â como ter um \"gravador de voo\" pra cada teste!\n",
       "\n",
       "#### **2. Arquivos CSV de Resultados**\n",
       "\n",
       "Resultados s√£o salvos no diret√≥rio `../outputs/` como `document_summarization_{model_id}_{timestamp}.csv`, contendo m√©tricas chave incluindo lat√™ncia total, tempo de processamento do servidor, contagem de tokens, indicadores de status da API e detalhes de configura√ß√£o.\n",
       "\n",
       "Esses resultados ser√£o usados no Passo 5 pra criar compara√ß√µes abrangentes e visualiza√ß√µes.\n",
       "\n",
       "### **Diretrizes de Benchmarking**\n",
       "\n",
       "Pra avalia√ß√£o estatisticamente v√°lida, considere estes princ√≠pios:\n",
       "\n",
       "| Par√¢metro | Descri√ß√£o | Configura√ß√£o do Workshop | Recomenda√ß√£o de Produ√ß√£o |\n",
       "|-----------|-----------|-------------------------|-------------------------|\n",
       "| `invocations_per_scenario` | Repeti√ß√µes por prompt | 1 (pra efici√™ncia) | 10+ pra signific√¢ncia estat√≠stica |\n",
       "| `experiment_counts` | Vezes de repetir todo experimento | 1 (pra workshop) | M√∫ltiplas execu√ß√µes em dias/hor√°rios diferentes |\n",
       "| `num_parallel_calls` | Requisi√ß√µes concorrentes | 1 (pra evitar throttling) | Combinar com sua concorr√™ncia de produ√ß√£o |\n",
       "\n",
       "> **‚ö†Ô∏è Nota Estat√≠stica**: Enquanto estamos usando par√¢metros simplificados pra este workshop, avalia√ß√µes de produ√ß√£o devem seguir pr√°ticas estat√≠sticas mais rigorosas. O Teorema do Limite Central nos diz que com amostras suficientes (1000+), nossas m√©tricas v√£o aproximar uma distribui√ß√£o normal. Idealmente, voc√™ deve:\n",
       "> - Coletar amostras ao longo de m√∫ltiplos dias pra considerar varia√ß√µes de hor√°rio\n",
       "> - Incluir per√≠odos de pico de tr√°fego na sua amostragem\n",
       "> - Combinar sua distribui√ß√£o de teste com seus padr√µes reais de tr√°fego de produ√ß√£o\n",
       "\n",
       "Vamos come√ßar nossa avalia√ß√£o de lat√™ncia!\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um cron√¥metro ou gr√°fico de tempo de resposta"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üõ†Ô∏è IMPORTANDO AS FERRAMENTAS NECESS√ÅRIAS\n",
       "import subprocess\n",
       "import sys\n",
       "import boto3\n",
       "import botocore\n",
       "import random\n",
       "import pprint\n",
       "import time\n",
       "import json\n",
       "import argparse\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import random\n",
       "from datetime import datetime, timedelta\n",
       "import pytz\n",
       "import os\n",
       "import logging\n",
       "from botocore.config import Config\n",
       "from botocore.exceptions import ClientError\n",
       "import concurrent.futures\n",
       "from concurrent.futures import ThreadPoolExecutor\n",
       "from threading import Lock\n",
       "from typing import List, Dict\n",
       "from tqdm.notebook import tqdm\n",
       "from IPython.display import display\n",
       "\n",
       "print(\"‚úÖ Todas as ferramentas importadas! Vamos medir a velocidade!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Carregando Nosso Progresso**\n",
       "\n",
       "Vamos carregar nosso dataframe de tracking do Passo 2, que cont√©m informa√ß√µes sobre nosso modelo fonte e os modelos candidatos que vamos avaliar. Esse dataframe vai servir como nosso reposit√≥rio central pra todas as m√©tricas de avalia√ß√£o durante o workshop."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üìä CARREGANDO NOSSO TRACKING\n",
       "evaluation_tracking_file = '../data/evaluation_tracking.csv'\n",
       "evaluation_tracking = pd.read_csv(evaluation_tracking_file)\n",
       "display(evaluation_tracking)\n",
       "\n",
       "print(\"\\nüí° Perfeito! Agora temos nosso plano de avalia√ß√£o carregado.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Configura√ß√£o do Setup**\n",
       "\n",
       "#### **Definindo Nossos Par√¢metros de Benchmarking**\n",
       "\n",
       "Os par√¢metros chave que vamos configurar incluem:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ‚öôÔ∏è CONFIGURANDO OS PAR√ÇMETROS DE TESTE\n",
       "\n",
       "# Par√¢metros de benchmarking\n",
       "BENCHMARK_CONFIG = {\n",
       "    'invocations_per_scenario': 1,  # Repeti√ß√µes por prompt (workshop: 1, produ√ß√£o: 10+)\n",
       "    'experiment_counts': 1,  # Vezes de repetir todo experimento (workshop: 1, produ√ß√£o: m√∫ltiplas)\n",
       "    'num_parallel_calls': 1,  # Chamadas concorrentes (workshop: 1, produ√ß√£o: baseado na concorr√™ncia)\n",
       "    'timeout_seconds': 300,  # Timeout pra cada chamada (5 minutos)\n",
       "    'retry_attempts': 3,  # Tentativas de retry em caso de erro\n",
       "    'delay_between_calls': 1.0  # Delay entre chamadas (segundos)\n",
       "}\n",
       "\n",
       "print(\"‚öôÔ∏è CONFIGURA√á√ÉO DE BENCHMARKING:\")\n",
       "for key, value in BENCHMARK_CONFIG.items():\n",
       "    print(f\"‚Ä¢ {key}: {value}\")\n",
       "\n",
       "print(\"\\nüí° Esses s√£o os par√¢metros que vamos usar pra medir a velocidade dos modelos.\")\n",
       "print(\"üí° Em produ√ß√£o, voc√™ aumentaria esses n√∫meros pra ter resultados mais confi√°veis.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Configurando o Sistema de Logging**\n",
       "\n",
       "Vamos configurar um sistema de logging robusto pra rastrear todas as nossas chamadas de API. √â como ter um \"gravador de voo\" que registra tudo que acontece durante os testes!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üìù CONFIGURANDO O SISTEMA DE LOGGING\n",
       "def setup_logging(model_id):\n",
       "    \"\"\"\n",
       "    Configura o sistema de logging pra um modelo espec√≠fico.\n",
       "    √â como configurar uma c√¢mera de seguran√ßa que filma tudo!\n",
       "    \"\"\"\n",
       "    \n",
       "    # Criando nome do arquivo de log\n",
       "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
       "    log_filename = f'model_latency_benchmarking_{model_id.replace(\":\", \"-\")}_{timestamp}.log'\n",
       "    \n",
       "    # Configurando o logger\n",
       "    logging.basicConfig(\n",
       "        level=logging.INFO,\n",
       "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
       "        handlers=[\n",
       "            logging.FileHandler(log_filename),\n",
       "            logging.StreamHandler()\n",
       "        ]\n",
       "    )\n",
       "    \n",
       "    logger = logging.getLogger(__name__)\n",
       "    logger.info(f\"üöÄ Iniciando benchmark para modelo: {model_id}\")\n",
       "    logger.info(f\"üìù Arquivo de log: {log_filename}\")\n",
       "    \n",
       "    return logger, log_filename\n",
       "\n",
       "print(\"‚úÖ Sistema de logging configurado! Vamos rastrear tudo que acontece.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Configurando o Cliente Bedrock**\n",
       "\n",
       "Vamos configurar o cliente Bedrock com configura√ß√µes otimizadas pra benchmarking. √â como ajustar o carro antes de uma corrida!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üîß CONFIGURANDO O CLIENTE BEDROCK\n",
       "def create_bedrock_client(region='us-east-1'):\n",
       "    \"\"\"\n",
       "    Cria um cliente Bedrock otimizado pra benchmarking.\n",
       "    √â como configurar um carro de corrida com as melhores configura√ß√µes!\n",
       "    \"\"\"\n",
       "    \n",
       "    # Configura√ß√µes otimizadas pra performance\n",
       "    config = Config(\n",
       "        region_name=region,\n",
       "        retries={\n",
       "            'max_attempts': 3,\n",
       "            'mode': 'adaptive'\n",
       "        },\n",
       "        connect_timeout=60,\n",
       "        read_timeout=300\n",
       "    )\n",
       "    \n",
       "    client = boto3.client('bedrock-runtime', config=config)\n",
       "    \n",
       "    return client\n",
       "\n",
       "print(\"‚úÖ Cliente Bedrock configurado com otimiza√ß√µes de performance!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Fun√ß√£o Principal de Benchmarking**\n",
       "\n",
       "Agora vamos criar a fun√ß√£o principal que vai fazer o trabalho pesado de medir a lat√™ncia. √â como ter um cron√¥metro profissional que mede tudo com precis√£o!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ‚è±Ô∏è FUN√á√ÉO PRINCIPAL DE BENCHMARKING\n",
       "def benchmark_model_latency(model_id, prompt, dataset_path, logger):\n",
       "    \"\"\"\n",
       "    Executa o benchmark de lat√™ncia para um modelo espec√≠fico.\n",
       "    √â como cronometrar cada volta de uma corrida!\n",
       "    \"\"\"\n",
       "    \n",
       "    results = []\n",
       "    client = create_bedrock_client()\n",
       "    \n",
       "    # Carregando o dataset\n",
       "    dataset = pd.read_csv(dataset_path)\n",
       "    logger.info(f\"ÔøΩÔøΩ Dataset carregado: {len(dataset)} amostras\")\n",
       "    \n",
       "    # Loop principal de benchmarking\n",
       "    for scenario_idx in range(BENCHMARK_CONFIG['experiment_counts']):\n",
       "        logger.info(f\"üîÑ Experimento {scenario_idx + 1}/{BENCHMARK_CONFIG['experiment_counts']}\")\n",
       "        \n",
       "        for invocation_idx in range(BENCHMARK_CONFIG['invocations_per_scenario']):\n",
       "            logger.info(f\"  üìù Invoca√ß√£o {invocation_idx + 1}/{BENCHMARK_CONFIG['invocations_per_scenario']}\")\n",
       "            \n",
       "            # Processando cada documento no dataset\n",
       "            for doc_idx, row in dataset.iterrows():\n",
       "                document = row['document']\n",
       "                reference_response = row['referenceResponse']\n",
       "                \n",
       "                # Formatando o prompt\n",
       "                formatted_prompt = prompt.format(context=document)\n",
       "                \n",
       "                # Fazendo a chamada da API\n",
       "                start_time = time.time()\n",
       "                \n",
       "                try:\n",
       "                    # Preparando a requisi√ß√£o\n",
       "                    request_body = {\n",
       "                        'messages': [\n",
       "                            {\n",
       "                                'role': 'user',\n",
       "                                'content': [{'text': formatted_prompt}]\n",
       "                            }\n",
       "                        ],\n",
       "                        'inferenceConfig': {\n",
       "                            'temperature': 0.7,\n",
       "                            'maxTokens': 1000,\n",
       "                            'topP': 0.9\n",
       "                        }\n",
       "                    }\n",
       "                    \n",
       "                    # Chamada da API\n",
       "                    response = client.converse(\n",
       "                        modelId=model_id,\n",
       "                        body=json.dumps(request_body)\n",
       "                    )\n",
       "                    \n",
       "                    end_time = time.time()\n",
       "                    \n",
       "                    # Extraindo a resposta\n",
       "                    model_response = response['output']['message']['content'][0]['text']\n",
       "                    \n",
       "                    # Calculando m√©tricas\n",
       "                    total_latency = end_time - start_time\n",
       "                    \n",
       "                    # Extraindo informa√ß√µes de tokens (se dispon√≠vel)\n",
       "                    input_tokens = response.get('usage', {}).get('inputTokens', 0)\n",
       "                    output_tokens = response.get('usage', {}).get('outputTokens', 0)\n",
       "                    \n",
       "                    # Criando resultado\n",
       "                    result = {\n",
       "                        'model': model_id,\n",
       "                        'region': 'us-east-1',\n",
       "                        'inference_profile': 'standard',\n",
       "                        'document': document,\n",
       "                        'referenceResponse': reference_response,\n",
       "                        'model_response': model_response,\n",
       "                        'latency': total_latency,\n",
       "                        'model_latencyMs': total_latency * 1000,  # Convertendo pra milissegundos\n",
       "                        'model_input_tokens': input_tokens,\n",
       "                        'model_output_tokens': output_tokens,\n",
       "                        'status': 'success',\n",
       "                        'timestamp': datetime.now().isoformat()\n",
       "                    }\n",
       "                    \n",
       "                    results.append(result)\n",
       "                    \n",
       "                    logger.info(f\"    ‚úÖ Documento {doc_idx + 1}: {total_latency:.3f}s\")\n",
       "                    \n",
       "                except Exception as e:\n",
       "                    end_time = time.time()\n",
       "                    total_latency = end_time - start_time\n",
       "                    \n",
       "                    result = {\n",
       "                        'model': model_id,\n",
       "                        'region': 'us-east-1',\n",
       "                        'inference_profile': 'standard',\n",
       "                        'document': document,\n",
       "                        'referenceResponse': reference_response,\n",
       "                        'model_response': f\"ERROR: {str(e)}\",\n",
       "                        'latency': total_latency,\n",
       "                        'model_latencyMs': total_latency * 1000,\n",
       "                        'model_input_tokens': 0,\n",
       "                        'model_output_tokens': 0,\n",
       "                        'status': 'error',\n",
       "                        'error_message': str(e),\n",
       "                        'timestamp': datetime.now().isoformat()\n",
       "                    }\n",
       "                    \n",
       "                    results.append(result)\n",
       "                    logger.error(f\"    ‚ùå Documento {doc_idx + 1}: Erro - {str(e)}\")\n",
       "                \n",
       "                # Delay entre chamadas\n",
       "                time.sleep(BENCHMARK_CONFIG['delay_between_calls'])\n",
       "    \n",
       "    logger.info(f\"üéâ Benchmark conclu√≠do para {model_id}!\")\n",
       "    logger.info(f\"üìä Total de resultados: {len(results)}\")\n",
       "    \n",
       "    return results\n",
       "\n",
       "print(\"‚úÖ Fun√ß√£o de benchmarking criada! Vamos come√ßar a medir!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Executando os Benchmarks**\n",
       "\n",
       "Agora vamos executar os benchmarks pra todos os nossos modelos candidatos. √â como fazer uma corrida cronometrada com todos os carros!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üèÅ EXECUTANDO OS BENCHMARKS\n",
       "print(\"ÔøΩÔøΩ INICIANDO BENCHMARKS DE LAT√äNCIA...\")\n",
       "print(\"=\" * 60)\n",
       "\n",
       "all_results = []\n",
       "dataset_path = '../data/document_sample_10.csv'  # Usando amostra pequena pro workshop\n",
       "\n",
       "for index, row in evaluation_tracking.iterrows():\n",
       "    model_id = row['model']\n",
       "    prompt = row['text_prompt']\n",
       "    \n",
       "    # Pular se n√£o tem prompt (modelo fonte)\n",
       "    if not prompt or model_id == 'source_model':\n",
       "        print(f\"‚è≠Ô∏è Pulando {model_id} (sem prompt ou modelo fonte)\")\n",
       "        continue\n",
       "    \n",
       "    print(f\"\\nüéØ BENCHMARK PARA: {model_id}\")\n",
       "    print(\"-\" * 40)\n",
       "    \n",
       "    # Configurando logging\n",
       "    logger, log_filename = setup_logging(model_id)\n",
       "    \n",
       "    try:\n",
       "        # Executando benchmark\n",
       "        results = benchmark_model_latency(model_id, prompt, dataset_path, logger)\n",
       "        all_results.extend(results)\n",
       "        \n",
       "        # Salvando resultados\n",
       "        results_df = pd.DataFrame(results)\n",
       "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
       "        output_filename = f'../outputs/document_summarization_{model_id.replace(\":\", \"-\")}_{timestamp}.csv'\n",
       "        results_df.to_csv(output_filename, index=False)\n",
       "        \n",
       "        print(f\"‚úÖ Resultados salvos em: {output_filename}\")\n",
       "        print(f\"ÔøΩÔøΩ Total de amostras: {len(results)}\")\n",
       "        \n",
       "        # Estat√≠sticas r√°pidas\n",
       "        successful_results = [r for r in results if r['status'] == 'success']\n",
       "        if successful_results:\n",
       "            latencies = [r['latency'] for r in successful_results]\n",
       "            print(f\"‚è±Ô∏è Lat√™ncia m√©dia: {np.mean(latencies):.3f}s\")\n",
       "            print(f\"‚è±Ô∏è Lat√™ncia mediana: {np.median(latencies):.3f}s\")\n",
       "            print(f\"‚è±Ô∏è Lat√™ncia P90: {np.percentile(latencies, 90):.3f}s\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro no benchmark de {model_id}: {str(e)}\")\n",
       "        logger.error(f\"Erro fatal: {str(e)}\")\n",
       "\n",
       "print(\"\\n\" + \"=\" * 60)\n",
       "print(\"üéâ TODOS OS BENCHMARKS CONCLU√çDOS!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Analisando os Resultados**\n",
       "\n",
       "Agora vamos dar uma olhada nos resultados que coletamos. √â como analisar os tempos de uma corrida pra ver quem foi mais r√°pido!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üìä ANALISANDO OS RESULTADOS\n",
       "print(\"ÔøΩÔøΩ AN√ÅLISE DOS RESULTADOS DE LAT√äNCIA\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "# Carregando todos os resultados salvos\n",
       "import glob\n",
       "import os\n",
       "\n",
       "output_directory = '../outputs'\n",
       "csv_files = glob.glob(os.path.join(output_directory, 'document_summarization_*.csv'))\n",
       "\n",
       "all_data = []\n",
       "for file in csv_files:\n",
       "    if 'source_model' not in file:  # Excluindo modelo fonte\n",
       "        df = pd.read_csv(file)\n",
       "        all_data.append(df)\n",
       "\n",
       "if all_data:\n",
       "    combined_df = pd.concat(all_data, ignore_index=True)\n",
       "    \n",
       "    print(f\"üìä Total de amostras coletadas: {len(combined_df)}\")\n",
       "    print(f\"üéØ Modelos testados: {combined_df['model'].unique()}\")\n",
       "    \n",
       "    # Estat√≠sticas por modelo\n",
       "    print(\"\\nüìà ESTAT√çSTICAS POR MODELO:\")\n",
       "    print(\"-\" * 40)\n",
       "    \n",
       "    for model in combined_df['model'].unique():\n",
       "        model_data = combined_df[combined_df['model'] == model]\n",
       "        successful_data = model_data[model_data['status'] == 'success']\n",
       "        \n",
       "        if len(successful_data) > 0:\n",
       "            latencies = successful_data['latency']\n",
       "            \n",
       "            print(f\"\\nüéØ {model}:\")\n",
       "            print(f\"  üìä Amostras: {len(successful_data)}\")\n",
       "            print(f\"  ‚è±Ô∏è M√©dia: {latencies.mean():.3f}s\")\n",
       "            print(f\"  ‚è±Ô∏è Mediana: {latencies.median():.3f}s\")\n",
       "            print(f\"  ‚è±Ô∏è P90: {latencies.quantile(0.9):.3f}s\")\n",
       "            print(f\"  ‚è±Ô∏è M√≠n: {latencies.min():.3f}s\")\n",
       "            print(f\"  ‚è±Ô∏è M√°x: {latencies.max():.3f}s\")\n",
       "            print(f\"  üìà Desvio Padr√£o: {latencies.std():.3f}s\")\n",
       "        else:\n",
       "            print(f\"\\n‚ùå {model}: Nenhum resultado bem-sucedido\")\n",
       "\n",
       "else:\n",
       "    print(\"‚ùå Nenhum resultado encontrado. Verifique se os benchmarks foram executados.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Visualizando os Resultados**\n",
       "\n",
       "Vamos criar algumas visualiza√ß√µes pra entender melhor os resultados. √â como transformar n√∫meros em gr√°ficos que contam uma hist√≥ria!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ CRIANDO VISUALIZA√á√ïES\n",
       "if 'combined_df' in locals() and len(combined_df) > 0:\n",
       "    # Filtrando apenas resultados bem-sucedidos\n",
       "    successful_df = combined_df[combined_df['status'] == 'success']\n",
       "    \n",
       "    if len(successful_df) > 0:\n",
       "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
       "        \n",
       "        # Gr√°fico 1: Boxplot de lat√™ncia por modelo\n",
       "        models = successful_df['model'].unique()\n",
       "        latency_data = [successful_df[successful_df['model'] == model]['latency'] for model in models]\n",
       "        \n",
       "        axes[0, 0].boxplot(latency_data, labels=models)\n",
       "        axes[0, 0].set_title('‚è±Ô∏è Distribui√ß√£o de Lat√™ncia por Modelo')\n",
       "        axes[0, 0].set_ylabel('Lat√™ncia (segundos)')\n",
       "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
       "        \n",
       "        # Gr√°fico 2: Histograma de lat√™ncia\n",
       "        for model in models:\n",
       "            model_data = successful_df[successful_df['model'] == model]['latency']\n",
       "            axes[0, 1].hist(model_data, alpha=0.7, label=model, bins=10)\n",
       "        \n",
       "        axes[0, 1].set_title('üìä Distribui√ß√£o de Lat√™ncia')\n",
       "        axes[0, 1].set_xlabel('Lat√™ncia (segundos)')\n",
       "        axes[0, 1].set_ylabel('Frequ√™ncia')\n",
       "        axes[0, 1].legend()\n",
       "        \n",
       "        # Gr√°fico 3: Lat√™ncia vs tokens de entrada\n",
       "        for model in models:\n",
       "            model_data = successful_df[successful_df['model'] == model]\n",
       "            axes[1, 0].scatter(model_data['model_input_tokens'], \n",
       "                              model_data['latency'], \n",
       "                              alpha=0.6, label=model)\n",
       "        \n",
       "        axes[1, 0].set_title('ÔøΩÔøΩ Lat√™ncia vs Tokens de Entrada')\n",
       "        axes[1, 0].set_xlabel('Tokens de Entrada')\n",
       "        axes[1, 0].set_ylabel('Lat√™ncia (segundos)')\n",
       "        axes[1, 0].legend()\n",
       "        \n",
       "        # Gr√°fico 4: Compara√ß√£o de m√©tricas\n",
       "        metrics_comparison = []\n",
       "        model_names = []\n",
       "        \n",
       "        for model in models:\n",
       "            model_data = successful_df[successful_df['model'] == model]['latency']\n",
       "            metrics_comparison.append([\n",
       "                model_data.mean(),\n",
       "                model_data.median(),\n",
       "                model_data.quantile(0.9)\n",
       "            ])\n",
       "            model_names.append(model.split('.')[-1])  # Nome mais curto\n",
       "        \n",
       "        metrics_comparison = np.array(metrics_comparison)\n",
       "        x = np.arange(len(model_names))\n",
       "        width = 0.25\n",
       "        \n",
       "        axes[1, 1].bar(x - width, metrics_comparison[:, 0], width, label='M√©dia', alpha=0.8)\n",
       "        axes[1, 1].bar(x, metrics_comparison[:, 1], width, label='Mediana', alpha=0.8)\n",
       "        axes[1, 1].bar(x + width, metrics_comparison[:, 2], width, label='P90', alpha=0.8)\n",
       "        \n",
       "        axes[1, 1].set_title('üìà Compara√ß√£o de M√©tricas de Lat√™ncia')\n",
       "        axes[1, 1].set_xlabel('Modelo')\n",
       "        axes[1, 1].set_ylabel('Lat√™ncia (segundos)')\n",
       "        axes[1, 1].set_xticks(x)\n",
       "        axes[1, 1].set_xticklabels(model_names)\n",
       "        axes[1, 1].legend()\n",
       "        \n",
       "        plt.tight_layout()\n",
       "        plt.show()\n",
       "        \n",
       "        print(\"\\nüí° O que esses gr√°ficos nos dizem?\")\n",
       "        print(\"‚Ä¢ O boxplot mostra a distribui√ß√£o de lat√™ncia de cada modelo\")\n",
       "        print(\"‚Ä¢ O histograma mostra como a lat√™ncia se distribui\")\n",
       "        print(\"‚Ä¢ O scatter plot mostra se h√° rela√ß√£o entre tamanho do input e lat√™ncia\")\n",
       "        print(\"‚Ä¢ O gr√°fico de barras compara as m√©tricas principais\")\n",
       "    \n",
       "else:\n",
       "    print(\"‚ùå N√£o h√° dados suficientes para criar visualiza√ß√µes.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Resumo do Passo 3**\n",
       "\n",
       " **Parab√©ns!** Voc√™ acabou de completar o terceiro passo da nossa jornada de migra√ß√£o. Vamos recapitular o que fizemos:\n",
       "\n",
       "‚úÖ **Entendemos a import√¢ncia da lat√™ncia**: Velocidade √© crucial em aplica√ß√µes reais\n",
       "‚úÖ **Configuramos benchmarking robusto**: Sistema completo de medi√ß√£o\n",
       "‚úÖ **Executamos testes de lat√™ncia**: Medimos a velocidade de todos os modelos\n",
       "‚úÖ **Coletamos m√©tricas detalhadas**: Lat√™ncia, tokens, status de cada chamada\n",
       "‚úÖ **Analisamos os resultados**: Estat√≠sticas e visualiza√ß√µes\n",
       "‚úÖ **Salvamos dados estruturados**: CSV com todos os resultados\n",
       "\n",
       "### **O Que Vem no Pr√≥ximo Passo**\n",
       "\n",
       "No pr√≥ximo notebook, vamos fazer algo super importante: **avaliar a qualidade**! √â como provar a comida depois de cronometrar o tempo de preparo. Vamos usar o LLM-as-a-Judge do Bedrock pra avaliar automaticamente a qualidade das respostas que geramos.\n",
       "\n",
       "---\n",
       "\n",
       "**üí° Dica do Pedro**: Lat√™ncia √© s√≥ uma parte da hist√≥ria! Um modelo pode ser r√°pido, mas se a qualidade for ruim, n√£o adianta nada. √â como ter um carro r√°pido que quebra toda hora!\n",
       "\n",
       "**üöÄ Pr√≥ximo passo**: Avalia√ß√£o de qualidade com LLM-as-a-Judge"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }