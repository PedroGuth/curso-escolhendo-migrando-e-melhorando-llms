{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **Passo 1: Preparando os Dados e Definindo o Que Queremos**\n",
       "\n",
       "## **Aula 1.1: Entendendo o Problema**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© migra√ß√£o de modelo?**\n",
       "\n",
       "Imagina que voc√™ tem um funcion√°rio super caro que faz um trabalho √≥timo, mas t√° pesando no bolso da empresa. A√≠ voc√™ descobre que tem outros candidatos que fazem o mesmo trabalho, √†s vezes at√© melhor, por um pre√ßo muito menor. √â isso que a gente vai fazer aqui - s√≥ que em vez de funcion√°rios, s√£o modelos de IA! üòÑ\n",
       "\n",
       "**Por que migra√ß√£o de modelo √© importante?**\n",
       "\n",
       "√â como trocar de plano de celular: √†s vezes voc√™ paga caro por algo que nem usa tanto, e tem planos melhores por menos dinheiro. Com IA √© a mesma coisa - modelos novos aparecem todo dia, e alguns s√£o mais baratos e r√°pidos que os antigos.\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um gr√°fico mostrando custos de diferentes modelos de IA\n",
       "\n",
       "### **Nosso Cen√°rio de Neg√≥cio**\n",
       "\n",
       "Vamos simular uma situa√ß√£o real:\n",
       "\n",
       "- **Problema**: Uma empresa usa um modelo propriet√°rio caro pra fazer resumos de documentos\n",
       "- **Dor**: O modelo √© bom, mas t√° custando uma fortuna pra rodar\n",
       "- **Solu√ß√£o**: Vamos testar modelos mais baratos do Amazon Bedrock\n",
       "\n",
       "### **Crit√©rios de Sucesso**\n",
       "\n",
       "√â como quando voc√™ vai comprar um carro - voc√™ tem uma lista do que quer:\n",
       "\n",
       "| Crit√©rio | O Que Queremos | Como Medir |\n",
       "|----------|----------------|------------|\n",
       "| **Qualidade** | Resumos t√£o bons quanto os atuais | IA avaliando os resumos |\n",
       "| **Velocidade** | Pelo menos t√£o r√°pido quanto | Cron√¥metro nas respostas |\n",
       "| **Custo** | Mais barato que o atual | Calculadora de pre√ßos |\n",
       "\n",
       "Vamos come√ßar explorando nossos dados e analisando como o modelo atual t√° performando!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üõ†Ô∏è INSTALANDO AS FERRAMENTAS\n",
       "!pip install --upgrade numexpr"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Preparando o Dataset**\n",
       "\n",
       "#### **Sobre o Dataset**\n",
       "\n",
       "Um dataset de qualidade √© fundamental pro sucesso da migra√ß√£o. √â como ter uma r√©gua boa pra medir - se a r√©gua t√° torta, todas as medidas v√£o dar errado!\n",
       "\n",
       "Vamos usar o dataset [EdinburghNLP/xsum](https://huggingface.co/datasets/EdinburghNLP/xsum), que √© tipo um \"padr√£o ouro\" pra resumos. Ele tem not√≠cias da BBC com resumos escritos por humanos - perfeito pra testar se nossos modelos conseguem fazer resumos t√£o bons quanto pessoas reais.\n",
       "\n",
       "Selecionamos 10 amostras representativas pra manter o workshop r√°pido e eficiente. Essas amostras representam v√°rios tamanhos e n√≠veis de complexidade pra dar uma avalia√ß√£o robusta.\n",
       "\n",
       "> **üí° Dica do Pedro**: Se voc√™ t√° rodando isso no seu ambiente, pode substituir por seu pr√≥prio dataset. S√≥ certifique que tem as colunas `document` e `referenceResponse`. Se quiser ver como preprocessamos o dataset, d√° uma olhada no arquivo `src/dataset.py`.\n",
       "\n",
       "Vamos dar uma olhada no que nossos dados parecem:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ DANDO UMA OLHADA NOS DADOS\n",
       "! head -5 ../data/document_sample_10.csv"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **An√°lise do Modelo Fonte**\n",
       "\n",
       "#### **Medindo a Performance Atual**\n",
       "\n",
       "> **üí° Nota**: Num cen√°rio real de migra√ß√£o, voc√™ coletaria essas m√©tricas do seu sistema de produ√ß√£o. Se voc√™ t√° construindo uma aplica√ß√£o nova, pode n√£o ter um modelo fonte pra comparar.\n",
       "\n",
       "Um passo cr√≠tico no processo de migra√ß√£o √© estabelecer uma linha de base confi√°vel pro seu modelo atual. √â como marcar onde voc√™ t√° antes de come√ßar uma corrida - sem isso, como voc√™ vai saber se melhorou?\n",
       "\n",
       "Pra esse workshop, j√° geramos respostas do nosso modelo fonte hipot√©tico usando o dataset.\n",
       "\n",
       "> **üí° Dica do Pedro**: Se quiser usar seu pr√≥prio modelo fonte (tipo GPT-4o-mini da OpenAI ou outro), pode gerar respostas seguindo o mesmo formato e substituir o arquivo CSV fornecido.\n",
       "\n",
       "Vamos examinar uma das respostas do modelo fonte pra entender suas caracter√≠sticas de performance:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ ANALISANDO O MODELO FONTE\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "# Configurando o visual\n",
       "plt.style.use('default')\n",
       "sns.set_palette(\"husl\")\n",
       "\n",
       "# Carregando os dados do modelo fonte\n",
       "source_model_data = pd.read_csv('../outputs/document_summarization_source_model.csv')\n",
       "\n",
       "print(\"üìä ESTAT√çSTICAS DO MODELO FONTE:\")\n",
       "print(f\"ÔøΩÔøΩ Total de amostras: {len(source_model_data)}\")\n",
       "print(f\"‚è±Ô∏è Lat√™ncia m√©dia: {source_model_data['latency'].mean():.3f} segundos\")\n",
       "print(f\"üí∞ Custo m√©dio: ${source_model_data['cost'].mean():.6f} por infer√™ncia\")\n",
       "print(f\"üî§ Tokens de entrada m√©dios: {source_model_data['model_input_tokens'].mean():.1f}\")\n",
       "print(f\"üî§ Tokens de sa√≠da m√©dios: {source_model_data['model_output_tokens'].mean():.1f}\")\n",
       "\n",
       "# Vamos dar uma olhada numa resposta espec√≠fica\n",
       "print(\"\\nÔøΩÔøΩ EXEMPLO DE RESPOSTA:\")\n",
       "sample_response = source_model_data.iloc[0]\n",
       "print(f\"Documento original: {sample_response['document'][:200]}...\")\n",
       "print(f\"Resumo gerado: {sample_response['model_response']}\")\n",
       "print(f\"Resumo de refer√™ncia: {sample_response['referenceResponse']}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Visualizando a Performance**\n",
       "\n",
       "Vamos criar alguns gr√°ficos pra entender melhor como o modelo fonte t√° performando:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ CRIANDO VISUALIZA√á√ïES\n",
       "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
       "\n",
       "# Gr√°fico 1: Distribui√ß√£o de lat√™ncia\n",
       "axes[0, 0].hist(source_model_data['latency'], bins=20, alpha=0.7, color='skyblue')\n",
       "axes[0, 0].set_title('‚è±Ô∏è Distribui√ß√£o de Lat√™ncia')\n",
       "axes[0, 0].set_xlabel('Lat√™ncia (segundos)')\n",
       "axes[0, 0].set_ylabel('Frequ√™ncia')\n",
       "\n",
       "# Gr√°fico 2: Distribui√ß√£o de custos\n",
       "axes[0, 1].hist(source_model_data['cost'], bins=20, alpha=0.7, color='lightgreen')\n",
       "axes[0, 1].set_title('üí∞ Distribui√ß√£o de Custos')\n",
       "axes[0, 1].set_xlabel('Custo por infer√™ncia ($)')\n",
       "axes[0, 1].set_ylabel('Frequ√™ncia')\n",
       "\n",
       "# Gr√°fico 3: Tokens de entrada vs sa√≠da\n",
       "axes[1, 0].scatter(source_model_data['model_input_tokens'], \n",
       "                   source_model_data['model_output_tokens'], \n",
       "                   alpha=0.6, color='orange')\n",
       "axes[1, 0].set_title('üî§ Tokens de Entrada vs Sa√≠da')\n",
       "axes[1, 0].set_xlabel('Tokens de Entrada')\n",
       "axes[1, 0].set_ylabel('Tokens de Sa√≠da')\n",
       "\n",
       "# Gr√°fico 4: Lat√™ncia vs custo\n",
       "axes[1, 1].scatter(source_model_data['latency'], \n",
       "                   source_model_data['cost'], \n",
       "                   alpha=0.6, color='purple')\n",
       "axes[1, 1].set_title('‚è±Ô∏è Lat√™ncia vs Custo')\n",
       "axes[1, 1].set_xlabel('Lat√™ncia (segundos)')\n",
       "axes[1, 1].set_ylabel('Custo ($)')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "print(\"\\nüí° O que esses gr√°ficos nos dizem?\")\n",
       "print(\"‚Ä¢ A lat√™ncia varia bastante - alguns documentos s√£o mais complexos que outros\")\n",
       "print(\"‚Ä¢ O custo t√° relacionado com a quantidade de tokens\")\n",
       "print(\"‚Ä¢ Documentos maiores geram resumos maiores (faz sentido!)\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Definindo Nossos Crit√©rios de Sucesso**\n",
       "\n",
       "Agora que entendemos como o modelo fonte t√° performando, vamos definir o que significa \"sucesso\" pra nossa migra√ß√£o. √â como definir as regras de um jogo - todo mundo precisa saber o que √© vit√≥ria!\n",
       "\n",
       "Vamos criar um dataframe de tracking que vai acompanhar todas as nossas avalia√ß√µes:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ CRIANDO NOSSO SISTEMA DE TRACKING\n",
       "import pandas as pd\n",
       "from datetime import datetime\n",
       "\n",
       "# Definindo os modelos que vamos testar\n",
       "models_to_evaluate = [\n",
       "    'source_model',  # Nosso baseline\n",
       "    'amazon.nova-lite-v1:0',  # Modelo da Amazon (barato e r√°pido)\n",
       "    'us.anthropic.claude-3-5-haiku-20241022-v1:0'  # Claude Haiku (bom custo-benef√≠cio)\n",
       "]\n",
       "\n",
       "# Criando o dataframe de tracking\n",
       "evaluation_tracking = pd.DataFrame({\n",
       "    'model': models_to_evaluate,\n",
       "    'region': 'us-east-1',  # Regi√£o da AWS\n",
       "    'inference_profile': 'standard',  # Perfil de infer√™ncia\n",
       "    'text_prompt': '',  # Vamos preencher isso no pr√≥ximo passo\n",
       "    'quality_evaluation_jobArn': '',  # Vamos preencher isso depois\n",
       "    'quality_evaluation_output': ''  # Vamos preencher isso depois\n",
       "})\n",
       "\n",
       "print(\"üìä NOSSO PLANO DE AVALIA√á√ÉO:\")\n",
       "print(evaluation_tracking)\n",
       "\n",
       "# Salvando o tracking\n",
       "evaluation_tracking.to_csv('../data/evaluation_tracking.csv', index=False)\n",
       "print(\"\\n‚úÖ Tracking salvo! Vamos usar isso pra acompanhar todo o processo.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Resumo do Passo 1**\n",
       "\n",
       "ÔøΩÔøΩ **Parab√©ns!** Voc√™ acabou de completar o primeiro passo da nossa jornada de migra√ß√£o. Vamos recapitular o que fizemos:\n",
       "\n",
       "‚úÖ **Entendemos o problema**: Modelo caro que precisa ser substitu√≠do\n",
       "‚úÖ **Analisamos os dados**: Dataset de qualidade pra testar\n",
       "‚úÖ **Estabelecemos baseline**: Performance atual do modelo fonte\n",
       "‚úÖ **Definimos crit√©rios**: O que significa sucesso\n",
       "‚úÖ **Criamos tracking**: Sistema pra acompanhar tudo\n",
       "\n",
       "### **O Que Vem no Pr√≥ximo Passo**\n",
       "\n",
       "No pr√≥ximo notebook, vamos fazer algo super legal: **otimizar prompts**! √â como ensinar cada modelo a falar da melhor forma poss√≠vel. Diferentes modelos respondem melhor a diferentes tipos de instru√ß√µes, ent√£o vamos usar o Amazon Bedrock Prompt Optimizer pra fazer essa m√°gica acontecer.\n",
       "\n",
       "---\n",
       "\n",
       "**ÔøΩÔøΩ Dica do Pedro**: Lembre-se, migra√ß√£o de modelo n√£o √© s√≥ trocar um pelo outro - √© um processo cuidadoso que precisa de dados, m√©tricas e muito teste!\n",
       "\n",
       "**üöÄ Pr√≥ximo passo**: Otimiza√ß√£o de prompts com Bedrock Prompt Optimizer"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }