{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f70b56-f12d-440d-a92f-330fa9e8b05b",
   "metadata": {},
   "source": [
    "# Step 2 - Optimize Prompt Using Bedrock Prompt Optimizer\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this step, we'll tackle a critical aspect of successful model migration: **prompt optimization**. Different models respond best to different prompt structures and instructions. Simply reusing your existing prompts with new models often leads to suboptimal performance.\n",
    "\n",
    "### Why Prompt Optimization Matters\n",
    "\n",
    "When migrating between models, especially across different model families (e.g., from OpenAI to Claude or Llama), the same prompt can produce dramatically different results.\n",
    "\n",
    "### Bedrock Prompt Optimizer\n",
    "\n",
    "For this workshop, we'll use [Amazon Bedrock Prompt Optimizer](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management-optimize.html), an automated tool that:\n",
    "\n",
    "1. Analyzes your source prompt structure and intent\n",
    "2. Reformats it to best leverage the target model's capabilities\n",
    "3. Optimizes instructions for clarity and effectiveness\n",
    "4. Preserves the core functionality while improving performance\n",
    "\n",
    "This saves hours of manual prompt engineering while producing more reliable results. Let's see it in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b639d",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "\n",
    "First, we'll import the necessary libraries for working with AWS services, data manipulation, and prompt optimization. These tools will enable us to interact with Bedrock services and maintain our evaluation tracking system from the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc74d65d-7e4a-4f91-b25d-50949bb0fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320fd764-27ed-4ef5-a300-0c7c47b6baaa",
   "metadata": {},
   "source": [
    "## Retrieving Our Progress\n",
    "\n",
    "Let's load our tracking dataframe from Step 1, which contains information about our source model and the candidate models we'll be evaluating. This dataframe will serve as our central repository for all evaluation metrics throughout the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa0302e8-5248-47fd-94b7-c701bbb366d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_clean_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>source_model</td>\n",
       "      <td>source_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon.nova-lite-v1:0</td>\n",
       "      <td>amazon.nova-lite-v1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1:0</td>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1-0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model  \\\n",
       "0                                 source_model   \n",
       "1                        amazon.nova-lite-v1:0   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1:0   \n",
       "\n",
       "                              model_clean_name  \n",
       "0                                 source_model  \n",
       "1                        amazon.nova-lite-v1-0  \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1-0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "### load our tracking df\n",
    "\n",
    "evaluation_tracking_file = '../data/evaluation_tracking.csv'\n",
    "evaluation_tracking = pd.read_csv(evaluation_tracking_file)\n",
    "display(evaluation_tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380174e3-14e2-49d7-93d3-b319bb05b3c1",
   "metadata": {},
   "source": [
    "## Understanding the Source Prompt\n",
    "\n",
    "### Analyzing Our Starting Point\n",
    "\n",
    "Before we can optimize prompts for our candidate models, we need to understand the prompt structure used by our source model. This prompt defines:\n",
    "\n",
    "1. How we present input context to the model\n",
    "2. What specific task we're asking the model to perform\n",
    "3. Any constraints or formatting requirements\n",
    "\n",
    "Let's first prepare our tracking dataframe to store prompts for each model, then examine the source model's prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "810a0c16-4489-4ed0-9426-64dc468f3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's prepare prompts for each model\n",
    "evaluation_tracking['text_prompt'] = ''\n",
    "evaluation_tracking['region'] = 'us-east-1' ## set it default to us-east-1\n",
    "evaluation_tracking['inference_profile'] = 'standard' ## standard or optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d23a74-9b83-4007-a923-0267b22d3611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_clean_name</th>\n",
       "      <th>text_prompt</th>\n",
       "      <th>region</th>\n",
       "      <th>inference_profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>source_model</td>\n",
       "      <td>source_model</td>\n",
       "      <td>\\nFirst, please read the article below.\\n{cont...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon.nova-lite-v1:0</td>\n",
       "      <td>amazon.nova-lite-v1-0</td>\n",
       "      <td></td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1:0</td>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1-0</td>\n",
       "      <td></td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model  \\\n",
       "0                                 source_model   \n",
       "1                        amazon.nova-lite-v1:0   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1:0   \n",
       "\n",
       "                              model_clean_name  \\\n",
       "0                                 source_model   \n",
       "1                        amazon.nova-lite-v1-0   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1-0   \n",
       "\n",
       "                                         text_prompt     region  \\\n",
       "0  \\nFirst, please read the article below.\\n{cont...  us-east-1   \n",
       "1                                                     us-east-1   \n",
       "2                                                     us-east-1   \n",
       "\n",
       "  inference_profile  \n",
       "0          standard  \n",
       "1          standard  \n",
       "2          standard  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## add the source_model prompt\n",
    "\n",
    "raw_prompt = \"\"\"\n",
    "First, please read the article below.\n",
    "{context}\n",
    " Now, can you write me an extremely short abstract for it?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "evaluation_tracking.loc[evaluation_tracking['model'] == 'source_model', 'text_prompt'] = raw_prompt\n",
    "display(evaluation_tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eafb52-da9d-419f-8b55-5f4467c075c2",
   "metadata": {},
   "source": [
    "## Automated Prompt Optimization\n",
    "\n",
    "Below, we'll define a function that calls the Bedrock Prompt Optimizer API to transform our source prompt for each candidate model. This function:\n",
    "\n",
    "- Takes a target model ID and our source prompt as inputs\n",
    "- Calls the Bedrock Prompt Optimizer API\n",
    "- Processes the response to extract the optimized prompt\n",
    "- Formats the result for use in our evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a79a1fea-d478-45aa-8a44-5409ec8b2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_prompt(target_model_id, prompt, region, inference_profile):\n",
    "    # Create input structure\n",
    "    input_structure = {\n",
    "        \"textPrompt\": {\n",
    "            \"text\": prompt\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = client.optimize_prompt(\n",
    "            input=input_structure,\n",
    "            targetModelId=target_model_id\n",
    "        )       \n",
    "        \n",
    "        # Print request ID for reference\n",
    "        print(\"Request ID:\", response.get(\"ResponseMetadata\").get(\"RequestId\"))        \n",
    "        # Print the original prompt\n",
    "        #print(\"========================== INPUT PROMPT ======================\\n\")\n",
    "        #print(prompt)\n",
    "        \n",
    "        # Process the response stream and extract the optimized prompt\n",
    "        optimized_prompt = None\n",
    "        try:\n",
    "            event_stream = response['optimizedPrompt']\n",
    "            for event in event_stream:\n",
    "                if 'optimizedPromptEvent' in event:\n",
    "                    print(f\"========================== OPTIMIZED PROMPT FOR MODEL {target_model_id} ======================\\n\")\n",
    "                    response = event['optimizedPromptEvent']['optimizedPrompt']['textPrompt']['text']\n",
    "\n",
    "                   # if response.startswith('\"') and text.endswith('\"'):\n",
    "                    optimized_prompt = json.loads(response)\n",
    "                    \n",
    "                    ## we will not use the prompt variables, because we need the full prompts for llm-as-a-judge evaluation in step3\n",
    "                    optimized_prompt = optimized_prompt.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "\n",
    "                    #print(optimized_prompt)\n",
    "                    # Return the optimized prompt                    \n",
    "                    return optimized_prompt\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing response stream: {e}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling optimize_prompt API: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06487a9-34ef-474a-b25a-aee1597380d9",
   "metadata": {},
   "source": [
    "### Applying Optimization to Our Candidate Models\n",
    "\n",
    "Now that we've defined our optimization function, let's apply it to each candidate model in our tracking dataframe. For each model:\n",
    "\n",
    "1. We'll extract the model ID (handling cross-region models appropriately)\n",
    "2. Send our source prompt to the Prompt Optimizer API\n",
    "3. Store the optimized prompt in our tracking dataframe\n",
    "\n",
    "This process will automatically tailor our summarization prompt for each model's specific capabilities and preferred instruction format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "806161f9-29d5-4134-ace8-958722012ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request ID: 35f59fb8-253f-4d9d-9a6a-bfc3f45c0678\n",
      "========================== OPTIMIZED PROMPT FOR MODEL amazon.nova-lite-v1:0 ======================\n",
      "\n",
      "Request ID: 4c1a402a-1403-4dc2-8ff0-38a762bb087e\n",
      "========================== OPTIMIZED PROMPT FOR MODEL anthropic.claude-3-5-haiku-20241022-v1:0 ======================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## let's get prompt optimized for our candidate models\n",
    "client = boto3.client('bedrock-agent-runtime')\n",
    "\n",
    "for index, row in evaluation_tracking.iterrows():\n",
    "    model_id = row['model']\n",
    "    if row['model'] == \"source_model\":\n",
    "        continue\n",
    "    if \"us.\" in row['model']:\n",
    "        model_id = row['model'].replace(\"us.\", \"\") ## us. prefix is for cross-region inference\n",
    "        #print(f\"modelID: {model_id}\")\n",
    "    optimized_prompt = optimize_prompt(model_id, raw_prompt, \"us-east-1\", \"standard\")\n",
    "    evaluation_tracking.loc[evaluation_tracking['model'] == row['model'], 'text_prompt'] = optimized_prompt\n",
    "    \n",
    "    \n",
    "#display(evaluation_tracking)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b622e024-6d87-428e-b3c8-dad29bbe7e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: source_model\n",
      "prompt: \n",
      "\n",
      "First, please read the article below.\n",
      "{context}\n",
      " Now, can you write me an extremely short abstract for it?\n",
      "\n",
      "\n",
      "================================================================================\n",
      "model: amazon.nova-lite-v1:0\n",
      "prompt: \n",
      "## Instruction\n",
      "Your task is to read the given article in the {context} section and provide an extremely short abstract or summary of the key points in 1-2 concise sentences.\n",
      "\n",
      "To produce a high-quality abstract, please follow these guidelines:\n",
      "\n",
      "### Summarization Guidelines\n",
      "- Read the article carefully to fully understand its content and main ideas.\n",
      "- Identify the core topics, arguments, and essential information presented.\n",
      "- Synthesize the key points into 1-2 brief sentences that capture the essence of the article.\n",
      "- Use clear and concise language to convey the main takeaways succinctly.\n",
      "- Avoid extraneous details and focus solely on the critical concepts.\n",
      "\n",
      "### Article to Summarize\n",
      "{context}\n",
      "\n",
      "Please provide your extremely short abstract immediately without any preamble:\n",
      "\n",
      "================================================================================\n",
      "model: us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "prompt: \n",
      "<task>\n",
      "Your task is to provide an extremely concise 1-2 sentence summary of the given text that captures the key points and main ideas.\n",
      "</task>\n",
      "\n",
      "<context>\n",
      "{context}\n",
      "</context>\n",
      "\n",
      "<instructions>\n",
      "Please read the provided <context>{context}</context> carefully and thoroughly to understand its content. Then, generate an \"abstract\" or high-level summary of the text in your own words that is extremely short - only 1-2 sentences long. The summary should concisely convey the core information and main takeaways from the original text in a clear and coherent way. Omit minor details and focus only on the essential points.\n",
      "\n",
      "Your summary should be provided as a parsable string output with no other preamble or formatting. For example:\n",
      "\n",
      "Summary: [your 1-2 sentence summary goes here]\n",
      "</instructions>\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# check optimized prompt\n",
    "\n",
    "for index, row in evaluation_tracking.iterrows():\n",
    "    print(f\"model: {row['model']}\")\n",
    "    print(f\"prompt: \\n{row['text_prompt']}\")\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede5474-e0ad-43bd-b63f-ed3a5da7ecc0",
   "metadata": {},
   "source": [
    "## Saving Our Progress\n",
    "\n",
    "With our prompts optimized for each model, let's save our updated tracking dataframe. This will ensure our optimized prompts are available for the next steps in our evaluation process.\n",
    "\n",
    "In the upcoming notebooks, we'll use these optimized prompts to:\n",
    "1. Generate responses from each candidate model\n",
    "2. Evaluate response quality\n",
    "3. Compare results across models to guide our migration decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153fb55b-3ea8-43be-9243-4e09f2d0c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the tracking\n",
    "\n",
    "evaluation_tracking.to_csv(evaluation_tracking_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32dee2d-9e83-4da5-8df2-1e5b75ccaecf",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. ✅ **Analyzed** our source model's prompt structure\n",
    "2. ✅ **Leveraged** Bedrock Prompt Optimizer to automatically adapt our prompt for each candidate model\n",
    "3. ✅ **Compared** the differences in prompt structure and instruction style between models\n",
    "4. ✅ **Prepared** optimized prompts for our evaluation process\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Different models respond best to different prompt structures\n",
    "- Automated tools like Bedrock Prompt Optimizer can significantly reduce the effort required for prompt migration\n",
    "- Effective prompts often include clear task definitions, structured instructions, and specific formatting guidance\n",
    "- Optimal prompts can vary dramatically between models, even for identical tasks\n",
    "\n",
    "### Important Considerations for Real-World Applications\n",
    "\n",
    "For this workshop, we've simplified the prompt optimization process to focus on the core evaluation workflow. In production scenarios, remember that:\n",
    "\n",
    "- **Prompt optimization is iterative**: The first optimized prompt is rarely the final one. In practice, you would:\n",
    "  - Test multiple prompt variations\n",
    "  - Analyze performance on different data samples\n",
    "  - Refine based on error patterns \n",
    "  \n",
    "- **Consider hybrid approaches**:\n",
    "  - Start with automated optimization \n",
    "  - Follow with human-in-the-loop refinement by domain experts\n",
    "  - Incorporate few-shot examples for complex tasks\n",
    "  - Add specific instructions to address error patterns\n",
    "\n",
    "- **Implement continuous optimization**:\n",
    "  - Establish a prompt version control system\n",
    "  - Regularly test prompt variations as models are updated\n",
    "  - Create an A/B testing framework for prompt comparison\n",
    "  - Document which prompts work best for which scenarios\n",
    "\n",
    "These practices ensure your prompts evolve alongside model capabilities and your business needs, maintaining optimal performance over time. We recommend reading this article on [Improve the performance of your Generative AI applications with Prompt Optimization on Amazon Bedrock](https://aws.amazon.com/blogs/machine-learning/improve-the-performance-of-your-generative-ai-applications-with-prompt-optimization-on-amazon-bedrock/) to learn more about Prompt Optimization techniques. \n",
    "\n",
    "In the next notebook, we'll use these optimized prompts to generate responses from our candidate models and measure their performance characteristics, particularly focusing on **latency metrics**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
