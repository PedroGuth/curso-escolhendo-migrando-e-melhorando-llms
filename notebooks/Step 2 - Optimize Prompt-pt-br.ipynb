{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **Passo 2: Otimizando Prompts com Bedrock Prompt Optimizer**\n",
       "\n",
       "## **Aula 2.1: Por Que Otimizar Prompts?**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© otimiza√ß√£o de prompt?**\n",
       "\n",
       "Imagina que voc√™ t√° tentando explicar algo pra um amigo, mas ele n√£o t√° entendendo. A√≠ voc√™ muda as palavras, d√° exemplos diferentes, e de repente ele entende perfeitamente! √â isso que a gente vai fazer aqui - s√≥ que em vez de amigos, s√£o modelos de IA. üòÑ\n",
       "\n",
       "**Por que otimiza√ß√£o de prompt √© importante?**\n",
       "\n",
       "Diferentes modelos de IA s√£o como pessoas com personalidades diferentes. O que funciona pra um pode n√£o funcionar pra outro. √â como tentar falar portugu√™s com algu√©m que s√≥ entende ingl√™s - voc√™ precisa adaptar sua linguagem!\n",
       "\n",
       "### **O Bedrock Prompt Optimizer**\n",
       "\n",
       "Pra esse workshop, vamos usar o [Amazon Bedrock Prompt Optimizer](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management-optimize.html), que √© tipo um \"tradutor autom√°tico\" pra prompts. Ele:\n",
       "\n",
       "1. **Analisa** a estrutura e inten√ß√£o do seu prompt original\n",
       "2. **Reformata** pra aproveitar melhor as capacidades do modelo alvo\n",
       "3. **Otimiza** as instru√ß√µes pra clareza e efic√°cia\n",
       "4. **Preserva** a funcionalidade principal enquanto melhora a performance\n",
       "\n",
       "Isso economiza horas de trabalho manual de engenharia de prompts e produz resultados mais confi√°veis. Vamos ver isso em a√ß√£o!\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um diagrama mostrando como prompts s√£o otimizados para diferentes modelos"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩÔ∏è PREPARANDO O AMBIENTE\n",
       "import boto3\n",
       "import random\n",
       "import json\n",
       "import sys\n",
       "import pandas as pd\n",
       "from IPython.display import display\n",
       "\n",
       "print(\"‚úÖ Ferramentas importadas! Vamos come√ßar a otimizar!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Recuperando Nosso Progresso**\n",
       "\n",
       "Vamos carregar nosso dataframe de tracking do Passo 1, que cont√©m informa√ß√µes sobre nosso modelo fonte e os modelos candidatos que vamos avaliar. Esse dataframe vai servir como nosso reposit√≥rio central pra todas as m√©tricas de avalia√ß√£o durante o workshop."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üìä CARREGANDO NOSSO TRACKING\n",
       "evaluation_tracking_file = '../data/evaluation_tracking.csv'\n",
       "evaluation_tracking = pd.read_csv(evaluation_tracking_file)\n",
       "display(evaluation_tracking)\n",
       "\n",
       "print(\"\\nüí° Perfeito! Agora temos nosso plano de avalia√ß√£o carregado.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Entendendo o Prompt Original**\n",
       "\n",
       "#### **Analisando Nosso Ponto de Partida**\n",
       "\n",
       "Antes de otimizar prompts pros nossos modelos candidatos, precisamos entender a estrutura do prompt usado pelo nosso modelo fonte. Esse prompt define:\n",
       "\n",
       "1. **Como apresentamos** o contexto de entrada pro modelo\n",
       "2. **Que tarefa espec√≠fica** estamos pedindo pro modelo fazer\n",
       "3. **Quaisquer restri√ß√µes** ou requisitos de formata√ß√£o\n",
       "\n",
       "Vamos primeiro preparar nosso dataframe de tracking pra armazenar prompts pra cada modelo, depois examinar o prompt do modelo fonte:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ PREPARANDO PROMPTS PRA CADA MODELO\n",
       "evaluation_tracking['text_prompt'] = ''\n",
       "evaluation_tracking['region'] = 'us-east-1'  # Definindo como us-east-1 por padr√£o\n",
       "evaluation_tracking['inference_profile'] = 'standard'  # Perfil padr√£o ou otimizado\n",
       "\n",
       "print(\"‚úÖ Colunas preparadas! Agora vamos adicionar o prompt do modelo fonte.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üìù ADICIONANDO O PROMPT DO MODELO FONTE\n",
       "raw_prompt = \"\"\"\n",
       "First, please read the article below.\n",
       "{context}\n",
       " Now, can you write me an extremely short abstract for it?\n",
       "\"\"\"\n",
       "\n",
       "evaluation_tracking.loc[evaluation_tracking['model'] == 'source_model', 'text_prompt'] = raw_prompt\n",
       "display(evaluation_tracking)\n",
       "\n",
       "print(\"\\nüí° Esse √© o prompt original que o modelo fonte usa. Simples e direto!\")\n",
       "print(\"\\nüîç Vamos analisar o que esse prompt faz:\")\n",
       "print(\"‚Ä¢ Pede pro modelo ler o artigo\")\n",
       "print(\"‚Ä¢ Solicita um resumo extremamente curto\")\n",
       "print(\"‚Ä¢ Usa uma linguagem informal e direta\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **O Poder do Bedrock Prompt Optimizer**\n",
       "\n",
       "Agora vamos usar a m√°gica do Bedrock Prompt Optimizer! √â como ter um especialista em comunica√ß√£o que sabe exatamente como falar com cada modelo de IA.\n",
       "\n",
       "#### **Como Funciona**\n",
       "\n",
       "O Prompt Optimizer √© tipo um \"tradutor inteligente\" que:\n",
       "\n",
       "1. **Entende** o que seu prompt original quer fazer\n",
       "2. **Adapta** a linguagem pro modelo espec√≠fico\n",
       "3. **Mant√©m** a inten√ß√£o original\n",
       "4. **Melhora** a clareza e efic√°cia\n",
       "\n",
       "Vamos ver isso em a√ß√£o com nossos modelos candidatos:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ CONFIGURANDO O BEDROCK CLIENT\n",
       "bedrock_client = boto3.client('bedrock')\n",
       "\n",
       "print(\"‚úÖ Cliente Bedrock configurado! Vamos come√ßar a otimizar.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ OTIMIZANDO PROMPTS PRA CADA MODELO\n",
       "def optimize_prompt_for_model(source_prompt, target_model):\n",
       "    \"\"\"\n",
       "    Otimiza um prompt pra um modelo espec√≠fico usando o Bedrock Prompt Optimizer.\n",
       "    √â como ter um tradutor especializado pra cada modelo!\n",
       "    \"\"\"\n",
       "    \n",
       "    try:\n",
       "        # Configurando a requisi√ß√£o de otimiza√ß√£o\n",
       "        optimization_request = {\n",
       "            'prompt': source_prompt,\n",
       "            'targetModel': target_model,\n",
       "            'optimizationType': 'EFFICIENCY',  # Otimizando pra efici√™ncia\n",
       "            'constraints': {\n",
       "                'maxTokens': 1000,  # Limite de tokens\n",
       "                'preserveIntent': True  # Mant√©m a inten√ß√£o original\n",
       "            }\n",
       "        }\n",
       "        \n",
       "        # Chamando o Prompt Optimizer\n",
       "        response = bedrock_client.optimize_prompt(**optimization_request)\n",
       "        \n",
       "        return response['optimizedPrompt']\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ö†Ô∏è Erro ao otimizar prompt para {target_model}: {str(e)}\")\n",
       "        # Se der erro, vamos usar uma vers√£o adaptada manualmente\n",
       "        return adapt_prompt_manually(source_prompt, target_model)\n",
       "\n",
       "def adapt_prompt_manually(source_prompt, target_model):\n",
       "    \"\"\"\n",
       "    Adapta o prompt manualmente se o otimizador autom√°tico falhar.\n",
       "    √â como ter um plano B na manga!\n",
       "    \"\"\"\n",
       "    \n",
       "    if 'nova' in target_model:\n",
       "        # Nova prefere instru√ß√µes mais estruturadas\n",
       "        return f\"\"\"\n",
       "        TAREFA: Criar um resumo extremamente conciso\n",
       "        \n",
       "        ARTIGO:\n",
       "        {{context}}\n",
       "        \n",
       "        INSTRU√á√ïES:\n",
       "        - Leia o artigo acima\n",
       "        - Crie um resumo muito curto e direto\n",
       "        - Mantenha apenas as informa√ß√µes essenciais\n",
       "        \"\"\"\n",
       "    elif 'claude' in target_model:\n",
       "        # Claude prefere linguagem mais natural\n",
       "        return f\"\"\"\n",
       "        Por favor, leia o seguinte artigo:\n",
       "        \n",
       "        {{context}}\n",
       "        \n",
       "        Agora, escreva um resumo extremamente breve e conciso deste artigo.\n",
       "        \"\"\"\n",
       "    else:\n",
       "        # Fallback gen√©rico\n",
       "        return source_prompt\n",
       "\n",
       "print(\"‚úÖ Fun√ß√µes de otimiza√ß√£o criadas! Vamos testar.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üéØ APLICANDO OTIMIZA√á√ÉO PRA CADA MODELO\n",
       "print(\"ÔøΩÔøΩ OTIMIZANDO PROMPTS...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "for index, row in evaluation_tracking.iterrows():\n",
       "    model = row['model']\n",
       "    \n",
       "    if model != 'source_model':  # S√≥ otimizar pros candidatos\n",
       "        print(f\"\\nÔøΩÔøΩ Otimizando para: {model}\")\n",
       "        \n",
       "        # Pegando o prompt original\n",
       "        original_prompt = evaluation_tracking.loc[evaluation_tracking['model'] == 'source_model', 'text_prompt'].iloc[0]\n",
       "        \n",
       "        # Otimizando o prompt\n",
       "        optimized_prompt = optimize_prompt_for_model(original_prompt, model)\n",
       "        \n",
       "        # Salvando o prompt otimizado\n",
       "        evaluation_tracking.loc[index, 'text_prompt'] = optimized_prompt\n",
       "        \n",
       "        print(f\"‚úÖ Prompt otimizado salvo!\")\n",
       "        print(f\"üìù Tamanho original: {len(original_prompt)} caracteres\")\n",
       "        print(f\"ÔøΩÔøΩ Tamanho otimizado: {len(optimized_prompt)} caracteres\")\n",
       "        \n",
       "        # Mostrando uma pr√©via do prompt otimizado\n",
       "        print(f\"\\nüîç Pr√©via do prompt otimizado:\")\n",
       "        print(optimized_prompt[:200] + \"...\" if len(optimized_prompt) > 200 else optimized_prompt)\n",
       "\n",
       "print(\"\\n\" + \"=\" * 50)\n",
       "print(\"ÔøΩÔøΩ OTIMIZA√á√ÉO CONCLU√çDA!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Comparando os Prompts**\n",
       "\n",
       "Agora vamos dar uma olhada em como os prompts ficaram diferentes pra cada modelo. √â como ver como a mesma hist√≥ria √© contada de formas diferentes pra p√∫blicos diferentes!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üìä COMPARANDO OS PROMPTS OTIMIZADOS\n",
       "print(\"ÔøΩÔøΩ COMPARA√á√ÉO DOS PROMPTS:\")\n",
       "print(\"=\" * 60)\n",
       "\n",
       "for index, row in evaluation_tracking.iterrows():\n",
       "    model = row['model']\n",
       "    prompt = row['text_prompt']\n",
       "    \n",
       "    print(f\"\\nüéØ MODELO: {model}\")\n",
       "    print(f\"üìè Tamanho: {len(prompt)} caracteres\")\n",
       "    print(f\"üî§ Palavras: {len(prompt.split())} palavras\")\n",
       "    print(f\"ÔøΩÔøΩ Prompt:\")\n",
       "    print(\"-\" * 40)\n",
       "    print(prompt)\n",
       "    print(\"-\" * 40)\n",
       "\n",
       "print(\"\\nÔøΩÔøΩ Observe as diferen√ßas:\")\n",
       "print(\"‚Ä¢ Cada modelo tem um estilo diferente de instru√ß√£o\")\n",
       "print(\"‚Ä¢ Alguns s√£o mais estruturados, outros mais naturais\")\n",
       "print(\"‚Ä¢ O tamanho varia conforme a complexidade necess√°ria\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Salvando Nossos Progressos**\n",
       "\n",
       "Agora vamos salvar nosso dataframe atualizado com todos os prompts otimizados. Isso vai ser crucial pros pr√≥ximos passos!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üíæ SALVANDO O TRACKING ATUALIZADO\n",
       "evaluation_tracking.to_csv('../data/evaluation_tracking.csv', index=False)\n",
       "\n",
       "print(\"‚úÖ Tracking atualizado salvo!\")\n",
       "print(\"\\nüìä RESUMO DO QUE FIZEMOS:\")\n",
       "print(f\"‚Ä¢ Modelos avaliados: {len(evaluation_tracking)}\")\n",
       "print(f\"‚Ä¢ Prompts otimizados: {len(evaluation_tracking[evaluation_tracking['model'] != 'source_model'])}\")\n",
       "print(f\"‚Ä¢ Arquivo salvo: ../data/evaluation_tracking.csv\")\n",
       "\n",
       "# Mostrando o status final\n",
       "display(evaluation_tracking[['model', 'text_prompt']])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Testando os Prompts Otimizados**\n",
       "\n",
       "Vamos fazer um teste r√°pido pra ver se nossos prompts otimizados est√£o funcionando. √â como fazer um \"teste de som\" antes do show principal!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üß™ TESTE R√ÅPIDO DOS PROMPTS\n",
       "def test_prompt_with_sample(prompt, model_id):\n",
       "    \"\"\"\n",
       "    Testa um prompt com uma amostra pequena pra ver se funciona.\n",
       "    √â como fazer um teste de dire√ß√£o antes de comprar o carro!\n",
       "    \"\"\"\n",
       "    \n",
       "    try:\n",
       "        # Carregando uma amostra pequena\n",
       "        sample_data = pd.read_csv('../data/document_sample_2.csv')\n",
       "        test_document = sample_data.iloc[0]['document']\n",
       "        \n",
       "        # Formatando o prompt com o documento\n",
       "        formatted_prompt = prompt.format(context=test_document)\n",
       "        \n",
       "        print(f\"\\nÔøΩÔøΩ TESTE PARA: {model_id}\")\n",
       "        print(f\"ÔøΩÔøΩ Prompt formatado (primeiros 200 chars):\")\n",
       "        print(formatted_prompt[:200] + \"...\")\n",
       "        print(f\"‚úÖ Prompt formatado com sucesso!\")\n",
       "        \n",
       "        return True\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro no teste: {str(e)}\")\n",
       "        return False\n",
       "\n",
       "# Testando cada prompt\n",
       "print(\"üß™ INICIANDO TESTES DOS PROMPTS...\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "for index, row in evaluation_tracking.iterrows():\n",
       "    model = row['model']\n",
       "    prompt = row['text_prompt']\n",
       "    \n",
       "    if prompt:  # S√≥ testar se tem prompt\n",
       "        success = test_prompt_with_sample(prompt, model)\n",
       "        if success:\n",
       "            print(f\"‚úÖ {model}: OK!\")\n",
       "        else:\n",
       "            print(f\"‚ùå {model}: Precisa ajuste!\")\n",
       "\n",
       "print(\"\\nüéâ TESTES CONCLU√çDOS!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Resumo do Passo 2**\n",
       "\n",
       "ÔøΩÔøΩ **Parab√©ns!** Voc√™ acabou de completar o segundo passo da nossa jornada de migra√ß√£o. Vamos recapitular o que fizemos:\n",
       "\n",
       "‚úÖ **Entendemos a import√¢ncia**: Diferentes modelos precisam de prompts diferentes\n",
       "‚úÖ **Usamos o Bedrock Prompt Optimizer**: Ferramenta autom√°tica pra otimiza√ß√£o\n",
       "‚úÖ **Otimizamos prompts**: Adaptamos pra cada modelo candidato\n",
       "‚úÖ **Comparamos resultados**: Vimos as diferen√ßas entre os prompts\n",
       "‚úÖ **Testamos funcionamento**: Verificamos se tudo t√° funcionando\n",
       "‚úÖ **Salvamos progresso**: Tracking atualizado com prompts otimizados\n",
       "\n",
       "### **O Que Vem no Pr√≥ximo Passo**\n",
       "\n",
       "No pr√≥ximo notebook, vamos fazer algo super importante: **medir a lat√™ncia**! √â como cronometrar quanto tempo cada modelo leva pra responder. Vamos gerar respostas usando os prompts otimizados que acabamos de criar e coletar m√©tricas detalhadas de performance.\n",
       "\n",
       "---\n",
       "\n",
       "**üí° Dica do Pedro**: Otimiza√ß√£o de prompt √© uma arte! √Äs vezes pequenas mudan√ßas fazem uma diferen√ßa enorme na qualidade das respostas.\n",
       "\n",
       "**üöÄ Pr√≥ximo passo**: Avalia√ß√£o de lat√™ncia dos modelos"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }