{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Evaluate Model Latency\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Latency is one of the three critical dimensions in model migration evaluation, alongside quality and cost. Measuring performance metrics accurately is essential for making informed migration decisions.\n",
    "\n",
    "In this notebook, we'll implement a robust framework to measure and compare latency across our candidate models. We'll generate responses using the optimized prompts we created in the previous step and collect detailed performance metrics.\n",
    "\n",
    "## Common Latency Metrics to measure\n",
    "\n",
    "* Time-To-First-Token (TTFT): Measures how quickly the model produces its first response token\n",
    "* Token Throughput Per Second (TTPS): Indicates how many tokens the model generates per second after the first token\n",
    "* Overall Latency: Measures total time from request submission to complete response delivery\n",
    "\n",
    "| Application Type | Primary API | Key Metrics | Examples |\n",
    "|-----------------|-------------|-------------|----------|\n",
    "| Streaming | [ConverseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html), [InvokeModelWithResponseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html) | TTFT, TTPS | Chatbots, virtual assistants, live content creation |\n",
    "| Non-streaming | [Converse](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html), [InvokeModel](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) | Overall Latency | Document summarization, batch processing, report generation |\n",
    "\n",
    "In this workshop, since we have the Documentation Summarization use case, we will measure the overall latency only.\n",
    "## What This Evaluation Produces\n",
    "\n",
    "### 1. Detailed Log Files\n",
    "\n",
    "A log file is automatically generated in your working directory as `model_latency_benchmarking-{timestamp}.log`, tracking all API calls, errors, and execution details to provide a complete audit trail and debugging information for your benchmarking process.\n",
    "\n",
    "### 2. Results CSV Files\n",
    "\n",
    "Results are saved in the `../outputs/` directory as `document_summarization_{model_id}_{timestamp}.csv`, containing key metrics including total latency, server-side processing time, token usage counts, API status indicators, and configuration details for comprehensive performance analysis.\n",
    "\n",
    "These results will be used in Step 5 to create comprehensive model comparisons and visualizations.\n",
    "\n",
    "## Benchmarking Guidelines\n",
    "\n",
    "For statistically valid latency evaluation, consider these principles:\n",
    "\n",
    "| Parameter | Description | Workshop Setting | Production Recommendation |\n",
    "|-----------|-------------|-----------------|---------------------------|\n",
    "| `invocations_per_scenario` | Repetitions per prompt | 1 (for workshop efficiency) | 10+ for statistical significance |\n",
    "| `experiment_counts` | Times to repeat the whole experiment | 1 (for workshop) | Multiple runs across different days/times |\n",
    "| `num_parallel_calls` | Concurrent API requests | 1 (to avoid throttling) | Match your production concurrency |\n",
    "\n",
    "> **⚠️ Statistical Note:** While we're using simplified parameters for this workshop, production evaluations should follow more rigorous statistical practices. The Central Limit Theorem tells us that with sufficient samples (1000+), our metrics will approximate a normal distribution. Ideally, you should:\n",
    "> - Collect samples across multiple days to account for time-of-day variations\n",
    "> - Include peak traffic periods in your sampling\n",
    "> - Match your test distribution to your actual production traffic patterns\n",
    "\n",
    "Let's begin our latency evaluation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "import random\n",
    "import pprint\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import os\n",
    "import logging\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_clean_name</th>\n",
       "      <th>text_prompt</th>\n",
       "      <th>region</th>\n",
       "      <th>inference_profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>source_model</td>\n",
       "      <td>source_model</td>\n",
       "      <td>\\nFirst, please read the article below.\\n{cont...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon.nova-lite-v1:0</td>\n",
       "      <td>amazon.nova-lite-v1-0</td>\n",
       "      <td>## Instruction\\nYour task is to read the given...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1:0</td>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1-0</td>\n",
       "      <td>&lt;task&gt;\\nYour task is to provide an extremely c...</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model  \\\n",
       "0                                 source_model   \n",
       "1                        amazon.nova-lite-v1:0   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1:0   \n",
       "\n",
       "                              model_clean_name  \\\n",
       "0                                 source_model   \n",
       "1                        amazon.nova-lite-v1-0   \n",
       "2  us.anthropic.claude-3-5-haiku-20241022-v1-0   \n",
       "\n",
       "                                         text_prompt     region  \\\n",
       "0  \\nFirst, please read the article below.\\n{cont...  us-east-1   \n",
       "1  ## Instruction\\nYour task is to read the given...  us-east-1   \n",
       "2  <task>\\nYour task is to provide an extremely c...  us-east-1   \n",
       "\n",
       "  inference_profile  \n",
       "0          standard  \n",
       "1          standard  \n",
       "2          standard  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "### load our tracking df\n",
    "\n",
    "evaluation_tracking_file = '../data/evaluation_tracking.csv'\n",
    "evaluation_tracking = pd.read_csv(evaluation_tracking_file)\n",
    "display(evaluation_tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "### Defining Our Benchmarking Parameters\n",
    "\n",
    "The key parameters we'll configure include:\n",
    "\n",
    "- **Data Source**: Location of our test documents\n",
    "- **Experiment Repetitions**: How many times to run the complete test suite\n",
    "- **Invocation Settings**: Controls for API call frequency and parallelism\n",
    "- **Model Parameters**: Configuration for temperature, token limits, etc.\n",
    "\n",
    "These settings directly impact the quality and reliability of our latency measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the prompt dataset and directory to save the results \n",
    "documents_path = \"../data/document_sample_10.csv\"\n",
    "directory = \"../outputs\" ## output to this directory\n",
    "\n",
    "dt = datetime.fromtimestamp(time.time(), tz=pytz.utc)\n",
    "job_timestamp_iso = dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Configuration to repeat experiment for reliable metrics\n",
    "scenario_config = {\n",
    "    \"sleep_between_invocations\": 5, # in seconds\n",
    "    \"invocations_per_scenario\": 1 # number of times you want to run the same prompt to get more samples - note: this means more cost \n",
    "}\n",
    "\n",
    "# Set the number of parallel calls\n",
    "num_parallel_calls = 1 ## STRONGLY RECOMMEND TO SET TO 1, TO AVOID THROTTLING IN WORKSHOP\n",
    "\n",
    "# how many times do you want to run the experiment (increase this for longer experiments, helps with more reliable numbers)\n",
    "experiment_counts = 1\n",
    "\n",
    "# Other inference parameters\n",
    "MAX_TOKENS = 1024\n",
    "TEMPERATURE = 0\n",
    "TOP_P = 1\n",
    "EXPERIMENT_NAME = f'experiment_{job_timestamp_iso}' # your custom experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We need to lock logging becasue we will use threads to evaluate the concurrency\n",
    "\n",
    "logging_lock = Lock()\n",
    "os.makedirs(f\"{directory}\", exist_ok=True)\n",
    "os.makedirs(f\"{directory}-analysis\", exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=f\"model_latency_benchmarking-{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\", \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework Implementation\n",
    "\n",
    "### Core Benchmarking Functions\n",
    "\n",
    "This section contains the heart of our latency evaluation system - the functions that handle the actual API calls, timing measurements, and result processing. \n",
    "\n",
    "> **Note for Workshop Participants**: This is the most complex part of the notebook. You don't need to fully understand every line of code, but it's helpful to grasp the overall approach to latency measurement. The key concept is that we're making controlled API calls and precisely measuring the time taken for each response.\n",
    "\n",
    "> **For Advanced Users**: This implementation includes thread-safe logging and concurrent execution capabilities that mirror production-grade benchmarking systems. These features ensure accurate measurements even under parallel testing conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_iteration(scenario_config):\n",
    "    logging.info(f'Sleeping for {scenario_config[\"sleep_between_invocations\"]} seconds.')\n",
    "    time.sleep(scenario_config[\"sleep_between_invocations\"])\n",
    "\n",
    "def benchmark(client, prompt, latency_inference_profile, model_id, inferenceConfig, system_config, sleep_on_throttling=5):\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "    api_call_status = 'Success'\n",
    "    full_error_message = 'Success'\n",
    "    dt = datetime.fromtimestamp(time.time(), tz=pytz.utc)\n",
    "    job_timestamp_iso = dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "      \n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    output_tokens, input_tokens = None, None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            start = time.time()\n",
    "           # print(f\"=============step 2222 in benchmark==================\")\n",
    "            response = client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=messages,\n",
    "                inferenceConfig=inferenceConfig,\n",
    "                system=system_config\n",
    "            )\n",
    "            end = time.time()\n",
    "            latency = round(end - start, 2)\n",
    "    \n",
    "            # Process and print the response\n",
    "            result = response.get(\"output\")\n",
    "            input_tokens = response[\"usage\"][\"inputTokens\"]\n",
    "            output_tokens = response[\"usage\"][\"outputTokens\"]\n",
    "            model_latencyMs = response[\"metrics\"][\"latencyMs\"]\n",
    "            output_list = result[\"message\"].get(\"content\", [])\n",
    "            model_response = \"\\n\".join([x[\"text\"] for x in output_list])\n",
    "            #print(f\"=============model response: ==================\\n{model_response}\")\n",
    "\n",
    "\n",
    "        except ClientError as err:\n",
    "            full_error_message = err\n",
    "            api_call_status = err.response['Error']['Code']\n",
    "            print(f\"Got Error: {api_call_status}\")\n",
    "            print(f\"Full Error Message: {full_error_message}\")\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    return model_response, latency, model_latencyMs, job_timestamp_iso, api_call_status, full_error_message, output_tokens, input_tokens\n",
    "\n",
    "def execute_benchmark(client, scenarios, scenario_config, inferenceConfig, system_config, num_parallel_calls=4, early_break=False):\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "    all_invocations = []\n",
    "    \n",
    "    # Create a master progress bar for overall scenarios\n",
    "    master_pbar = tqdm(total=len(scenarios), desc=\"Scenarios\", position=0, leave=True)\n",
    "    \n",
    "    def process_scenario(scenario_idx, scenario):\n",
    "        local_invocations = []\n",
    "        prompt = scenario['prompt']\n",
    "        #print(f\"=============111111==================\\n{prompt}\")\n",
    "            \n",
    "        for invocation_id in range(scenario_config[\"invocations_per_scenario\"]):\n",
    "            try:\n",
    "                model_response, latency, model_latencyMs, job_timestamp_iso, api_call_status, \\\n",
    "                full_error_message, model_output_tokens, model_input_tokens = benchmark(\n",
    "                    client,\n",
    "                    prompt,\n",
    "                    latency_inference_profile=scenario['latency_inference_profile'],\n",
    "                    model_id=scenario['model_id'],\n",
    "                    inferenceConfig = inferenceConfig,\n",
    "                    system_config = system_config,\n",
    "                    sleep_on_throttling=scenario_config['sleep_between_invocations']\n",
    "                )\n",
    "\n",
    "                invocation = {\n",
    "                    'model': scenario['model_id'],\n",
    "                    'model_clean_name': scenario['model_clean_name'],\n",
    "                    'region': scenario['region'],\n",
    "                    'invocation_id': invocation_id,\n",
    "                    'prompt':prompt,\n",
    "                    'model_response':model_response,\n",
    "                    'referenceResponse': scenario['referenceResponse'],\n",
    "                    'latency':latency,\n",
    "                    'model_latencyMs':model_latencyMs,\n",
    "                    'job_timestamp_iso': job_timestamp_iso,\n",
    "                    'model_input_tokens': model_input_tokens,\n",
    "                    'model_output_tokens': model_output_tokens,\n",
    "                    'api_call_status': api_call_status,\n",
    "                    'full_error_message': full_error_message,\n",
    "                    'TEMPERATURE': TEMPERATURE,\n",
    "                    'TOP_P': TOP_P,\n",
    "                    'EXPERIMENT_NAME': EXPERIMENT_NAME,\n",
    "                    #'task_type': scenario['task_type'],\n",
    "                    'inference_profile': scenario['latency_inference_profile'],\n",
    "                }\n",
    "                local_invocations.append(invocation)\n",
    "                \n",
    "                # Thread-safe logging\n",
    "                with logging_lock:\n",
    "                    logging.info(f'Invocation: {invocation}')\n",
    "                \n",
    "                post_iteration(scenario_config=scenario_config)\n",
    "                \n",
    "            except Exception as e:\n",
    "                with logging_lock:\n",
    "                    logging.error(f\"Error while processing scenario: {scenario['model_id']}. Error: {e}\")\n",
    "                \n",
    "        # Update master progress bar when a scenario is complete\n",
    "        master_pbar.update(1)\n",
    "        return local_invocations\n",
    "\n",
    "    # Execute scenarios in parallel\n",
    "    with ThreadPoolExecutor(max_workers=num_parallel_calls) as executor:\n",
    "        # Submit all scenarios and store futures\n",
    "        future_to_scenario = {executor.submit(process_scenario, idx, scenario): scenario \n",
    "                            for idx, scenario in enumerate(scenarios)}\n",
    "        \n",
    "        # Print initial state\n",
    "        # print(f\"Total scenarios submitted: {len(future_to_scenario)}\")\n",
    "        # print(f\"Number of parallel workers: {num_parallel_calls}\")\n",
    "        \n",
    "        # Monitor futures as they complete\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_scenario):\n",
    "            scenario = future_to_scenario[future]\n",
    "            current_time = time.time() - start_time\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_invocations.extend(result)\n",
    "            except Exception as e:\n",
    "                with logging_lock:\n",
    "                    logging.error(f\"Scenario failed: {e}\")\n",
    "\n",
    "        master_pbar.close()\n",
    "        return all_invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data\n",
    "\n",
    "### Preparing Our Document Samples\n",
    "\n",
    "To evaluate latency consistently across models, we need a standardized set of input documents. We'll use the dataset from our previous steps - a collection of news articles from the XSum dataset.\n",
    "\n",
    "Each document will be processed by each of our candidate models using their respective optimized prompts. \n",
    "\n",
    "The dataset includes both the source documents and reference summaries, though for latency evaluation we're primarily focused on processing the source documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>referenceResponse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rita King, 81, who had dementia, died after be...</td>\n",
       "      <td>An 87-year-old man who shot his wife dead at a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scottish winger Matt Williams' early touchdown...</td>\n",
       "      <td>Worcester Warriors booked their place in the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joe Lawton, 17, shot himself at his family far...</td>\n",
       "      <td>The parents of a boy who killed himself after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here's BBC Sport's day-by-day guide so you kno...</td>\n",
       "      <td>The Paralympic Games are about to reach their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It said such a move would \"reduce the burden\" ...</td>\n",
       "      <td>British Airways should automatically compensat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  Rita King, 81, who had dementia, died after be...   \n",
       "1  Scottish winger Matt Williams' early touchdown...   \n",
       "2  Joe Lawton, 17, shot himself at his family far...   \n",
       "3  Here's BBC Sport's day-by-day guide so you kno...   \n",
       "4  It said such a move would \"reduce the burden\" ...   \n",
       "\n",
       "                                   referenceResponse  \n",
       "0  An 87-year-old man who shot his wife dead at a...  \n",
       "1  Worcester Warriors booked their place in the C...  \n",
       "2  The parents of a boy who killed himself after ...  \n",
       "3  The Paralympic Games are about to reach their ...  \n",
       "4  British Airways should automatically compensat...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.read_csv(documents_path)\n",
    "documents.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Latency Evaluations\n",
    "### Executing Model Benchmarks\n",
    "Now we're ready to run our latency evaluations against each candidate model. This process will:\n",
    "\n",
    "1. Skip the source model (we already have its metrics from existing data)\n",
    "2. Process each candidate model sequentially\n",
    "3. Generate responses for each document in our dataset\n",
    "4. Capture detailed performance metrics for every API call\n",
    "5. Save comprehensive results to CSV files for later analysis\n",
    "\n",
    "\n",
    "> This is a time-consuming part of the notebook. Depending on your settings and the number of documents, it may take several minutes to complete as we make multiple API calls with appropriate spacing between them to avoid throttling.\n",
    "\n",
    "Note: In the code below, we're using progress bars to help visualize the benchmarking process. This gives you real-time feedback on how far along each model's evaluation has progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== GENERATING RESPONSE WITH MODEL amazon.nova-lite-v1:0 ======================\n",
      "\n",
      "Creating scenarios for model amazon.nova-lite-v1:0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa03b11e21564d519de841e9c9ded29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building scenarios:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments for model amazon.nova-lite-v1:0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cde444572d4246a685c96cd9c59ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment runs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02eceaed3fa04072b23b3e319b2f7732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scenarios:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "========================== GENERATING RESPONSE WITH MODEL us.anthropic.claude-3-5-haiku-20241022-v1:0 ======================\n",
      "\n",
      "Creating scenarios for model us.anthropic.claude-3-5-haiku-20241022-v1:0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f4e4789d1643cd8df0d4937ea9a6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building scenarios:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments for model us.anthropic.claude-3-5-haiku-20241022-v1:0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea159148514e4ca99873f0db1de52651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment runs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f09a028306433ea724e3117acbbbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scenarios:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each model with its own standalone progress bar\n",
    "for index, evaluation in evaluation_tracking.iterrows():\n",
    "    ## skip the source model\n",
    "    if evaluation['model'] == \"source_model\":\n",
    "        continue\n",
    "    \n",
    "    use_cases_scenarios = []\n",
    "    model_id = evaluation['model']\n",
    "    print(f\"========================== GENERATING RESPONSE WITH MODEL {model_id} ======================\\n\")\n",
    "    prompt = evaluation['text_prompt']\n",
    "    region = evaluation['region']\n",
    "    latency_inference_profile = evaluation['inference_profile']\n",
    "\n",
    "    inferenceConfig={\n",
    "        'maxTokens': MAX_TOKENS,\n",
    "        'temperature': TEMPERATURE,\n",
    "        'topP': TOP_P\n",
    "    }\n",
    "    system_config = []\n",
    "    system_config.append({\"text\": \"You are an assistant.\"})\n",
    "    \n",
    "    # Create a progress bar for documents processing for this model\n",
    "    print(f\"Creating scenarios for model {model_id}:\")\n",
    "    for index, row in tqdm(documents.iterrows(), total=len(documents), desc=f\"Building scenarios\"):\n",
    "        user_prompt = prompt.format(\n",
    "            context=row['document']\n",
    "        )\n",
    "\n",
    "        use_cases_scenarios.append({\n",
    "            \"prompt\": user_prompt,\n",
    "            \"referenceResponse\": row['referenceResponse'],\n",
    "            \"model_id\": model_id,\n",
    "            \"model_clean_name\": evaluation['model_clean_name'],\n",
    "            \"region\": region,\n",
    "            \"latency_inference_profile\": latency_inference_profile\n",
    "        })\n",
    "    \n",
    "    # Main experiment loop with its own progress bar\n",
    "    print(f\"Running experiments for model {model_id}:\")\n",
    "    for run_count in tqdm(range(1, experiment_counts + 1), desc=f\"Experiment runs\"):\n",
    "        selected_scenarios = random.sample(\n",
    "            use_cases_scenarios, \n",
    "            k=len(use_cases_scenarios) // 1\n",
    "        )\n",
    "    \n",
    "        with logging_lock:\n",
    "            logging.info(f\"{len(selected_scenarios)} scenarios x {scenario_config['invocations_per_scenario']} invocations = {len(selected_scenarios) * scenario_config['invocations_per_scenario']} total invocations\")\n",
    "        \n",
    "        logging.info(f\"Running iteration {run_count}\")\n",
    "        \n",
    "        # Create a new client for the main thread\n",
    "        client = boto3.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name=region\n",
    "        )\n",
    "        \n",
    "        # Run the scenarios and measure times\n",
    "        invocations = execute_benchmark(\n",
    "            client, \n",
    "            selected_scenarios, \n",
    "            scenario_config,\n",
    "            inferenceConfig,\n",
    "            system_config,\n",
    "            num_parallel_calls=num_parallel_calls,\n",
    "            early_break=False\n",
    "        )\n",
    "        \n",
    "        # Convert the invocations list to a pandas DataFrame\n",
    "        df = pd.DataFrame(invocations)\n",
    "        df['timestamp'] = pd.Timestamp.now()\n",
    "        df['run_count'] = run_count\n",
    "\n",
    "        # Write the DataFrame to a CSV file\n",
    "        document_summarization_output_file_name = f\"../outputs/document_summarization_{model_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "        df.to_csv(document_summarization_output_file_name, index=False)\n",
    "    \n",
    "        with logging_lock:\n",
    "            logging.info(f\"Results written to {document_summarization_output_file_name}\")\n",
    "            logging.info(f\"Completed run {run_count} of {scenario_config['invocations_per_scenario']}\")\n",
    "    \n",
    "    # Add a visual separator between models\n",
    "    print(f\"\\n{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Evaluation Results\n",
    "\n",
    "\n",
    "Before moving on, it's important to verify that our latency evaluation ran successfully and produced the expected outputs. \n",
    "\n",
    "Let's examine the output directory and peek at the contents of our result files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 356\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  56287 Jun 24 00:34 document_summarization_amazon.nova-lite-v1:0_20250624_003419.csv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 242338 Jun  1 01:08 document_summarization_source_model.csv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  58047 Jun 24 00:35 document_summarization_us.anthropic.claude-3-5-haiku-20241022-v1:0_20250624_003534.csv\n"
     ]
    }
   ],
   "source": [
    "! ls -l ../outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model,model_clean_name,region,invocation_id,prompt,model_response,referenceResponse,latency,model_latencyMs,job_timestamp_iso,model_input_tokens,model_output_tokens,api_call_status,full_error_message,TEMPERATURE,TOP_P,EXPERIMENT_NAME,inference_profile,timestamp,run_count\n",
      "amazon.nova-lite-v1:0,amazon.nova-lite-v1-0,us-east-1,0,\"## Instruction\n",
      "Your task is to read the given article in the Ronan O'Mahony and James Cronin both notched two tries with Tommy O'Donnell and the Scannell brothers Niall and Rory also touching down for Munster.\n"
     ]
    }
   ],
   "source": [
    "! head -3 ../outputs/document_summarization_amazon.nova-lite-v1:0_*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Our Progress - Updating the Evaluation Tracking System\n",
    "\n",
    "Now that we've successfully completed our latency evaluations, we'll update our central tracking dataframe to record:\n",
    "\n",
    "1. The location of our latency evaluation results\n",
    "2. Any additional metadata about the evaluation process\n",
    "3. References to output files for later analysis\n",
    "\n",
    "This information will be critical in Step 5 when we consolidate all our evaluation data for the final comparison. The tracking file maintains continuity between the different evaluation steps and ensures we can easily access all results during the analysis phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the progress\n",
    "\n",
    "evaluation_tracking['latency_evaluation_output'] = \"../outputs/\"\n",
    "\n",
    "evaluation_tracking.to_csv(evaluation_tracking_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. ✅ **Implemented** a comprehensive latency evaluation framework\n",
    "2. ✅ **Generated** responses from our candidate models using optimized prompts\n",
    "3. ✅ **Measured** critical performance metrics including end-to-end latency\n",
    "4. ✅ **Collected** token usage data for later cost analysis\n",
    "5. ✅ **Saved** detailed results for further analysis\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, **Step 4 - Evaluate Model Quality**, we'll focus on the second critical dimension of model evaluation: response quality. We'll use automated techniques like LLM-as-a-Judge to assess how well each model performs the summarization task.\n",
    "\n",
    "By combining the latency metrics we've gathered here with the quality metrics from the next step, we'll build a comprehensive understanding of each model's strengths and weaknesses, enabling data-driven migration decisions.\n",
    "\n",
    "> **For Production Implementations:** Remember that real-world implementations would typically include more extensive testing with larger sample sizes, testing across different time periods, and evaluating under various load conditions. The approach demonstrated here can be scaled up for more robust evaluation in production scenarios.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
