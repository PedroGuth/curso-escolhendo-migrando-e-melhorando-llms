{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Prepare Data and Define Success Criteria\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this initial phase, we'll establish the foundation for our model migration journey. Proper preparation is crucial for successful model transitions. We need to clearly define our business case, understand current performance, and establish measurable success criteria.\n",
    "\n",
    "### Business Context\n",
    "\n",
    "For this workshop, we're working with the following scenario:\n",
    "\n",
    "- **Business Case:** A customer is using a proprietary LLM (our \"source model\") for document summarization, which generates concise abstracts from longer texts.\n",
    "- **Pain Points:** The source model has become expensive to operate at scale, putting pressure on the operation budget.\n",
    "- **Current Performance:** The model provides good quality summaries but at a high cost per inference.\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "For our migration to be considered successful, we need to meet or exceed these criteria:\n",
    "\n",
    "| Dimension | Success Criteria | Measurement Method |\n",
    "|-----------|------------------|-------------------|\n",
    "| Quality | Similar or better accuracy in summarization | LLM-as-judge evaluation comparing summary quality |\n",
    "| Latency | Similar or better latency | Direct measurement of response times |\n",
    "| Cost | Lower overall cost per inference | Calculation based on token counts and pricing |\n",
    "\n",
    "Let's begin by exploring our dataset and analyzing the source model's performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://plugin.us-east-1.prod.workshops.aws\n",
      "Requirement already satisfied: numexpr in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.7.3)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.11.0.tar.gz (108 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from numexpr) (1.26.4)\n",
      "Building wheels for collected packages: numexpr\n",
      "  Building wheel for numexpr (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for numexpr: filename=numexpr-2.11.0-cp310-cp310-linux_x86_64.whl size=149340 sha256=1b11388fc2f7cde048a4b8124c95df360a3a038b5a88bd93fb8ef7a0fc52b673\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/a7/d0/17/e38daa1110f54ba5f7330d38440f592c063251a6456053e2ed\n",
      "Successfully built numexpr\n",
      "Installing collected packages: numexpr\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.7.3\n",
      "    Uninstalling numexpr-2.7.3:\n",
      "      Successfully uninstalled numexpr-2.7.3\n",
      "Successfully installed numexpr-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "A high-quality evaluation dataset is critical for migration success. For this workshop, we're using the [EdinburghNLP/xsum](https://huggingface.co/datasets/EdinburghNLP/xsum) dataset, a well-established benchmark for text summarization that contains news articles paired with human-written summaries.\n",
    "\n",
    "We've selected 10 representative samples from this dataset to keep the workshop computationally efficient. These samples represent various document lengths and complexity levels to provide a robust evaluation.\n",
    "\n",
    "> **Note for Self-Paced Learners:** If you're running this workshop in your own environment, you're welcome to substitute your own domain-specific dataset. Just make sure it follows the same CSV format with `document` and `referenceResponse` columns. If you're interested in how we preprocessed the dataset, check the `src/dataset.py` file.\n",
    "\n",
    "Let's examine a sample to understand what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test dataset\n",
    "! head -5 ../data/document_sample_10.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Model Analysis\n",
    "\n",
    "### Baseline Performance Measurement\n",
    "\n",
    "> **Note:** In a real migration scenario, you would collect these metrics from your production system. If you're building a new application rather than migrating, you might not have a source model to compare against.\n",
    "\n",
    "A critical step in the migration process is establishing a reliable performance baseline for your source model. This baseline will serve as the comparison point for all candidate models.\n",
    "\n",
    "For this workshop, we've already generated responses from our hypothetical \"source model\" using the dataset.\n",
    "\n",
    "\n",
    "\n",
    "> **For Self-Paced Learners:** If you wish to use your own source model (e.g., OpenAI's GPT-4o-mini or another model), you can generate responses following the same format and replace the provided CSV file.\n",
    "\n",
    "Let's examine one of the source model's responses to understand its performance characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First row contents:\n",
      "--------------------------------------------------------------------------------\n",
      "model: source_model\n",
      "region: us-east-1\n",
      "invocation_id: 0\n",
      "prompt: \n",
      "First, please read the article below.\n",
      "BAE Systems blamed the closure plan for the former Vickers plant in Scotswood on fewer Ministry of Defence orders.\n",
      "Now neighbouring defence and construction giant Reece Group has signed a deal with BAE to take on the factory for an undisclosed sum.\n",
      "Reece plans to transfer work from existing sites into Scotswood.\n",
      "Reece already employs about 500 people and hopes to expand the workforce at its new premises over time.\n",
      "The Group includes Pearson Engineering, which designs and manufactures a range of military equipment for customers, including BAE Systems. It also produces equipment for oil and gas fields.\n",
      "Chairman John Reece said: \"This landmark site will be fully used as a manufacturing facility. We have also acquired plant and machinery no longer required by BAE Systems which will be used to support our activities in the oil and gas, and the subsea markets.\n",
      "\"We believe that engineering and manufacturing still has a valid and prosperous future in the North East.\n",
      "\"We place great emphasis on research and development and our investment in Scotswood will include the establishment of the Reece Innovation Centre, whose focus will be to design the group's next generation of innovative engineering products.\"\n",
      " Now, can you write me an extremely short abstract for it?\n",
      "\n",
      "model_response: Reece Group acquires former BAE Systems plant in Scotswood, plans to expand workforce and establish Reece Innovation Centre for next-gen engineering products, utilizing the site for manufacturing in oil & gas, and subsea markets.\n",
      "referenceResponse: A former tank factory in Newcastle, which closed last year with the loss of 300 jobs, has been bought by a rival engineering firm.\n",
      "latency: 0.94\n",
      "model_latencyMs: 916\n",
      "job_timestamp_iso: 2025-04-29T19:17:43Z\n",
      "model_input_tokens: 300\n",
      "model_output_tokens: 52\n",
      "api_call_status: Success\n",
      "full_error_message: Success\n",
      "TEMPERATURE: 0\n",
      "TOP_P: 1\n",
      "EXPERIMENT_NAME: experiment_2025-04-29T19:16:00Z\n",
      "inference_profile: standard\n",
      "timestamp: 2025-04-29 19:26:04.417843\n",
      "run_count: 1\n",
      "cost: 0.000231\n",
      "quality_evaluation_cost: 0.0002816\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check responses from source model \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "csv_path = '../outputs/document_summarization_source_model.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Get and print the first row nicely\n",
    "first_row = df.iloc[0]\n",
    "print(\"\\nFirst row contents:\")\n",
    "print(\"-\" * 80)\n",
    "for column, value in first_row.items():\n",
    "    print(f\"{column}: {value}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source Model Performance Summary:\n",
      "Average latency: 1.66 seconds\n",
      "Average input tokens: 656\n",
      "Average output tokens: 90\n"
     ]
    }
   ],
   "source": [
    "# get summary statistics for source model performance. \n",
    "# This is just the inference response file. We will measure the quality and cost in the later steps\n",
    "\n",
    "source_df = pd.read_csv('../outputs/document_summarization_source_model.csv')\n",
    "print(\"\\nSource Model Performance Summary:\")\n",
    "print(f\"Average latency: {source_df['latency'].mean():.2f} seconds\")\n",
    "print(f\"Average input tokens: {source_df['model_input_tokens'].mean():.0f}\")\n",
    "print(f\"Average output tokens: {source_df['model_output_tokens'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Model Selection\n",
    "\n",
    "### Selection Strategy\n",
    "\n",
    "Selecting the right models for evaluation is crucial for a successful migration. We should consider several factors including:\n",
    "\n",
    "- Input and output modalities\n",
    "- Context window size\n",
    "- Cost per inference/token\n",
    "- Performance capabilities\n",
    "- Domain specialization\n",
    "\n",
    "Our source model is a small-sized foundation model optimized for document summarization. Based on our analysis of available models in AWS Bedrock, we've selected two promising candidates:\n",
    "\n",
    "1. **Amazon Nova Lite (amazon.nova-lite-v1:0)**\n",
    "   - A lightweight, cost-efficient model\n",
    "   - Designed for efficient text processing\n",
    "   - Lower token pricing compared to the source model\n",
    "   - Well-suited for concise summarization tasks\n",
    "\n",
    "2. **Meta's Llama 3.2 11B (us.meta.llama3-2-11b-instruct-v1:0)**\n",
    "   - Open-weights model with strong performance\n",
    "   - Enhanced instruction following capabilities\n",
    "   - Competitive token pricing\n",
    "   - Available through cross-region inference\n",
    "\n",
    "> **Note:** For a comprehensive comparison of Bedrock models and their capabilities, refer to [AWS Bedrock's model documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).\n",
    "\n",
    "In a real migration scenario, you might test more models or different variants based on your specific requirements and constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking Setup\n",
    "\n",
    "### Creating an Evaluation Framework\n",
    "\n",
    "As we progress through this workshop, we'll evaluate multiple models across various dimensions. To maintain organization and enable comprehensive comparison at the end, we'll create a tracking system.\n",
    "\n",
    "The tracking framework will help us maintain consistency and capture all relevant metrics.\n",
    "\n",
    "We'll start by creating a simple dataframe with our source and candidate models. As we progress through the workshop, we'll add more metrics and evaluation results to this tracking system.\n",
    "\n",
    "> **Note:** If you're re-running this notebook, we'll first clean up any existing tracking file to start fresh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/evaluation_tracking.csv has been deleted successfully\n"
     ]
    }
   ],
   "source": [
    "## clean up the tracking file. It is necessary if you rerun the notebooks\n",
    "\n",
    "import os\n",
    "\n",
    "file_path = \"../data/evaluation_tracking.csv\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} has been deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while deleting {file_path}: {str(e)}\")\n",
    "else:\n",
    "    print(f\"File {file_path} does not exist. You can continue the workshop.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         model  \\\n",
      "0                                 source_model   \n",
      "1                        amazon.nova-lite-v1:0   \n",
      "2  us.anthropic.claude-3-5-haiku-20241022-v1:0   \n",
      "\n",
      "                              model_clean_name  \n",
      "0                                 source_model  \n",
      "1                        amazon.nova-lite-v1-0  \n",
      "2  us.anthropic.claude-3-5-haiku-20241022-v1-0  \n",
      "\n",
      "DataFrame saved to ../data/evaluation_tracking.csv\n"
     ]
    }
   ],
   "source": [
    "## Supported foundation models in Amazon Bedrock: https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html\n",
    "\n",
    "data = {\n",
    "    'model': ['source_model', ## the model we are going to evaluate against, DO NOT CHANGE THIS NAME\n",
    "              'amazon.nova-lite-v1:0', \n",
    "             # 'us.meta.llama3-2-11b-instruct-v1:0'] ## LLAMA 3.2 11b only avaialbe through cross region inference. read more: https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html#inference-profiles-support-system\n",
    "             # 'anthropic.claude-3-haiku-20240307-v1:0']\n",
    "              'us.anthropic.claude-3-5-haiku-20241022-v1:0'] ## cross region inference\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "evaluation_tracking = pd.DataFrame(data)\n",
    "\n",
    "evaluation_tracking['model_clean_name'] = evaluation_tracking['model'].str.replace(\":\", \"-\")\n",
    "\n",
    "print(evaluation_tracking)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "evaluation_tracking_file = '../data/evaluation_tracking.csv'\n",
    "evaluation_tracking.to_csv(evaluation_tracking_file, index=False)\n",
    "print(f\"\\nDataFrame saved to {evaluation_tracking_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we've completed the critical first phase of the LLM migration process:\n",
    "\n",
    "1. ✅ **Defined our business case** and understood the limitations of our current solution\n",
    "2. ✅ **Established clear success criteria** across quality, performance, and cost dimensions\n",
    "3. ✅ **Examined our evaluation dataset** and its characteristics\n",
    "4. ✅ **Analyzed our source model's performance** to establish a baseline\n",
    "5. ✅ **Selected promising candidate models** for evaluation\n",
    "6. ✅ **Created a tracking system** to organize our evaluation process\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Move to Step 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
